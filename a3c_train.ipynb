{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a3c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "f159fdcf-f879-4008-c351-e63fc32d6671"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'a3c_train.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m51Mu4xy9-Nj"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class MasterBrain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model(input_, [actor, critic])\n",
        "        mastermodel.compile(optimizer=Adam(lr=lr))\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        [l_p.assign(g_p) for l_p, g_p in zip(model.trainable_weights, self.mastermodel.trainable_weights)]\n",
        "\n",
        "    def integration(self, model):\n",
        "        [g_p.assign(l_p) for g_p, l_p in zip(self.mastermodel.trainable_weights, model.trainable_weights)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "def play_game(env, actor, critic, scaler, episodes_times = 25, batch_size = 32, mode = 'train'):\n",
        "\n",
        "    experiences = []\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        state = state.flatten()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        while not done:\n",
        "            action = actor.policynetwork(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            next_state = next_state.flatten()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if mode == 'train':\n",
        "                experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                if len(experiences) == batch_size:\n",
        "                    critic.valuenetwork(experiences)\n",
        "                    experiences = []\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        actor.integration()\n",
        "        actor.layering()\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f} TradeTimes: {info['trade_time']} TradeWin: {info['trade_win']}\")\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f}\")\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef88cc7-bcba-486d-9042-1858c3f9e157"
      },
      "source": [
        "initial_money=1000000\n",
        "mode = 'train'\n",
        "episodes_times = 25\n",
        "batch_size = 32\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "if mode == 'test':\n",
        "    masterbrain.load(f'{models_folder}/a3c_model.h5')\n",
        "\n",
        "\n",
        "if mode == 'test':\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "        print(row, file=f)\n",
        "else:\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit'\n",
        "        print(row, file=f)\n",
        "\n",
        "thred_num = 4\n",
        "envs = []\n",
        "for i in range(thred_num):\n",
        "    e = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    a = Actor(brain)\n",
        "    c = Critic(model)\n",
        "    s = make_scaler(e)\n",
        "    arr = [e,a,c,s]\n",
        "    envs.append(arr)\n",
        "\n",
        "features = []\n",
        "with PoolExecutor(max_workers=thred_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: play_game(env[0],env[1],env[2],env[3], episodes_times, batch_size, mode)\n",
        "        features.append(executor.submit(job))\n",
        "\n",
        "if mode == 'train':\n",
        "    masterbrain.save(f'{models_folder}/a3c_model.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/25 RapTime: 0:00:06.311206 FixedProfit: 1087386\n",
            "Episode: 1/25 RapTime: 0:00:06.338844 FixedProfit: 1118026\n",
            "Episode: 1/25 RapTime: 0:00:06.343835 FixedProfit: 1155092\n",
            "Episode: 1/25 RapTime: 0:00:06.387071 FixedProfit: 1234473\n",
            "Episode: 2/25 RapTime: 0:00:05.791966 FixedProfit: 1103558\n",
            "Episode: 2/25 RapTime: 0:00:05.830939 FixedProfit: 1189961\n",
            "Episode: 2/25 RapTime: 0:00:05.827383 FixedProfit: 1220824\n",
            "Episode: 2/25 RapTime: 0:00:05.802611 FixedProfit: 1143210\n",
            "Episode: 3/25 RapTime: 0:00:05.658802 FixedProfit: 1091796\n",
            "Episode: 3/25 RapTime: 0:00:05.649205 FixedProfit: 993026\n",
            "Episode: 3/25 RapTime: 0:00:05.685038 FixedProfit: 1197165\n",
            "Episode: 3/25 RapTime: 0:00:05.676323 FixedProfit: 966151\n",
            "Episode: 4/25 RapTime: 0:00:06.101482 FixedProfit: 988197\n",
            "Episode: 4/25 RapTime: 0:00:06.132172 FixedProfit: 1044784\n",
            "Episode: 4/25 RapTime: 0:00:06.128192 FixedProfit: 1197165\n",
            "Episode: 4/25 RapTime: 0:00:06.132575 FixedProfit: 982482\n",
            "Episode: 5/25 RapTime: 0:00:05.937959 FixedProfit: 1095045\n",
            "Episode: 5/25 RapTime: 0:00:05.983106 FixedProfit: 1022063\n",
            "Episode: 5/25 RapTime: 0:00:06.025756 FixedProfit: 1197165\n",
            "Episode: 5/25 RapTime: 0:00:06.005251 FixedProfit: 1068877\n",
            "Episode: 6/25 RapTime: 0:00:06.037132 FixedProfit: 1219075Episode: 6/25 RapTime: 0:00:05.994169 FixedProfit: 1108033\n",
            "\n",
            "Episode: 6/25 RapTime: 0:00:06.065616 FixedProfit: 1197165\n",
            "Episode: 6/25 RapTime: 0:00:06.060747 FixedProfit: 1085828\n",
            "Episode: 7/25 RapTime: 0:00:05.997957 FixedProfit: 1198992\n",
            "Episode: 7/25 RapTime: 0:00:06.013907 FixedProfit: 1117218\n",
            "Episode: 7/25 RapTime: 0:00:06.021735 FixedProfit: 1112689\n",
            "Episode: 7/25 RapTime: 0:00:06.045211 FixedProfit: 1197165\n",
            "Episode: 8/25 RapTime: 0:00:05.630416 FixedProfit: 951755\n",
            "Episode: 8/25 RapTime: 0:00:05.659728 FixedProfit: 943307\n",
            "Episode: 8/25 RapTime: 0:00:05.638555 FixedProfit: 1197165\n",
            "Episode: 8/25 RapTime: 0:00:05.688624 FixedProfit: 1133566\n",
            "Episode: 9/25 RapTime: 0:00:05.800313 FixedProfit: 826199\n",
            "Episode: 9/25 RapTime: 0:00:05.848342 FixedProfit: 1343379\n",
            "Episode: 9/25 RapTime: 0:00:05.788657 FixedProfit: 1357244\n",
            "Episode: 9/25 RapTime: 0:00:05.839243 FixedProfit: 1197165\n",
            "Episode: 10/25 RapTime: 0:00:05.655014 FixedProfit: 1281607\n",
            "Episode: 10/25 RapTime: 0:00:05.645528 FixedProfit: 1201192\n",
            "Episode: 10/25 RapTime: 0:00:05.649310 FixedProfit: 1197165\n",
            "Episode: 10/25 RapTime: 0:00:05.693459 FixedProfit: 1010840\n",
            "Episode: 11/25 RapTime: 0:00:05.698981 FixedProfit: 1119781\n",
            "Episode: 11/25 RapTime: 0:00:05.703863 FixedProfit: 1103503\n",
            "Episode: 11/25 RapTime: 0:00:05.688704 FixedProfit: 1197165\n",
            "Episode: 11/25 RapTime: 0:00:05.728068 FixedProfit: 1129678\n",
            "Episode: 12/25 RapTime: 0:00:05.709470 FixedProfit: 1182657\n",
            "Episode: 12/25 RapTime: 0:00:05.714979 FixedProfit: 1095580\n",
            "Episode: 12/25 RapTime: 0:00:05.732202 FixedProfit: 1197165\n",
            "Episode: 12/25 RapTime: 0:00:05.695395 FixedProfit: 1158546\n",
            "Episode: 13/25 RapTime: 0:00:05.668389 FixedProfit: 972421\n",
            "Episode: 13/25 RapTime: 0:00:05.701158 FixedProfit: 1236223\n",
            "Episode: 13/25 RapTime: 0:00:05.657732 FixedProfit: 1197165\n",
            "Episode: 13/25 RapTime: 0:00:05.708160 FixedProfit: 988391\n",
            "Episode: 14/25 RapTime: 0:00:05.683626 FixedProfit: 1160983\n",
            "Episode: 14/25 RapTime: 0:00:05.675660 FixedProfit: 1081157\n",
            "Episode: 14/25 RapTime: 0:00:05.690819 FixedProfit: 1197165\n",
            "Episode: 14/25 RapTime: 0:00:05.660730 FixedProfit: 1053475\n",
            "Episode: 15/25 RapTime: 0:00:05.881003 FixedProfit: 1021400\n",
            "Episode: 15/25 RapTime: 0:00:05.870627 FixedProfit: 1064182\n",
            "Episode: 15/25 RapTime: 0:00:05.894305 FixedProfit: 1197165\n",
            "Episode: 15/25 RapTime: 0:00:05.904148 FixedProfit: 921515\n",
            "Episode: 16/25 RapTime: 0:00:05.968661 FixedProfit: 1010214\n",
            "Episode: 16/25 RapTime: 0:00:06.025892 FixedProfit: 1064412\n",
            "Episode: 16/25 RapTime: 0:00:05.992740 FixedProfit: 1197165\n",
            "Episode: 16/25 RapTime: 0:00:06.053835 FixedProfit: 1123288\n",
            "Episode: 17/25 RapTime: 0:00:06.068922 FixedProfit: 1105736\n",
            "Episode: 17/25 RapTime: 0:00:06.035372 FixedProfit: 1314546\n",
            "Episode: 17/25 RapTime: 0:00:06.083588 FixedProfit: 1197165\n",
            "Episode: 17/25 RapTime: 0:00:06.073638 FixedProfit: 1158101\n",
            "Episode: 18/25 RapTime: 0:00:06.002624 FixedProfit: 1127502\n",
            "Episode: 18/25 RapTime: 0:00:06.038648 FixedProfit: 1044936\n",
            "Episode: 18/25 RapTime: 0:00:05.976876 FixedProfit: 1197165\n",
            "Episode: 18/25 RapTime: 0:00:06.029905 FixedProfit: 1025880\n",
            "Episode: 19/25 RapTime: 0:00:05.826218 FixedProfit: 898850\n",
            "Episode: 19/25 RapTime: 0:00:05.826970 FixedProfit: 1134393\n",
            "Episode: 19/25 RapTime: 0:00:05.869567 FixedProfit: 1197165\n",
            "Episode: 19/25 RapTime: 0:00:05.836878 FixedProfit: 1235461\n",
            "Episode: 20/25 RapTime: 0:00:05.674842 FixedProfit: 1149771\n",
            "Episode: 20/25 RapTime: 0:00:05.628297 FixedProfit: 1134583\n",
            "Episode: 20/25 RapTime: 0:00:05.647388 FixedProfit: 1197165\n",
            "Episode: 20/25 RapTime: 0:00:05.588957 FixedProfit: 1228573\n",
            "Episode: 21/25 RapTime: 0:00:05.717879 FixedProfit: 1260463\n",
            "Episode: 21/25 RapTime: 0:00:05.685488 FixedProfit: 1116535\n",
            "Episode: 21/25 RapTime: 0:00:05.691297 FixedProfit: 1197165\n",
            "Episode: 21/25 RapTime: 0:00:05.716926 FixedProfit: 1141735\n",
            "Episode: 22/25 RapTime: 0:00:05.679919 FixedProfit: 1007800\n",
            "Episode: 22/25 RapTime: 0:00:05.732532 FixedProfit: 1260160\n",
            "Episode: 22/25 RapTime: 0:00:05.647959 FixedProfit: 1197165\n",
            "Episode: 22/25 RapTime: 0:00:05.696942 FixedProfit: 1145380\n",
            "Episode: 23/25 RapTime: 0:00:05.638152 FixedProfit: 1129978\n",
            "Episode: 23/25 RapTime: 0:00:05.596677 FixedProfit: 1227351\n",
            "Episode: 23/25 RapTime: 0:00:05.603416 FixedProfit: 1197165\n",
            "Episode: 23/25 RapTime: 0:00:05.592656 FixedProfit: 1009636\n",
            "Episode: 24/25 RapTime: 0:00:05.662320 FixedProfit: 1026004\n",
            "Episode: 24/25 RapTime: 0:00:05.691694 FixedProfit: 1019188\n",
            "Episode: 24/25 RapTime: 0:00:05.681545 FixedProfit: 1197165\n",
            "Episode: 24/25 RapTime: 0:00:05.675897 FixedProfit: 1046428\n",
            "Episode: 25/25 RapTime: 0:00:05.626441 FixedProfit: 1271145\n",
            "Episode: 25/25 RapTime: 0:00:05.619041 FixedProfit: 1208082\n",
            "Episode: 25/25 RapTime: 0:00:05.566201 FixedProfit: 1197165\n",
            "Episode: 25/25 RapTime: 0:00:05.473339 FixedProfit: 1117760\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}