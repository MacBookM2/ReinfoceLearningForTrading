{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/ppo_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "af571064-883d-4e40-fedf-5f065b7d2f46"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'train'\n",
        "name = 'ppo'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP-xlKoLAwG7"
      },
      "source": [
        "class Leaner:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1UX215aeoXq"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : List[List[float]] = field(default_factory=list)\n",
        "    action : List[int] = field(default_factory=list)\n",
        "    reward : List[float] = field(default_factory=list)\n",
        "    next_state : List[List[int]] = field(default_factory=list)\n",
        "    done : List[bool] = field(default_factory=list)\n",
        "    # action_prob : List[List[float]] = field(default_factory=list)\n",
        "    # v : List[float] = field(default_factory=list)\n",
        "\n",
        "    def random_experiences(self, batch_size):\n",
        "        # len(self.state) = 627\n",
        "        max_size = len(self.state) - 1\n",
        "        batch_num = self._random_num(1, max_size, batch_size)\n",
        "        experiences = []\n",
        "        for i in batch_num:\n",
        "            experiences.append({\"state\": self.state[i], \"action\": self.action[i], \n",
        "                                \"reward\": self.reward[i], \"next_state\": self.next_state[i], \n",
        "                                \"done\": self.done[i]})\n",
        "        return experiences\n",
        "\n",
        "    def _random_num(self, a, b, k):\n",
        "        ns = []\n",
        "        while len(ns) < k:\n",
        "            n = random.randint(a, b)\n",
        "            if not n in ns:\n",
        "                ns.append(n)\n",
        "        return ns"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, num, mdl_dir, name, batch_size = 128, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory()\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    memory.state.append(state)\n",
        "                    memory.action.append(action)\n",
        "                    memory.reward.append(reward)\n",
        "                    memory.next_state.append(next_state)\n",
        "                    memory.done.append(done)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                experiences = memory.random_experiences(self.batch_size)\n",
        "                self.critic.valuenetwork(experiences)\n",
        "\n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b3f8c1-5884-4818-deea-9b6ac9acd5e7"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 50\n",
        "batch_size = 128\n",
        "\n",
        "brain = ParameterServer()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    model = brain.model\n",
        "    actor = Actor(brain)\n",
        "    critic = Critic(model)\n",
        "    main = Main(env, actor, critic, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:00:05.020228 FixedProfit: 1160645\n",
            "Episode: 1/50 RapTime: 0:00:05.018359 FixedProfit: 974606\n",
            "Episode: 1/50 RapTime: 0:00:05.024773 FixedProfit: 921013\n",
            "Episode: 1/50 RapTime: 0:00:05.033723 FixedProfit: 1252716\n",
            "Episode: 2/50 RapTime: 0:00:04.986189 FixedProfit: 1169529\n",
            "Episode: 2/50 RapTime: 0:00:04.988610 FixedProfit: 1217081\n",
            "Episode: 2/50 RapTime: 0:00:05.034762 FixedProfit: 1220363\n",
            "Episode: 2/50 RapTime: 0:00:05.040737 FixedProfit: 1057728\n",
            "Episode: 3/50 RapTime: 0:00:05.100688 FixedProfit: 1150616\n",
            "Episode: 3/50 RapTime: 0:00:05.100640 FixedProfit: 983804\n",
            "Episode: 3/50 RapTime: 0:00:05.078584 FixedProfit: 1049694\n",
            "Episode: 3/50 RapTime: 0:00:05.133979 FixedProfit: 1152430\n",
            "Episode: 4/50 RapTime: 0:00:05.020857 FixedProfit: 1184802\n",
            "Episode: 4/50 RapTime: 0:00:05.006021 FixedProfit: 1255531\n",
            "Episode: 4/50 RapTime: 0:00:05.047719 FixedProfit: 1216459\n",
            "Episode: 4/50 RapTime: 0:00:05.072950 FixedProfit: 1187885\n",
            "Episode: 5/50 RapTime: 0:00:05.017398 FixedProfit: 1077346\n",
            "Episode: 5/50 RapTime: 0:00:05.017916 FixedProfit: 1183348\n",
            "Episode: 5/50 RapTime: 0:00:05.028380 FixedProfit: 1171514\n",
            "Episode: 5/50 RapTime: 0:00:05.014425 FixedProfit: 1085637\n",
            "Episode: 6/50 RapTime: 0:00:04.967192 FixedProfit: 1193794\n",
            "Episode: 6/50 RapTime: 0:00:05.000779 FixedProfit: 1021783\n",
            "Episode: 6/50 RapTime: 0:00:04.972128 FixedProfit: 1145277\n",
            "Episode: 6/50 RapTime: 0:00:05.000313 FixedProfit: 1169080\n",
            "Episode: 7/50 RapTime: 0:00:04.793968 FixedProfit: 1197924\n",
            "Episode: 7/50 RapTime: 0:00:04.779968 FixedProfit: 1079463\n",
            "Episode: 7/50 RapTime: 0:00:04.826943 FixedProfit: 1137356\n",
            "Episode: 7/50 RapTime: 0:00:04.775045 FixedProfit: 1089632\n",
            "Episode: 8/50 RapTime: 0:00:04.682594 FixedProfit: 1238951\n",
            "Episode: 8/50 RapTime: 0:00:04.687184 FixedProfit: 1157137\n",
            "Episode: 8/50 RapTime: 0:00:04.730861 FixedProfit: 1077831\n",
            "Episode: 8/50 RapTime: 0:00:04.741424 FixedProfit: 1163918\n",
            "Episode: 9/50 RapTime: 0:00:04.675591 FixedProfit: 1132892\n",
            "Episode: 9/50 RapTime: 0:00:04.713280 FixedProfit: 1220098\n",
            "Episode: 9/50 RapTime: 0:00:04.754771 FixedProfit: 1150062\n",
            "Episode: 9/50 RapTime: 0:00:04.720129 FixedProfit: 1140348\n",
            "Episode: 10/50 RapTime: 0:00:04.700851 FixedProfit: 1139756\n",
            "Episode: 10/50 RapTime: 0:00:04.707040 FixedProfit: 1029053\n",
            "Episode: 10/50 RapTime: 0:00:04.677268 FixedProfit: 1119335\n",
            "Episode: 10/50 RapTime: 0:00:04.746093 FixedProfit: 1056658\n",
            "Episode: 11/50 RapTime: 0:00:04.667541 FixedProfit: 1117381\n",
            "Episode: 11/50 RapTime: 0:00:04.681176 FixedProfit: 1131443\n",
            "Episode: 11/50 RapTime: 0:00:04.649800 FixedProfit: 1037278\n",
            "Episode: 11/50 RapTime: 0:00:04.703268 FixedProfit: 980432\n",
            "Episode: 12/50 RapTime: 0:00:04.734669 FixedProfit: 1241533\n",
            "Episode: 12/50 RapTime: 0:00:04.735042 FixedProfit: 1129038\n",
            "Episode: 12/50 RapTime: 0:00:04.769113 FixedProfit: 1161535\n",
            "Episode: 12/50 RapTime: 0:00:04.703880 FixedProfit: 1210280\n",
            "Episode: 13/50 RapTime: 0:00:04.677541 FixedProfit: 1105371\n",
            "Episode: 13/50 RapTime: 0:00:04.655013 FixedProfit: 1191305\n",
            "Episode: 13/50 RapTime: 0:00:04.648866 FixedProfit: 1188276\n",
            "Episode: 13/50 RapTime: 0:00:04.691502 FixedProfit: 1218422\n",
            "Episode: 14/50 RapTime: 0:00:04.681554 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:04.714641 FixedProfit: 1111926\n",
            "Episode: 14/50 RapTime: 0:00:04.652988 FixedProfit: 1206930\n",
            "Episode: 14/50 RapTime: 0:00:04.687148 FixedProfit: 1212277\n",
            "Episode: 15/50 RapTime: 0:00:04.680934 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:04.679370 FixedProfit: 1199402\n",
            "Episode: 15/50 RapTime: 0:00:04.670988 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:04.696237 FixedProfit: 1177757\n",
            "Episode: 16/50 RapTime: 0:00:04.963072 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:04.980072 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:04.997951 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:04.974719 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:04.990616 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:05.014441 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:04.957579 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:04.999348 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:04.969474 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:04.998149 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:04.959126 FixedProfit: 1202953\n",
            "Episode: 18/50 RapTime: 0:00:05.028149 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:05.195623 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:05.208051 FixedProfit: 1177790\n",
            "Episode: 19/50 RapTime: 0:00:05.260573 FixedProfit: 1179128\n",
            "Episode: 19/50 RapTime: 0:00:05.316237 FixedProfit: 1208128\n",
            "Episode: 20/50 RapTime: 0:00:05.258572 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:05.257500 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:05.295333 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:05.244458 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:04.876761 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:04.898562 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:04.875685 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:04.881623 FixedProfit: 1206060\n",
            "Episode: 22/50 RapTime: 0:00:04.771470 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:04.787356 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:04.788042 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:04.729392 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:04.958720 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:04.947484 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:04.953481 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:05.008195 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:04.973809 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:04.961412 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:04.960102 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:04.980771 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:05.014927 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:04.993709 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:04.968538 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:04.963253 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:05.006469 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:05.022273 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:05.019903 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:05.035926 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:04.881177 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:04.919164 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:04.888608 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:04.833703 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:04.695079 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:04.677093 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:04.671593 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:04.661238 FixedProfit: 1208128\n",
            "Episode: 29/50 RapTime: 0:00:04.718151 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:04.717441 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:04.711810 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:04.724175 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:04.644097 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:04.670179 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:04.652594 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:04.644620 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:04.684706 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:04.641318 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:04.625597 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:04.680231 FixedProfit: 1193306\n",
            "Episode: 32/50 RapTime: 0:00:04.639274 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:04.694802 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:04.703406 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:04.628321 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:04.666680 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:04.666579 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:04.687245 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:04.699240 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:04.691728 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:04.631708 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:04.670247 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:04.666356 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:04.689935 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:04.611183 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:04.692059 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:04.688069 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:04.848313 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:04.852869 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:04.867926 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:04.914281 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:05.021672 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:05.033127 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:05.011427 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:04.986541 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:05.000555 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:04.966008 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:04.982497 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:04.932930 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:04.973546 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:04.937041 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:04.960419 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:04.961118 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:04.959130 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:04.940235 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:04.913922 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:04.957228 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:04.785825 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:04.819704 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:04.858039 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:04.843391 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:04.662443 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:04.702085 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:04.685831 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:04.714795 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:04.642345 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:04.710002 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:04.700311 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:04.666751 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:04.664116 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:04.685172 FixedProfit: 1194143\n",
            "Episode: 44/50 RapTime: 0:00:04.682587 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:04.671340 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:04.694221 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:04.713618 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:04.662223 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:04.681105 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:04.721805 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:04.678064 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:04.661302 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:04.630673 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:04.700284 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:04.681255 FixedProfit: 1208128\n",
            "Episode: 47/50 RapTime: 0:00:04.695869 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:04.685374 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:04.641088 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:04.677756 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:04.700352 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:04.663194 FixedProfit: 1208557\n",
            "Episode: 49/50 RapTime: 0:00:04.655687 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:04.695833 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:04.684400 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:04.678041 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:04.925000 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:05.314765 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:05.280698 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:05.278261 FixedProfit: 1197165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}