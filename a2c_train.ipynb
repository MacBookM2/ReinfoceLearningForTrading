{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a2c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "6c0f0b0c-77fc-4878-faa1-68a7b6762d74"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "mode = 'train'\n",
        "name = 'a2c'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=1000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.now_step        = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_shape = 3\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        Brain.model = model\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        Brain.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        Brain.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = Brain.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(3, p=act_p[0].numpy())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic(Brain):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "        self.beta  = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, 3)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = Brain.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, Brain.model.trainable_variables)\n",
        "\n",
        "        Brain.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, Brain.model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = Brain.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "        self.total_reward = None\n",
        "        self.experiences = None\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "            self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "        else:\n",
        "            self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "\n",
        "            self.total_reward = 0.0\n",
        "            self.experiences = []\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                self.total_reward += reward\n",
        "\n",
        "                if mode == 'train':\n",
        "                    self.experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                    if len(self.experiences) == self.batch_size:\n",
        "                        self.critic.valuenetwork(self.experiences)\n",
        "                        self.experiences = []\n",
        "\n",
        "                state = next_state\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=self.df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, self.episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            else:\n",
        "                record = pd.Series(info['cur_revenue'], index=self.df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, self.episodes_times, play_time, info['cur_revenue']))\n",
        "\n",
        "            self.df_rec = self.df_rec.append(record, ignore_index=True)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "        self._save_csv()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "    def _save_csv(self):\n",
        "        self.df_rec.to_csv(csv_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb378b7-4c17-417a-b8e9-d238aeee6497"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "\n",
        "actor = Actor()\n",
        "critic = Critic()\n",
        "env = Environment(df, initial_money, mode)\n",
        "main = Main(env, actor, critic, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:02.001197 FixedProfit: 1190760\n",
            "Episode: 2/100 RapTime: 0:00:01.673022 FixedProfit: 1265314\n",
            "Episode: 3/100 RapTime: 0:00:01.642552 FixedProfit: 1206262\n",
            "Episode: 4/100 RapTime: 0:00:01.677965 FixedProfit: 1177159\n",
            "Episode: 5/100 RapTime: 0:00:01.677405 FixedProfit: 1186132\n",
            "Episode: 6/100 RapTime: 0:00:01.643937 FixedProfit: 1249269\n",
            "Episode: 7/100 RapTime: 0:00:01.648332 FixedProfit: 1150097\n",
            "Episode: 8/100 RapTime: 0:00:01.685340 FixedProfit: 1261603\n",
            "Episode: 9/100 RapTime: 0:00:01.645407 FixedProfit: 1217603\n",
            "Episode: 10/100 RapTime: 0:00:01.680715 FixedProfit: 1223785\n",
            "Episode: 11/100 RapTime: 0:00:01.675976 FixedProfit: 1212453\n",
            "Episode: 12/100 RapTime: 0:00:01.659206 FixedProfit: 1199756\n",
            "Episode: 13/100 RapTime: 0:00:01.655165 FixedProfit: 1214954\n",
            "Episode: 14/100 RapTime: 0:00:01.661055 FixedProfit: 1174148\n",
            "Episode: 15/100 RapTime: 0:00:01.647058 FixedProfit: 1329685\n",
            "Episode: 16/100 RapTime: 0:00:01.664511 FixedProfit: 1247524\n",
            "Episode: 17/100 RapTime: 0:00:01.654013 FixedProfit: 1219100\n",
            "Episode: 18/100 RapTime: 0:00:01.633802 FixedProfit: 1238133\n",
            "Episode: 19/100 RapTime: 0:00:01.645783 FixedProfit: 1160931\n",
            "Episode: 20/100 RapTime: 0:00:01.640536 FixedProfit: 1184764\n",
            "Episode: 21/100 RapTime: 0:00:01.661204 FixedProfit: 1174535\n",
            "Episode: 22/100 RapTime: 0:00:01.679723 FixedProfit: 1096303\n",
            "Episode: 23/100 RapTime: 0:00:01.698560 FixedProfit: 941231\n",
            "Episode: 24/100 RapTime: 0:00:01.690006 FixedProfit: 915232\n",
            "Episode: 25/100 RapTime: 0:00:01.716461 FixedProfit: 1057073\n",
            "Episode: 26/100 RapTime: 0:00:01.697505 FixedProfit: 1157549\n",
            "Episode: 27/100 RapTime: 0:00:01.665553 FixedProfit: 1187942\n",
            "Episode: 28/100 RapTime: 0:00:01.663103 FixedProfit: 1113863\n",
            "Episode: 29/100 RapTime: 0:00:01.649580 FixedProfit: 1282565\n",
            "Episode: 30/100 RapTime: 0:00:01.612674 FixedProfit: 1245276\n",
            "Episode: 31/100 RapTime: 0:00:01.636700 FixedProfit: 1187199\n",
            "Episode: 32/100 RapTime: 0:00:01.647629 FixedProfit: 1204314\n",
            "Episode: 33/100 RapTime: 0:00:01.674071 FixedProfit: 1200809\n",
            "Episode: 34/100 RapTime: 0:00:01.698708 FixedProfit: 1197165\n",
            "Episode: 35/100 RapTime: 0:00:01.688897 FixedProfit: 1197165\n",
            "Episode: 36/100 RapTime: 0:00:01.657184 FixedProfit: 1194076\n",
            "Episode: 37/100 RapTime: 0:00:01.668324 FixedProfit: 1197165\n",
            "Episode: 38/100 RapTime: 0:00:01.646051 FixedProfit: 1197165\n",
            "Episode: 39/100 RapTime: 0:00:01.626413 FixedProfit: 1197165\n",
            "Episode: 40/100 RapTime: 0:00:01.617917 FixedProfit: 1197165\n",
            "Episode: 41/100 RapTime: 0:00:01.657437 FixedProfit: 1208638\n",
            "Episode: 42/100 RapTime: 0:00:01.661623 FixedProfit: 1197165\n",
            "Episode: 43/100 RapTime: 0:00:01.640066 FixedProfit: 1197165\n",
            "Episode: 44/100 RapTime: 0:00:01.659844 FixedProfit: 1197165\n",
            "Episode: 45/100 RapTime: 0:00:01.656808 FixedProfit: 1197165\n",
            "Episode: 46/100 RapTime: 0:00:01.679617 FixedProfit: 1197165\n",
            "Episode: 47/100 RapTime: 0:00:01.642639 FixedProfit: 1197165\n",
            "Episode: 48/100 RapTime: 0:00:01.626250 FixedProfit: 1197165\n",
            "Episode: 49/100 RapTime: 0:00:01.625554 FixedProfit: 1197165\n",
            "Episode: 50/100 RapTime: 0:00:01.611066 FixedProfit: 1197165\n",
            "Episode: 51/100 RapTime: 0:00:01.605603 FixedProfit: 1197165\n",
            "Episode: 52/100 RapTime: 0:00:01.677454 FixedProfit: 1197165\n",
            "Episode: 53/100 RapTime: 0:00:01.650969 FixedProfit: 1197165\n",
            "Episode: 54/100 RapTime: 0:00:01.657075 FixedProfit: 1197165\n",
            "Episode: 55/100 RapTime: 0:00:01.642015 FixedProfit: 1197165\n",
            "Episode: 56/100 RapTime: 0:00:01.657674 FixedProfit: 1197165\n",
            "Episode: 57/100 RapTime: 0:00:01.647486 FixedProfit: 1197165\n",
            "Episode: 58/100 RapTime: 0:00:01.629514 FixedProfit: 1197165\n",
            "Episode: 59/100 RapTime: 0:00:01.615832 FixedProfit: 1197165\n",
            "Episode: 60/100 RapTime: 0:00:01.631408 FixedProfit: 1197165\n",
            "Episode: 61/100 RapTime: 0:00:01.668033 FixedProfit: 1197165\n",
            "Episode: 62/100 RapTime: 0:00:01.668221 FixedProfit: 1197165\n",
            "Episode: 63/100 RapTime: 0:00:01.628092 FixedProfit: 1197165\n",
            "Episode: 64/100 RapTime: 0:00:01.709711 FixedProfit: 1197165\n",
            "Episode: 65/100 RapTime: 0:00:01.644017 FixedProfit: 1197165\n",
            "Episode: 66/100 RapTime: 0:00:01.665513 FixedProfit: 1197165\n",
            "Episode: 67/100 RapTime: 0:00:01.637965 FixedProfit: 1197165\n",
            "Episode: 68/100 RapTime: 0:00:01.613408 FixedProfit: 1197165\n",
            "Episode: 69/100 RapTime: 0:00:01.625975 FixedProfit: 1202860\n",
            "Episode: 70/100 RapTime: 0:00:01.643038 FixedProfit: 1197165\n",
            "Episode: 71/100 RapTime: 0:00:01.687139 FixedProfit: 1197165\n",
            "Episode: 72/100 RapTime: 0:00:01.632971 FixedProfit: 1197165\n",
            "Episode: 73/100 RapTime: 0:00:01.673370 FixedProfit: 1197165\n",
            "Episode: 74/100 RapTime: 0:00:01.647353 FixedProfit: 1197165\n",
            "Episode: 75/100 RapTime: 0:00:01.636177 FixedProfit: 1197165\n",
            "Episode: 76/100 RapTime: 0:00:01.615665 FixedProfit: 1197165\n",
            "Episode: 77/100 RapTime: 0:00:01.639196 FixedProfit: 1197165\n",
            "Episode: 78/100 RapTime: 0:00:01.662561 FixedProfit: 1197165\n",
            "Episode: 79/100 RapTime: 0:00:01.666390 FixedProfit: 1197165\n",
            "Episode: 80/100 RapTime: 0:00:01.641153 FixedProfit: 1197165\n",
            "Episode: 81/100 RapTime: 0:00:01.663904 FixedProfit: 1197165\n",
            "Episode: 82/100 RapTime: 0:00:01.619963 FixedProfit: 1197165\n",
            "Episode: 83/100 RapTime: 0:00:01.644799 FixedProfit: 1197165\n",
            "Episode: 84/100 RapTime: 0:00:01.646571 FixedProfit: 1197165\n",
            "Episode: 85/100 RapTime: 0:00:01.667609 FixedProfit: 1197165\n",
            "Episode: 86/100 RapTime: 0:00:01.651360 FixedProfit: 1197165\n",
            "Episode: 87/100 RapTime: 0:00:01.626804 FixedProfit: 1197165\n",
            "Episode: 88/100 RapTime: 0:00:01.613701 FixedProfit: 1197165\n",
            "Episode: 89/100 RapTime: 0:00:01.664381 FixedProfit: 1197165\n",
            "Episode: 90/100 RapTime: 0:00:01.668214 FixedProfit: 1197165\n",
            "Episode: 91/100 RapTime: 0:00:01.649122 FixedProfit: 1197165\n",
            "Episode: 92/100 RapTime: 0:00:01.667613 FixedProfit: 1197165\n",
            "Episode: 93/100 RapTime: 0:00:01.632711 FixedProfit: 1197165\n",
            "Episode: 94/100 RapTime: 0:00:01.625322 FixedProfit: 1197165\n",
            "Episode: 95/100 RapTime: 0:00:01.639716 FixedProfit: 1197165\n",
            "Episode: 96/100 RapTime: 0:00:01.618692 FixedProfit: 1197165\n",
            "Episode: 97/100 RapTime: 0:00:01.657742 FixedProfit: 1197165\n",
            "Episode: 98/100 RapTime: 0:00:01.669141 FixedProfit: 1197165\n",
            "Episode: 99/100 RapTime: 0:00:01.628960 FixedProfit: 1197165\n",
            "Episode: 100/100 RapTime: 0:00:01.663280 FixedProfit: 1197165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}