{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a3c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "c6af8084-634c-48ef-e039-c1666edf9ba1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'a3c_train.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m51Mu4xy9-Nj"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class MasterBrain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model(input_, [actor, critic])\n",
        "        mastermodel.compile(optimizer=Adam(lr=lr))\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "def play_game(env, actor, critic, scaler, episodes_times = 25, batch_size = 32, mode = 'train'):\n",
        "\n",
        "    experiences = []\n",
        "    episode_rewards = []\n",
        "    actor.layering()\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        state = state.flatten()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        while not done:\n",
        "            action = actor.policynetwork(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            next_state = next_state.flatten()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if mode == 'train':\n",
        "                experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                if len(experiences) == batch_size:\n",
        "                    critic.valuenetwork(experiences)\n",
        "                    experiences = []\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            actor.integration()\n",
        "            actor.layering()\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a7f31d-9166-4c84-db05-c1ba941f7b98"
      },
      "source": [
        "initial_money=1000000\n",
        "mode = 'train'\n",
        "episodes_times = 25\n",
        "batch_size = 32\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "if mode == 'test':\n",
        "    masterbrain.load(f'{models_folder}/a3c_model.h5')\n",
        "\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "        print(row, file=f)\n",
        "else:\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit'\n",
        "        print(row, file=f)\n",
        "\n",
        "thred_num = 4\n",
        "envs = []\n",
        "for i in range(thred_num):\n",
        "    e = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    a = Actor(brain)\n",
        "    c = Critic(model)\n",
        "    s = make_scaler(e)\n",
        "    arr = [e,a,c,s]\n",
        "    envs.append(arr)\n",
        "\n",
        "datas = []\n",
        "with PoolExecutor(max_workers=thred_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: play_game(env[0],env[1],env[2],env[3], episodes_times, batch_size, mode)\n",
        "        datas.append(executor.submit(job))\n",
        "\n",
        "if mode == 'train':\n",
        "    masterbrain.save(f'{models_folder}/a3c_model.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/25 RapTime: 0:00:07.880543 FixedProfit: 1134010\n",
            "Episode: 1/25 RapTime: 0:00:07.899008 FixedProfit: 1159249\n",
            "Episode: 1/25 RapTime: 0:00:07.908316 FixedProfit: 1002954\n",
            "Episode: 1/25 RapTime: 0:00:07.931106 FixedProfit: 1042125\n",
            "Episode: 2/25 RapTime: 0:00:07.578910 FixedProfit: 1122961\n",
            "Episode: 2/25 RapTime: 0:00:07.619365 FixedProfit: 1568004\n",
            "Episode: 2/25 RapTime: 0:00:07.618748 FixedProfit: 988128\n",
            "Episode: 2/25 RapTime: 0:00:07.672180 FixedProfit: 1286219\n",
            "Episode: 3/25 RapTime: 0:00:07.525002 FixedProfit: 1041293\n",
            "Episode: 3/25 RapTime: 0:00:07.539673 FixedProfit: 1095002\n",
            "Episode: 3/25 RapTime: 0:00:07.550147 FixedProfit: 1158133\n",
            "Episode: 3/25 RapTime: 0:00:07.630647 FixedProfit: 1031749\n",
            "Episode: 4/25 RapTime: 0:00:07.582675 FixedProfit: 1036266\n",
            "Episode: 4/25 RapTime: 0:00:07.649023 FixedProfit: 1009812\n",
            "Episode: 4/25 RapTime: 0:00:07.630961 FixedProfit: 1123080\n",
            "Episode: 4/25 RapTime: 0:00:07.683120 FixedProfit: 963235\n",
            "Episode: 5/25 RapTime: 0:00:07.565025 FixedProfit: 1072901\n",
            "Episode: 5/25 RapTime: 0:00:07.560886 FixedProfit: 1088590\n",
            "Episode: 5/25 RapTime: 0:00:07.558899 FixedProfit: 1201761\n",
            "Episode: 5/25 RapTime: 0:00:07.583851 FixedProfit: 1076876\n",
            "Episode: 6/25 RapTime: 0:00:07.996798 FixedProfit: 1124943\n",
            "Episode: 6/25 RapTime: 0:00:08.043998 FixedProfit: 1152131\n",
            "Episode: 6/25 RapTime: 0:00:08.043937 FixedProfit: 1238282\n",
            "Episode: 6/25 RapTime: 0:00:08.104054 FixedProfit: 1061291\n",
            "Episode: 7/25 RapTime: 0:00:07.627536 FixedProfit: 1207886\n",
            "Episode: 7/25 RapTime: 0:00:07.717801 FixedProfit: 985028\n",
            "Episode: 7/25 RapTime: 0:00:07.610986 FixedProfit: 1197165\n",
            "Episode: 7/25 RapTime: 0:00:07.725584 FixedProfit: 1060538\n",
            "Episode: 8/25 RapTime: 0:00:07.583228 FixedProfit: 1122540\n",
            "Episode: 8/25 RapTime: 0:00:07.561625 FixedProfit: 1160415\n",
            "Episode: 8/25 RapTime: 0:00:07.578478 FixedProfit: 954470\n",
            "Episode: 8/25 RapTime: 0:00:07.570865 FixedProfit: 1199047\n",
            "Episode: 9/25 RapTime: 0:00:07.630031 FixedProfit: 1280810\n",
            "Episode: 9/25 RapTime: 0:00:07.565054 FixedProfit: 1233952\n",
            "Episode: 9/25 RapTime: 0:00:07.667110 FixedProfit: 1109224\n",
            "Episode: 9/25 RapTime: 0:00:07.636622 FixedProfit: 1156941\n",
            "Episode: 10/25 RapTime: 0:00:07.575099 FixedProfit: 1139060\n",
            "Episode: 10/25 RapTime: 0:00:07.600516 FixedProfit: 1168695\n",
            "Episode: 10/25 RapTime: 0:00:07.603729 FixedProfit: 1037355\n",
            "Episode: 10/25 RapTime: 0:00:07.597217 FixedProfit: 978760\n",
            "Episode: 11/25 RapTime: 0:00:07.873872 FixedProfit: 1225707\n",
            "Episode: 11/25 RapTime: 0:00:07.896249 FixedProfit: 1231712\n",
            "Episode: 11/25 RapTime: 0:00:08.001679 FixedProfit: 1132095\n",
            "Episode: 11/25 RapTime: 0:00:07.984573 FixedProfit: 1094113\n",
            "Episode: 12/25 RapTime: 0:00:07.836033 FixedProfit: 1131096\n",
            "Episode: 12/25 RapTime: 0:00:07.851278 FixedProfit: 1287177\n",
            "Episode: 12/25 RapTime: 0:00:07.820295 FixedProfit: 1270905\n",
            "Episode: 12/25 RapTime: 0:00:07.871334 FixedProfit: 1159155\n",
            "Episode: 13/25 RapTime: 0:00:07.396671 FixedProfit: 1207518\n",
            "Episode: 13/25 RapTime: 0:00:07.435519 FixedProfit: 1147477\n",
            "Episode: 13/25 RapTime: 0:00:07.379442 FixedProfit: 1120931\n",
            "Episode: 13/25 RapTime: 0:00:07.443790 FixedProfit: 1115879\n",
            "Episode: 14/25 RapTime: 0:00:07.377948 FixedProfit: 1269447\n",
            "Episode: 14/25 RapTime: 0:00:07.369763 FixedProfit: 1200490\n",
            "Episode: 14/25 RapTime: 0:00:07.361499 FixedProfit: 1247753\n",
            "Episode: 14/25 RapTime: 0:00:07.397221 FixedProfit: 1080166\n",
            "Episode: 15/25 RapTime: 0:00:07.378155 FixedProfit: 1194849\n",
            "Episode: 15/25 RapTime: 0:00:07.335228 FixedProfit: 1197165\n",
            "Episode: 15/25 RapTime: 0:00:07.294344 FixedProfit: 1147055\n",
            "Episode: 15/25 RapTime: 0:00:07.340072 FixedProfit: 1186982\n",
            "Episode: 16/25 RapTime: 0:00:07.418154 FixedProfit: 1208128\n",
            "Episode: 16/25 RapTime: 0:00:07.453504 FixedProfit: 1197165\n",
            "Episode: 16/25 RapTime: 0:00:07.463165 FixedProfit: 1056817\n",
            "Episode: 16/25 RapTime: 0:00:07.463834 FixedProfit: 1256860\n",
            "Episode: 17/25 RapTime: 0:00:07.453781 FixedProfit: 1197165\n",
            "Episode: 17/25 RapTime: 0:00:07.525277 FixedProfit: 1197165\n",
            "Episode: 17/25 RapTime: 0:00:07.450451 FixedProfit: 1116780\n",
            "Episode: 17/25 RapTime: 0:00:07.472808 FixedProfit: 1164855\n",
            "Episode: 18/25 RapTime: 0:00:07.357171 FixedProfit: 1208128\n",
            "Episode: 18/25 RapTime: 0:00:07.400608 FixedProfit: 1197165\n",
            "Episode: 18/25 RapTime: 0:00:07.377421 FixedProfit: 1206951\n",
            "Episode: 18/25 RapTime: 0:00:07.363263 FixedProfit: 1402623\n",
            "Episode: 19/25 RapTime: 0:00:07.387967 FixedProfit: 1183321\n",
            "Episode: 19/25 RapTime: 0:00:07.431329 FixedProfit: 1197165\n",
            "Episode: 19/25 RapTime: 0:00:07.389296 FixedProfit: 1159821\n",
            "Episode: 19/25 RapTime: 0:00:07.402961 FixedProfit: 1144125\n",
            "Episode: 20/25 RapTime: 0:00:07.370234 FixedProfit: 1206916\n",
            "Episode: 20/25 RapTime: 0:00:07.368788 FixedProfit: 1362976\n",
            "Episode: 20/25 RapTime: 0:00:07.400833 FixedProfit: 1197165\n",
            "Episode: 20/25 RapTime: 0:00:07.349060 FixedProfit: 1198079\n",
            "Episode: 21/25 RapTime: 0:00:07.427737 FixedProfit: 1197165\n",
            "Episode: 21/25 RapTime: 0:00:07.418805 FixedProfit: 931739\n",
            "Episode: 21/25 RapTime: 0:00:07.499437 FixedProfit: 1197165\n",
            "Episode: 21/25 RapTime: 0:00:07.530148 FixedProfit: 1226769\n",
            "Episode: 22/25 RapTime: 0:00:07.568895 FixedProfit: 1183321\n",
            "Episode: 22/25 RapTime: 0:00:07.510659 FixedProfit: 1166020\n",
            "Episode: 22/25 RapTime: 0:00:07.509725 FixedProfit: 1197165\n",
            "Episode: 22/25 RapTime: 0:00:07.493645 FixedProfit: 1448091\n",
            "Episode: 23/25 RapTime: 0:00:07.355603 FixedProfit: 1194439\n",
            "Episode: 23/25 RapTime: 0:00:07.331413 FixedProfit: 988027\n",
            "Episode: 23/25 RapTime: 0:00:07.383846 FixedProfit: 1197165\n",
            "Episode: 23/25 RapTime: 0:00:07.343642 FixedProfit: 1321087\n",
            "Episode: 24/25 RapTime: 0:00:07.333345 FixedProfit: 1173059\n",
            "Episode: 24/25 RapTime: 0:00:07.433409 FixedProfit: 1197165\n",
            "Episode: 24/25 RapTime: 0:00:07.427441 FixedProfit: 1197165\n",
            "Episode: 24/25 RapTime: 0:00:07.453152 FixedProfit: 1384043\n",
            "Episode: 25/25 RapTime: 0:00:07.328384 FixedProfit: 1208128\n",
            "Episode: 25/25 RapTime: 0:00:07.272212 FixedProfit: 1197165\n",
            "Episode: 25/25 RapTime: 0:00:07.216605 FixedProfit: 1197165\n",
            "Episode: 25/25 RapTime: 0:00:06.891477 FixedProfit: 1305805\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}