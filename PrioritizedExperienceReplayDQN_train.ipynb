{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrioritizedExperienceReplayDQN_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPklhv90qkBnqO4kuRyyDLJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/PrioritizedExperienceReplayDQN_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "081d61ea-f345-4225-91a1-c12462dbadb2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from statistics import mean\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "mode = 'train'\n",
        "name = 'per_dqn'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNPHarYI9zGY"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(3, input_shape=(3,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(3))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(3))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZLHqi3CpnxI"
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.tderrors_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done, tderror):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.tderrors_memory[self.cntr] = tderror\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def findall(self):\n",
        "        return self.states_memory,self.next_states_memory,self.acts_memory,self.rewards_memory,self.done_memory,self.tderrors_memory\n",
        "\n",
        "    def update_memory_tderror(self, val):\n",
        "        self.tderrors_memory = val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, model, memory, max_size=500, batch_size=32):\n",
        "        self.model = model\n",
        "        self.memory = memory\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done, tderror):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done, tderror)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if self.memory.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.model.predict(states)\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def pioritized_experience_replay(self):\n",
        "\n",
        "        sum_ab_tderror = self._absolute_tderror()\n",
        "        td_list = np.random.uniform(0, sum_ab_tderror, self.batch_size)\n",
        "        td_list = np.sort(td_list)\n",
        "\n",
        "        num_np = np.array([], dtype=np.int16)\n",
        "        i, sum_ab_tderror_tmp = 0, 0\n",
        "        states, next_states, actions, rewards, done, tderror = self.memory.findall()\n",
        "        for item in td_list:\n",
        "            while sum_ab_tderror_tmp < item:\n",
        "                sum_ab_tderror_tmp += abs(tderror[i]) + 0.0001\n",
        "                i += 1\n",
        "            num_np = np.append(num_np, i)\n",
        "\n",
        "        states = states[num_np]\n",
        "        next_states = next_states[num_np]\n",
        "        actions = actions[num_np]\n",
        "        rewards = rewards[num_np]\n",
        "        done = done[num_np]\n",
        "\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "        target_full = self.model.predict(states)\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def tderror(self, state, action, reward, next_state, done):\n",
        "        next_action = np.argmax(self.model.predict(next_state)[0])\n",
        "        target = reward + self.gamma * self.model.predict(next_state)[0][next_action]\n",
        "        tderror = target - self.model.predict(state)[0][action]\n",
        "        return tderror\n",
        "\n",
        "    def _absolute_tderror(self):\n",
        "        absolute_tderror = 0\n",
        "        tderror = self.memory.tderrors_memory\n",
        "        for i in range(0, (len(tderror)-1)):\n",
        "            absolute_tderror += abs(tderror[i]) + 0.0001\n",
        "        return absolute_tderror\n",
        "\n",
        "    def update_tderror(self):\n",
        "        states, next_states, acts, rewards, done, tderror = self.memory.findall()\n",
        "        next_action = np.argmax(self.model.predict(next_states)[0])\n",
        "        target = rewards + self.gamma * self.model.predict(next_states)[0][next_action]\n",
        "        TDerror = target - self.model.predict(next_states)[0][acts]\n",
        "        self.memory.update_memory_tderror(TDerror)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        total_reward = [0]\n",
        "        #total_reward = [1000000]\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    tderror = self.agent.tderror(state, action, reward, next_state, done)\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done, tderror)\n",
        "\n",
        "                    if mean(total_reward) < 1050000:\n",
        "                        self.agent.replay()\n",
        "                    else:\n",
        "                        self.agent.pioritized_experience_replay()\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                self.agent.update_tderror()\n",
        "                total_reward.append(info['cur_revenue'])\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.agent.save('{}/{}.h5'.format(self.mdl_dir, self.name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "c4544389-d34b-4234-8cd1-a05f266ec99b"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 200\n",
        "batch_size = 32\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "\n",
        "brain = Brain()\n",
        "model = brain.model\n",
        "memory = Memory()\n",
        "agent = Agent(model, memory, max_size, batch_size)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/200 RapTime: 0:02:26.553354 FixedProfit: 1223032\n",
            "Episode: 2/200 RapTime: 0:02:34.845952 FixedProfit: 1196397\n",
            "Episode: 3/200 RapTime: 0:02:35.253997 FixedProfit: 1200877\n",
            "Episode: 4/200 RapTime: 0:02:35.643407 FixedProfit: 1201474\n",
            "Episode: 5/200 RapTime: 0:02:36.040615 FixedProfit: 1195453\n",
            "Episode: 6/200 RapTime: 0:02:33.420788 FixedProfit: 1198318\n",
            "Episode: 7/200 RapTime: 0:02:32.935538 FixedProfit: 1210374\n",
            "Episode: 8/200 RapTime: 0:02:33.907476 FixedProfit: 1219569\n",
            "Episode: 9/200 RapTime: 0:02:34.219125 FixedProfit: 1182617\n",
            "Episode: 10/200 RapTime: 0:02:35.155485 FixedProfit: 1196745\n",
            "Episode: 11/200 RapTime: 0:02:34.832614 FixedProfit: 1208778\n",
            "Episode: 12/200 RapTime: 0:02:36.029018 FixedProfit: 1197165\n",
            "Episode: 13/200 RapTime: 0:02:34.904183 FixedProfit: 1201223\n",
            "Episode: 14/200 RapTime: 0:02:34.573215 FixedProfit: 1189578\n",
            "Episode: 15/200 RapTime: 0:02:34.401121 FixedProfit: 1189626\n",
            "Episode: 16/200 RapTime: 0:02:35.486812 FixedProfit: 1198957\n",
            "Episode: 17/200 RapTime: 0:02:35.618743 FixedProfit: 1177635\n",
            "Episode: 18/200 RapTime: 0:02:35.127112 FixedProfit: 1197868\n",
            "Episode: 19/200 RapTime: 0:02:35.747603 FixedProfit: 1166655\n",
            "Episode: 20/200 RapTime: 0:02:35.588431 FixedProfit: 1197165\n",
            "Episode: 21/200 RapTime: 0:02:35.582876 FixedProfit: 1201236\n",
            "Episode: 22/200 RapTime: 0:02:36.204428 FixedProfit: 1199793\n",
            "Episode: 23/200 RapTime: 0:02:36.308086 FixedProfit: 1167846\n",
            "Episode: 24/200 RapTime: 0:02:35.956282 FixedProfit: 1214955\n",
            "Episode: 25/200 RapTime: 0:02:35.907732 FixedProfit: 1183491\n",
            "Episode: 26/200 RapTime: 0:02:38.334629 FixedProfit: 1202025\n",
            "Episode: 27/200 RapTime: 0:02:37.278569 FixedProfit: 1198241\n",
            "Episode: 28/200 RapTime: 0:02:33.214952 FixedProfit: 1200835\n",
            "Episode: 29/200 RapTime: 0:02:33.092587 FixedProfit: 1197165\n",
            "Episode: 30/200 RapTime: 0:02:31.979025 FixedProfit: 1185936\n",
            "Episode: 31/200 RapTime: 0:02:32.813388 FixedProfit: 1214115\n",
            "Episode: 32/200 RapTime: 0:02:32.386566 FixedProfit: 1210960\n",
            "Episode: 33/200 RapTime: 0:02:30.488738 FixedProfit: 1191843\n",
            "Episode: 34/200 RapTime: 0:02:26.382311 FixedProfit: 1195874\n",
            "Episode: 35/200 RapTime: 0:02:27.521735 FixedProfit: 1206686\n",
            "Episode: 36/200 RapTime: 0:02:30.696163 FixedProfit: 1186722\n",
            "Episode: 37/200 RapTime: 0:02:28.139272 FixedProfit: 1197165\n",
            "Episode: 38/200 RapTime: 0:02:26.278815 FixedProfit: 1216237\n",
            "Episode: 39/200 RapTime: 0:02:26.782170 FixedProfit: 1229385\n",
            "Episode: 40/200 RapTime: 0:02:27.130657 FixedProfit: 1197165\n",
            "Episode: 41/200 RapTime: 0:02:26.925381 FixedProfit: 1197165\n",
            "Episode: 42/200 RapTime: 0:02:25.186638 FixedProfit: 1188307\n",
            "Episode: 43/200 RapTime: 0:02:27.450938 FixedProfit: 1187111\n",
            "Episode: 44/200 RapTime: 0:02:28.597421 FixedProfit: 1197165\n",
            "Episode: 45/200 RapTime: 0:02:26.955817 FixedProfit: 1197792\n",
            "Episode: 46/200 RapTime: 0:02:31.044536 FixedProfit: 1169387\n",
            "Episode: 47/200 RapTime: 0:02:34.142431 FixedProfit: 1188099\n",
            "Episode: 48/200 RapTime: 0:02:34.515094 FixedProfit: 1191458\n",
            "Episode: 49/200 RapTime: 0:02:26.986607 FixedProfit: 1233246\n",
            "Episode: 50/200 RapTime: 0:02:27.443840 FixedProfit: 1199708\n",
            "Episode: 51/200 RapTime: 0:02:27.182715 FixedProfit: 1181311\n",
            "Episode: 52/200 RapTime: 0:02:27.213312 FixedProfit: 1197165\n",
            "Episode: 53/200 RapTime: 0:02:29.197392 FixedProfit: 1172351\n",
            "Episode: 54/200 RapTime: 0:02:27.698133 FixedProfit: 1183569\n",
            "Episode: 55/200 RapTime: 0:02:27.478568 FixedProfit: 1184007\n",
            "Episode: 56/200 RapTime: 0:02:26.825235 FixedProfit: 1193773\n",
            "Episode: 57/200 RapTime: 0:02:26.473585 FixedProfit: 1197165\n",
            "Episode: 58/200 RapTime: 0:02:28.190183 FixedProfit: 1177644\n",
            "Episode: 59/200 RapTime: 0:02:27.014592 FixedProfit: 1197165\n",
            "Episode: 60/200 RapTime: 0:02:27.134578 FixedProfit: 1200692\n",
            "Episode: 61/200 RapTime: 0:02:26.894174 FixedProfit: 1199288\n",
            "Episode: 62/200 RapTime: 0:02:28.408447 FixedProfit: 1201718\n",
            "Episode: 63/200 RapTime: 0:02:27.081312 FixedProfit: 1168135\n",
            "Episode: 64/200 RapTime: 0:02:27.758369 FixedProfit: 1202284\n",
            "Episode: 65/200 RapTime: 0:02:30.564736 FixedProfit: 1197165\n",
            "Episode: 66/200 RapTime: 0:02:34.753281 FixedProfit: 1229443\n",
            "Episode: 67/200 RapTime: 0:02:35.276355 FixedProfit: 1201112\n",
            "Episode: 68/200 RapTime: 0:02:35.150421 FixedProfit: 1201873\n",
            "Episode: 69/200 RapTime: 0:02:35.435566 FixedProfit: 1197165\n",
            "Episode: 70/200 RapTime: 0:02:34.805034 FixedProfit: 1189854\n",
            "Episode: 71/200 RapTime: 0:02:35.953015 FixedProfit: 1167995\n",
            "Episode: 72/200 RapTime: 0:02:36.089426 FixedProfit: 1191925\n",
            "Episode: 73/200 RapTime: 0:02:34.622796 FixedProfit: 1198291\n",
            "Episode: 74/200 RapTime: 0:02:36.539376 FixedProfit: 1184784\n",
            "Episode: 75/200 RapTime: 0:02:37.832080 FixedProfit: 1232565\n",
            "Episode: 76/200 RapTime: 0:02:36.731311 FixedProfit: 1198273\n",
            "Episode: 77/200 RapTime: 0:02:34.269491 FixedProfit: 1223721\n",
            "Episode: 78/200 RapTime: 0:02:34.847531 FixedProfit: 1197803\n",
            "Episode: 79/200 RapTime: 0:02:34.689797 FixedProfit: 1195416\n",
            "Episode: 80/200 RapTime: 0:02:34.390637 FixedProfit: 1187132\n",
            "Episode: 81/200 RapTime: 0:02:34.842509 FixedProfit: 1199264\n",
            "Episode: 82/200 RapTime: 0:02:35.381432 FixedProfit: 1218681\n",
            "Episode: 83/200 RapTime: 0:02:35.518088 FixedProfit: 1214115\n",
            "Episode: 84/200 RapTime: 0:02:35.642559 FixedProfit: 1193937\n",
            "Episode: 85/200 RapTime: 0:02:35.522659 FixedProfit: 1205658\n",
            "Episode: 86/200 RapTime: 0:02:35.179426 FixedProfit: 1182467\n",
            "Episode: 87/200 RapTime: 0:02:35.409382 FixedProfit: 1200649\n",
            "Episode: 88/200 RapTime: 0:02:32.263238 FixedProfit: 1165622\n",
            "Episode: 89/200 RapTime: 0:02:32.326480 FixedProfit: 1201355\n",
            "Episode: 90/200 RapTime: 0:02:31.288039 FixedProfit: 1182612\n",
            "Episode: 91/200 RapTime: 0:02:33.211758 FixedProfit: 1184944\n",
            "Episode: 92/200 RapTime: 0:02:33.354925 FixedProfit: 1197699\n",
            "Episode: 93/200 RapTime: 0:02:33.956566 FixedProfit: 1217875\n",
            "Episode: 94/200 RapTime: 0:02:33.791365 FixedProfit: 1189062\n",
            "Episode: 95/200 RapTime: 0:02:34.036691 FixedProfit: 1202894\n",
            "Episode: 96/200 RapTime: 0:02:34.729270 FixedProfit: 1200596\n",
            "Episode: 97/200 RapTime: 0:02:34.058688 FixedProfit: 1205361\n",
            "Episode: 98/200 RapTime: 0:02:39.368361 FixedProfit: 1207071\n",
            "Episode: 99/200 RapTime: 0:02:34.986595 FixedProfit: 1198134\n",
            "Episode: 100/200 RapTime: 0:02:32.527873 FixedProfit: 1203013\n",
            "Episode: 101/200 RapTime: 0:02:33.999769 FixedProfit: 1192527\n",
            "Episode: 102/200 RapTime: 0:02:34.287750 FixedProfit: 1203408\n",
            "Episode: 103/200 RapTime: 0:02:34.392027 FixedProfit: 1191209\n",
            "Episode: 104/200 RapTime: 0:02:32.792804 FixedProfit: 1191121\n",
            "Episode: 105/200 RapTime: 0:02:33.623363 FixedProfit: 1186456\n",
            "Episode: 106/200 RapTime: 0:02:34.959704 FixedProfit: 1200693\n",
            "Episode: 107/200 RapTime: 0:02:34.503792 FixedProfit: 1194793\n",
            "Episode: 108/200 RapTime: 0:02:32.908921 FixedProfit: 1204041\n",
            "Episode: 109/200 RapTime: 0:02:33.947194 FixedProfit: 1200895\n",
            "Episode: 110/200 RapTime: 0:02:32.389596 FixedProfit: 1198331\n",
            "Episode: 111/200 RapTime: 0:02:34.016356 FixedProfit: 1209928\n",
            "Episode: 112/200 RapTime: 0:02:34.617507 FixedProfit: 1186457\n",
            "Episode: 113/200 RapTime: 0:02:34.334474 FixedProfit: 1190281\n",
            "Episode: 114/200 RapTime: 0:02:34.237391 FixedProfit: 1192913\n",
            "Episode: 115/200 RapTime: 0:02:35.147638 FixedProfit: 1181710\n",
            "Episode: 116/200 RapTime: 0:02:33.962024 FixedProfit: 1200486\n",
            "Episode: 117/200 RapTime: 0:02:33.926473 FixedProfit: 1144969\n",
            "Episode: 118/200 RapTime: 0:02:34.523331 FixedProfit: 1194527\n",
            "Episode: 119/200 RapTime: 0:02:34.309528 FixedProfit: 1172636\n",
            "Episode: 120/200 RapTime: 0:02:34.022391 FixedProfit: 1218099\n",
            "Episode: 121/200 RapTime: 0:02:34.510729 FixedProfit: 1191169\n",
            "Episode: 122/200 RapTime: 0:02:34.084568 FixedProfit: 1187633\n",
            "Episode: 123/200 RapTime: 0:02:33.759259 FixedProfit: 1193490\n",
            "Episode: 124/200 RapTime: 0:02:33.756003 FixedProfit: 1209857\n",
            "Episode: 125/200 RapTime: 0:02:33.410590 FixedProfit: 1213793\n",
            "Episode: 126/200 RapTime: 0:02:32.373143 FixedProfit: 1169936\n",
            "Episode: 127/200 RapTime: 0:02:34.387767 FixedProfit: 1209850\n",
            "Episode: 128/200 RapTime: 0:02:34.184924 FixedProfit: 1195014\n",
            "Episode: 129/200 RapTime: 0:02:35.178224 FixedProfit: 1197165\n",
            "Episode: 130/200 RapTime: 0:02:34.394527 FixedProfit: 1211985\n",
            "Episode: 131/200 RapTime: 0:02:35.271206 FixedProfit: 1197116\n",
            "Episode: 132/200 RapTime: 0:02:34.140221 FixedProfit: 1197165\n",
            "Episode: 133/200 RapTime: 0:02:34.719109 FixedProfit: 1209057\n",
            "Episode: 134/200 RapTime: 0:02:34.066300 FixedProfit: 1184630\n",
            "Episode: 135/200 RapTime: 0:02:34.524506 FixedProfit: 1203315\n",
            "Episode: 136/200 RapTime: 0:02:34.518942 FixedProfit: 1169986\n",
            "Episode: 137/200 RapTime: 0:02:35.494738 FixedProfit: 1166958\n",
            "Episode: 138/200 RapTime: 0:02:35.345836 FixedProfit: 1200895\n",
            "Episode: 139/200 RapTime: 0:02:35.385620 FixedProfit: 1184213\n",
            "Episode: 140/200 RapTime: 0:02:35.404152 FixedProfit: 1201706\n",
            "Episode: 141/200 RapTime: 0:02:35.744090 FixedProfit: 1184419\n",
            "Episode: 142/200 RapTime: 0:02:34.350119 FixedProfit: 1176405\n",
            "Episode: 143/200 RapTime: 0:02:32.897429 FixedProfit: 1171151\n",
            "Episode: 144/200 RapTime: 0:02:33.162093 FixedProfit: 1197165\n",
            "Episode: 145/200 RapTime: 0:02:34.411642 FixedProfit: 1196523\n",
            "Episode: 146/200 RapTime: 0:02:34.797877 FixedProfit: 1220484\n",
            "Episode: 147/200 RapTime: 0:02:34.807805 FixedProfit: 1166860\n",
            "Episode: 148/200 RapTime: 0:02:34.610053 FixedProfit: 1213756\n",
            "Episode: 149/200 RapTime: 0:02:35.031053 FixedProfit: 1210062\n",
            "Episode: 150/200 RapTime: 0:02:34.515835 FixedProfit: 1209582\n",
            "Episode: 151/200 RapTime: 0:02:35.815926 FixedProfit: 1182496\n",
            "Episode: 152/200 RapTime: 0:02:35.196274 FixedProfit: 1190733\n",
            "Episode: 153/200 RapTime: 0:02:35.583835 FixedProfit: 1205396\n",
            "Episode: 154/200 RapTime: 0:02:35.404240 FixedProfit: 1168732\n",
            "Episode: 155/200 RapTime: 0:02:34.358150 FixedProfit: 1179241\n",
            "Episode: 156/200 RapTime: 0:02:34.020396 FixedProfit: 1193080\n",
            "Episode: 157/200 RapTime: 0:02:34.240181 FixedProfit: 1199458\n",
            "Episode: 158/200 RapTime: 0:02:34.135135 FixedProfit: 1197165\n",
            "Episode: 159/200 RapTime: 0:02:34.708220 FixedProfit: 1222963\n",
            "Episode: 160/200 RapTime: 0:02:34.188868 FixedProfit: 1197165\n",
            "Episode: 161/200 RapTime: 0:02:34.589006 FixedProfit: 1206485\n",
            "Episode: 162/200 RapTime: 0:02:34.476672 FixedProfit: 1196894\n",
            "Episode: 163/200 RapTime: 0:02:34.908007 FixedProfit: 1190962\n",
            "Episode: 164/200 RapTime: 0:02:35.176050 FixedProfit: 1197165\n",
            "Episode: 165/200 RapTime: 0:02:35.304505 FixedProfit: 1207790\n",
            "Episode: 166/200 RapTime: 0:02:34.671245 FixedProfit: 1204377\n",
            "Episode: 167/200 RapTime: 0:02:34.741946 FixedProfit: 1206145\n",
            "Episode: 168/200 RapTime: 0:02:34.363800 FixedProfit: 1218059\n",
            "Episode: 169/200 RapTime: 0:02:34.637566 FixedProfit: 1207601\n",
            "Episode: 170/200 RapTime: 0:02:34.966012 FixedProfit: 1190309\n",
            "Episode: 171/200 RapTime: 0:02:34.395271 FixedProfit: 1197972\n",
            "Episode: 172/200 RapTime: 0:02:34.098562 FixedProfit: 1197587\n",
            "Episode: 173/200 RapTime: 0:02:33.343853 FixedProfit: 1206175\n",
            "Episode: 174/200 RapTime: 0:02:33.075152 FixedProfit: 1185382\n",
            "Episode: 175/200 RapTime: 0:02:34.341351 FixedProfit: 1188128\n",
            "Episode: 176/200 RapTime: 0:02:34.193384 FixedProfit: 1195892\n",
            "Episode: 177/200 RapTime: 0:02:34.049992 FixedProfit: 1194020\n",
            "Episode: 178/200 RapTime: 0:02:34.029425 FixedProfit: 1200705\n",
            "Episode: 179/200 RapTime: 0:02:34.655536 FixedProfit: 1197165\n",
            "Episode: 180/200 RapTime: 0:02:33.949629 FixedProfit: 1174914\n",
            "Episode: 181/200 RapTime: 0:02:34.146424 FixedProfit: 1197165\n",
            "Episode: 182/200 RapTime: 0:02:34.237037 FixedProfit: 1217257\n",
            "Episode: 183/200 RapTime: 0:02:34.671822 FixedProfit: 1165729\n",
            "Episode: 184/200 RapTime: 0:02:31.774464 FixedProfit: 1197811\n",
            "Episode: 185/200 RapTime: 0:02:31.630745 FixedProfit: 1197779\n",
            "Episode: 186/200 RapTime: 0:02:30.077271 FixedProfit: 1196736\n",
            "Episode: 187/200 RapTime: 0:02:29.467652 FixedProfit: 1215450\n",
            "Episode: 188/200 RapTime: 0:02:30.354572 FixedProfit: 1197165\n",
            "Episode: 189/200 RapTime: 0:02:30.879407 FixedProfit: 1149954\n",
            "Episode: 190/200 RapTime: 0:02:30.406056 FixedProfit: 1189466\n",
            "Episode: 191/200 RapTime: 0:02:30.919927 FixedProfit: 1199219\n",
            "Episode: 192/200 RapTime: 0:02:30.767848 FixedProfit: 1192996\n",
            "Episode: 193/200 RapTime: 0:02:31.013346 FixedProfit: 1197165\n",
            "Episode: 194/200 RapTime: 0:02:31.513089 FixedProfit: 1208738\n",
            "Episode: 195/200 RapTime: 0:02:32.322079 FixedProfit: 1197165\n",
            "Episode: 196/200 RapTime: 0:02:34.672266 FixedProfit: 1195446\n",
            "Episode: 197/200 RapTime: 0:02:34.912038 FixedProfit: 1197165\n",
            "Episode: 198/200 RapTime: 0:02:35.042988 FixedProfit: 1228589\n",
            "Episode: 199/200 RapTime: 0:02:36.589621 FixedProfit: 1182095\n",
            "Episode: 200/200 RapTime: 0:02:35.841703 FixedProfit: 1197165\n"
          ]
        }
      ]
    }
  ]
}