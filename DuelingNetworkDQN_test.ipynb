{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DuelingNetworkDQN_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOQgpp2fVmYV3+3dMowLI+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/DuelingNetworkDQN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "f36bfaba-ba36-467f-d8be-8c2153c51878"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU, Conv1D, Input, Lambda\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "mode = 'test'\n",
        "name = 'dn_dqn'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        learning_rate = 0.00001\n",
        "\n",
        "        input = Input(shape=(3,))\n",
        "        common = Dense(15, activation='relu')(input)\n",
        "        common = Dense(15, activation='relu')(common)\n",
        "\n",
        "        common = Dense(4, activation='linear')(common)\n",
        "        output = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - 0.0*K.mean(a[:, 1:], keepdims=True),output_shape=(3,))(common)\n",
        "\n",
        "        model = keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        model.compile(loss=Huber(), optimizer=optimizer)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent(Brain, ReplayMemory):\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        Brain.__init__(self)\n",
        "        ReplayMemory.__init__(self, max_size, batch_size)\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if self.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.model.predict(states)\n",
        "\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                    self.agent.replay()                \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.agent.save('{}/{}.h5'.format(self.mdl_dir, self.name))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "08b4dc9a-7215-4342-d447-ece9188fabd7"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "agent = Agent(max_size, batch_size)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 15)                60        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 64        \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 364\n",
            "Trainable params: 364\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:37.301438 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/100 RapTime: 0:00:35.380102 FixedProfit: 1016693 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 3/100 RapTime: 0:00:35.526637 FixedProfit: 1002346 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 4/100 RapTime: 0:00:35.370137 FixedProfit: 1012728 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 5/100 RapTime: 0:00:35.726699 FixedProfit: 992044 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 6/100 RapTime: 0:00:35.106036 FixedProfit: 1002186 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 7/100 RapTime: 0:00:35.278396 FixedProfit: 985708 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 8/100 RapTime: 0:00:35.337580 FixedProfit: 1008510 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 9/100 RapTime: 0:00:35.432518 FixedProfit: 1001687 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 10/100 RapTime: 0:00:35.138413 FixedProfit: 1003606 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 11/100 RapTime: 0:00:35.412850 FixedProfit: 1008342 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 12/100 RapTime: 0:00:35.370222 FixedProfit: 1000123 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 13/100 RapTime: 0:00:35.320459 FixedProfit: 1007306 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 14/100 RapTime: 0:00:35.019765 FixedProfit: 1015902 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 15/100 RapTime: 0:00:35.389207 FixedProfit: 1008682 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 16/100 RapTime: 0:00:34.806462 FixedProfit: 988167 TradeTimes: 7 TradeWin: 4\n",
            "Episode: 17/100 RapTime: 0:00:34.993400 FixedProfit: 1032865 TradeTimes: 9 TradeWin: 7\n",
            "Episode: 18/100 RapTime: 0:00:35.005293 FixedProfit: 983947 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 19/100 RapTime: 0:00:35.006553 FixedProfit: 1008375 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 20/100 RapTime: 0:00:35.405073 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/100 RapTime: 0:00:34.645966 FixedProfit: 989711 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 22/100 RapTime: 0:00:35.064037 FixedProfit: 1038426 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 23/100 RapTime: 0:00:34.943253 FixedProfit: 1014150 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 24/100 RapTime: 0:00:35.134119 FixedProfit: 998769 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 25/100 RapTime: 0:00:36.126801 FixedProfit: 997249 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 26/100 RapTime: 0:00:37.165304 FixedProfit: 996870 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 27/100 RapTime: 0:00:37.120955 FixedProfit: 950930 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 28/100 RapTime: 0:00:37.368472 FixedProfit: 999104 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 29/100 RapTime: 0:00:36.851131 FixedProfit: 1009668 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 30/100 RapTime: 0:00:37.222234 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 31/100 RapTime: 0:00:36.918934 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 32/100 RapTime: 0:00:37.245544 FixedProfit: 1017407 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 33/100 RapTime: 0:00:36.741222 FixedProfit: 973962 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 34/100 RapTime: 0:00:36.863692 FixedProfit: 1003482 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 35/100 RapTime: 0:00:36.802156 FixedProfit: 1021560 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 36/100 RapTime: 0:00:37.027585 FixedProfit: 1009921 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 37/100 RapTime: 0:00:37.099064 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 38/100 RapTime: 0:00:37.077277 FixedProfit: 1003365 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 39/100 RapTime: 0:00:36.567857 FixedProfit: 997257 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 40/100 RapTime: 0:00:36.696101 FixedProfit: 1011978 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 41/100 RapTime: 0:00:36.691591 FixedProfit: 1013779 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 42/100 RapTime: 0:00:36.974363 FixedProfit: 1025494 TradeTimes: 7 TradeWin: 4\n",
            "Episode: 43/100 RapTime: 0:00:37.032312 FixedProfit: 1003817 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 44/100 RapTime: 0:00:37.069274 FixedProfit: 1003920 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 45/100 RapTime: 0:00:36.762626 FixedProfit: 1032297 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 46/100 RapTime: 0:00:36.927905 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 47/100 RapTime: 0:00:37.002100 FixedProfit: 874255 TradeTimes: 4 TradeWin: 0\n",
            "Episode: 48/100 RapTime: 0:00:36.664885 FixedProfit: 1023632 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 49/100 RapTime: 0:00:37.120492 FixedProfit: 963085 TradeTimes: 4 TradeWin: 0\n",
            "Episode: 50/100 RapTime: 0:00:36.973440 FixedProfit: 973590 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 51/100 RapTime: 0:00:37.109323 FixedProfit: 986726 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 52/100 RapTime: 0:00:36.577674 FixedProfit: 994122 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 53/100 RapTime: 0:00:36.832072 FixedProfit: 1041174 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 54/100 RapTime: 0:00:36.998686 FixedProfit: 992634 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 55/100 RapTime: 0:00:37.172338 FixedProfit: 1017771 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 56/100 RapTime: 0:00:36.846395 FixedProfit: 1010660 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 57/100 RapTime: 0:00:36.930884 FixedProfit: 1009608 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 58/100 RapTime: 0:00:36.662910 FixedProfit: 995113 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 59/100 RapTime: 0:00:36.954059 FixedProfit: 1000487 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 60/100 RapTime: 0:00:36.579775 FixedProfit: 977864 TradeTimes: 6 TradeWin: 2\n",
            "Episode: 61/100 RapTime: 0:00:36.839107 FixedProfit: 1096897 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 62/100 RapTime: 0:00:36.759162 FixedProfit: 991994 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 63/100 RapTime: 0:00:37.064782 FixedProfit: 1000445 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 64/100 RapTime: 0:00:36.831770 FixedProfit: 989432 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 65/100 RapTime: 0:00:37.266867 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 66/100 RapTime: 0:00:36.957906 FixedProfit: 1022729 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 67/100 RapTime: 0:00:37.194998 FixedProfit: 980871 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 68/100 RapTime: 0:00:36.839303 FixedProfit: 1004648 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 69/100 RapTime: 0:00:35.535912 FixedProfit: 979650 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 70/100 RapTime: 0:00:35.032593 FixedProfit: 1016232 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 71/100 RapTime: 0:00:35.101979 FixedProfit: 1002186 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 72/100 RapTime: 0:00:35.189367 FixedProfit: 992191 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 73/100 RapTime: 0:00:34.749849 FixedProfit: 959522 TradeTimes: 7 TradeWin: 0\n",
            "Episode: 74/100 RapTime: 0:00:35.227103 FixedProfit: 983293 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 75/100 RapTime: 0:00:35.420450 FixedProfit: 1001189 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 76/100 RapTime: 0:00:35.276206 FixedProfit: 1001233 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 77/100 RapTime: 0:00:34.863982 FixedProfit: 1000711 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 78/100 RapTime: 0:00:35.152380 FixedProfit: 1010412 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 79/100 RapTime: 0:00:35.029611 FixedProfit: 1004758 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 80/100 RapTime: 0:00:34.942438 FixedProfit: 969209 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 81/100 RapTime: 0:00:34.490437 FixedProfit: 958244 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 82/100 RapTime: 0:00:34.778296 FixedProfit: 1005412 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 83/100 RapTime: 0:00:34.335977 FixedProfit: 987535 TradeTimes: 7 TradeWin: 3\n",
            "Episode: 84/100 RapTime: 0:00:34.743528 FixedProfit: 1013590 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 85/100 RapTime: 0:00:34.393728 FixedProfit: 993398 TradeTimes: 4 TradeWin: 0\n",
            "Episode: 86/100 RapTime: 0:00:34.480124 FixedProfit: 996168 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 87/100 RapTime: 0:00:34.180582 FixedProfit: 994185 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 88/100 RapTime: 0:00:34.663615 FixedProfit: 981035 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 89/100 RapTime: 0:00:34.451115 FixedProfit: 1014048 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 90/100 RapTime: 0:00:34.676911 FixedProfit: 995983 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 91/100 RapTime: 0:00:34.456789 FixedProfit: 1006319 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 92/100 RapTime: 0:00:34.402882 FixedProfit: 992553 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 93/100 RapTime: 0:00:34.028296 FixedProfit: 1018866 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 94/100 RapTime: 0:00:34.095292 FixedProfit: 936151 TradeTimes: 4 TradeWin: 0\n",
            "Episode: 95/100 RapTime: 0:00:34.139943 FixedProfit: 986689 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 96/100 RapTime: 0:00:34.531338 FixedProfit: 1019220 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 97/100 RapTime: 0:00:34.104387 FixedProfit: 1012011 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 98/100 RapTime: 0:00:34.311966 FixedProfit: 967845 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 99/100 RapTime: 0:00:34.127119 FixedProfit: 998036 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 100/100 RapTime: 0:00:34.443106 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n"
          ]
        }
      ]
    }
  ]
}