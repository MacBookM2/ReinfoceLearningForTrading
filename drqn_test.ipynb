{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drqn_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMYf8g3OV1iRxuQ8NZh8PkZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/drqn_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "39c3a1a3-9eb4-4f5d-cf2b-5e89b3e7d9b7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU, LSTM, Activation, Input, MaxPool1D, Conv1D\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "mode = 'test'\n",
        "name = 'drqn'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        dice = np.arange(10, self.size)\n",
        "        mb_index = np.random.choice(dice, self.batch_size, replace=False)\n",
        "        mb_index_min = mb_index - 10\n",
        "\n",
        "        states_3d = np.empty((0, 10, 3))\n",
        "        next_states_3d = np.empty((0, 10, 3))\n",
        "        for a,b in zip(mb_index, mb_index_min):\n",
        "            states_tmp = self.states_memory[b:a]\n",
        "            next_states_tmp = self.next_states_memory[b:a]\n",
        "\n",
        "            states_tmp  = np.reshape(states_tmp, (1, 10, 3))\n",
        "            next_states_tmp  = np.reshape(next_states_tmp, (1, 10, 3))\n",
        "\n",
        "            states_3d = np.append(states_3d, states_tmp,axis=0)\n",
        "            next_states_3d = np.append(next_states_3d, next_states_tmp,axis=0)\n",
        "\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [states_3d ,next_states_3d, self.acts_memory[mb_index], self.rewards_memory[mb_index], self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        conv_filter = 12\n",
        "        units = 16\n",
        "        look_back = 10\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\",batch_input_shape=(None, look_back, 3)))\n",
        "        model.add(MaxPool1D(pool_size=1, padding='same'))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(LSTM(units))\n",
        "        model.add(Dense(3, kernel_initializer='random_uniform'))\n",
        "        model.compile(loss = \"mean_absolute_error\", optimizer=opt)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent(Brain, ReplayMemory):\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        self.local_state = np.empty((0,3), float)\n",
        "        Brain.__init__(self)\n",
        "        ReplayMemory.__init__(self, max_size, batch_size)\n",
        "\n",
        "    def reset(self):\n",
        "        self.local_state = np.empty((0,3), float)\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state):\n",
        "        if len(self.local_state) >= 10:\n",
        "            self.local_state = self.local_state[1:]\n",
        "            self.local_state = np.append(self.local_state, np.array(state), axis=0)\n",
        "            tmp_state = copy.deepcopy(self.local_state)\n",
        "            tmp_state  = np.reshape(tmp_state, (1, 10, 3))\n",
        "\n",
        "            if np.random.rand() <= self.epsilon:\n",
        "                return np.random.choice(3)\n",
        "            act_values = self.model.predict(tmp_state)\n",
        "            return np.argmax(act_values[0])\n",
        "        else:\n",
        "            self.local_state = np.append(self.local_state, np.array(state), axis=0)\n",
        "            return np.random.choice(3)\n",
        "\n",
        "    def replay(self):\n",
        "        if self.size < (self.batch_size + 11):\n",
        "            return\n",
        "\n",
        "        m_batch = self.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "        d = self.model.predict(next_states)\n",
        "        c = np.amax(self.model.predict(next_states), axis=1)\n",
        "        target_full = self.model.predict(states)\n",
        "\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "            self.agent.reset()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                    self.agent.replay()                \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.agent.save('{}/{}.h5'.format(self.mdl_dir, self.name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "78497ba2-1754-4759-9b1f-988194539252"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "agent = Agent(max_size, batch_size)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 10, 12)            48        \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 10, 12)            0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10, 12)            0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 16)                1856      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 51        \n",
            "=================================================================\n",
            "Total params: 1,955\n",
            "Trainable params: 1,955\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:53.055393 FixedProfit: 1524977 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 2/100 RapTime: 0:00:37.056046 FixedProfit: 1506431 TradeTimes: 8 TradeWin: 5\n",
            "Episode: 3/100 RapTime: 0:00:37.839912 FixedProfit: 1512761 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 4/100 RapTime: 0:00:37.543241 FixedProfit: 1526041 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 5/100 RapTime: 0:00:38.709917 FixedProfit: 1492300 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 6/100 RapTime: 0:00:37.546164 FixedProfit: 1523309 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 7/100 RapTime: 0:00:37.306971 FixedProfit: 1542679 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 8/100 RapTime: 0:00:37.229539 FixedProfit: 1587303 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 9/100 RapTime: 0:00:36.887197 FixedProfit: 1560410 TradeTimes: 9 TradeWin: 7\n",
            "Episode: 10/100 RapTime: 0:00:37.459263 FixedProfit: 1511262 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 11/100 RapTime: 0:00:36.993578 FixedProfit: 1659239 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 12/100 RapTime: 0:00:37.391505 FixedProfit: 1500549 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 13/100 RapTime: 0:00:36.910232 FixedProfit: 1510266 TradeTimes: 8 TradeWin: 7\n",
            "Episode: 14/100 RapTime: 0:00:37.134381 FixedProfit: 1514391 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 15/100 RapTime: 0:00:37.072968 FixedProfit: 1499518 TradeTimes: 8 TradeWin: 5\n",
            "Episode: 16/100 RapTime: 0:00:36.850343 FixedProfit: 1553154 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 17/100 RapTime: 0:00:37.007720 FixedProfit: 1506255 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 18/100 RapTime: 0:00:37.090564 FixedProfit: 1480940 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 19/100 RapTime: 0:00:37.274529 FixedProfit: 1508095 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 20/100 RapTime: 0:00:36.752645 FixedProfit: 1533219 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 21/100 RapTime: 0:00:37.362217 FixedProfit: 1560849 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 22/100 RapTime: 0:00:36.940771 FixedProfit: 1538789 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 23/100 RapTime: 0:00:37.148425 FixedProfit: 1516115 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 24/100 RapTime: 0:00:37.052811 FixedProfit: 1542383 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 25/100 RapTime: 0:00:37.106224 FixedProfit: 1523151 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 26/100 RapTime: 0:00:37.013984 FixedProfit: 1531848 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 27/100 RapTime: 0:00:37.509448 FixedProfit: 1562527 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 28/100 RapTime: 0:00:37.039787 FixedProfit: 1496719 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 29/100 RapTime: 0:00:37.559471 FixedProfit: 1552689 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 30/100 RapTime: 0:00:37.256169 FixedProfit: 1526101 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 31/100 RapTime: 0:00:37.131258 FixedProfit: 1511462 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 32/100 RapTime: 0:00:36.919044 FixedProfit: 1883888 TradeTimes: 9 TradeWin: 8\n",
            "Episode: 33/100 RapTime: 0:00:36.672004 FixedProfit: 1541821 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 34/100 RapTime: 0:00:37.179808 FixedProfit: 1578139 TradeTimes: 9 TradeWin: 8\n",
            "Episode: 35/100 RapTime: 0:00:37.216126 FixedProfit: 1500341 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 36/100 RapTime: 0:00:37.094796 FixedProfit: 1525575 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 37/100 RapTime: 0:00:36.942725 FixedProfit: 1519069 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 38/100 RapTime: 0:00:37.073126 FixedProfit: 1557264 TradeTimes: 8 TradeWin: 5\n",
            "Episode: 39/100 RapTime: 0:00:37.153257 FixedProfit: 1529112 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 40/100 RapTime: 0:00:37.453299 FixedProfit: 1533104 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 41/100 RapTime: 0:00:37.162084 FixedProfit: 1706642 TradeTimes: 9 TradeWin: 6\n",
            "Episode: 42/100 RapTime: 0:00:37.062979 FixedProfit: 1509050 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 43/100 RapTime: 0:00:37.274870 FixedProfit: 1552057 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 44/100 RapTime: 0:00:36.927039 FixedProfit: 1474779 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 45/100 RapTime: 0:00:37.233052 FixedProfit: 1505762 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 46/100 RapTime: 0:00:36.814490 FixedProfit: 1522663 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 47/100 RapTime: 0:00:37.211155 FixedProfit: 1525664 TradeTimes: 7 TradeWin: 7\n",
            "Episode: 48/100 RapTime: 0:00:36.529020 FixedProfit: 1498979 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 49/100 RapTime: 0:00:36.980581 FixedProfit: 1538497 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 50/100 RapTime: 0:00:36.703047 FixedProfit: 1520671 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 51/100 RapTime: 0:00:37.027291 FixedProfit: 1490306 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 52/100 RapTime: 0:00:36.775219 FixedProfit: 1503957 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 53/100 RapTime: 0:00:36.952642 FixedProfit: 1521426 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 54/100 RapTime: 0:00:36.825225 FixedProfit: 1462540 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 55/100 RapTime: 0:00:37.290301 FixedProfit: 1560815 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 56/100 RapTime: 0:00:37.258730 FixedProfit: 1540497 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 57/100 RapTime: 0:00:36.888447 FixedProfit: 1552573 TradeTimes: 10 TradeWin: 7\n",
            "Episode: 58/100 RapTime: 0:00:37.179757 FixedProfit: 1532488 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 59/100 RapTime: 0:00:36.845936 FixedProfit: 1520067 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 60/100 RapTime: 0:00:37.227468 FixedProfit: 1522096 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 61/100 RapTime: 0:00:36.993274 FixedProfit: 1525851 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 62/100 RapTime: 0:00:37.202768 FixedProfit: 1538413 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 63/100 RapTime: 0:00:36.889806 FixedProfit: 1530304 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 64/100 RapTime: 0:00:36.996375 FixedProfit: 1605438 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 65/100 RapTime: 0:00:36.708848 FixedProfit: 1542723 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 66/100 RapTime: 0:00:36.787225 FixedProfit: 1514548 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 67/100 RapTime: 0:00:37.081186 FixedProfit: 1556282 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 68/100 RapTime: 0:00:36.944349 FixedProfit: 1552831 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 69/100 RapTime: 0:00:37.243141 FixedProfit: 1506625 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 70/100 RapTime: 0:00:36.878966 FixedProfit: 1496691 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 71/100 RapTime: 0:00:37.148148 FixedProfit: 1533318 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 72/100 RapTime: 0:00:36.946883 FixedProfit: 1508141 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 73/100 RapTime: 0:00:36.744673 FixedProfit: 1516355 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 74/100 RapTime: 0:00:36.605114 FixedProfit: 1525486 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 75/100 RapTime: 0:00:36.955130 FixedProfit: 1527888 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 76/100 RapTime: 0:00:36.732622 FixedProfit: 1538645 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 77/100 RapTime: 0:00:37.413419 FixedProfit: 1541636 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 78/100 RapTime: 0:00:36.677967 FixedProfit: 1525218 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 79/100 RapTime: 0:00:36.749908 FixedProfit: 1463759 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 80/100 RapTime: 0:00:37.179989 FixedProfit: 1560174 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 81/100 RapTime: 0:00:37.014214 FixedProfit: 1516378 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 82/100 RapTime: 0:00:37.294976 FixedProfit: 1533591 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 83/100 RapTime: 0:00:36.971231 FixedProfit: 1525391 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 84/100 RapTime: 0:00:37.174033 FixedProfit: 1489633 TradeTimes: 9 TradeWin: 6\n",
            "Episode: 85/100 RapTime: 0:00:36.799911 FixedProfit: 1571557 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 86/100 RapTime: 0:00:36.847166 FixedProfit: 1583592 TradeTimes: 9 TradeWin: 6\n",
            "Episode: 87/100 RapTime: 0:00:37.120617 FixedProfit: 1544379 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 88/100 RapTime: 0:00:37.526226 FixedProfit: 1537836 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 89/100 RapTime: 0:00:36.921243 FixedProfit: 1510094 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 90/100 RapTime: 0:00:37.318179 FixedProfit: 1509995 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 91/100 RapTime: 0:00:37.307645 FixedProfit: 1520167 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 92/100 RapTime: 0:00:36.974527 FixedProfit: 1518687 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 93/100 RapTime: 0:00:37.099897 FixedProfit: 1512904 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 94/100 RapTime: 0:00:37.098390 FixedProfit: 1536866 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 95/100 RapTime: 0:00:37.068319 FixedProfit: 1529956 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 96/100 RapTime: 0:00:36.695893 FixedProfit: 1507007 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 97/100 RapTime: 0:00:36.857708 FixedProfit: 1463200 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 98/100 RapTime: 0:00:36.956537 FixedProfit: 1491954 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 99/100 RapTime: 0:00:37.391716 FixedProfit: 1517506 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 100/100 RapTime: 0:00:36.679361 FixedProfit: 1655876 TradeTimes: 6 TradeWin: 6\n"
          ]
        }
      ]
    }
  ]
}