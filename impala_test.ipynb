{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "impala_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/impala_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "5ac53a6c-8506-4ba0-bc4c-043f754a3079"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPool1D, Activation, concatenate\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'test'\n",
        "name = 'impala'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m48th46c8otj"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.n_action = 3\n",
        "        self.gamma = 0.99\n",
        "        self.alfa = 0.5\n",
        "        self.beta = 0.00025\n",
        "\n",
        "    def valuenetwork(self, state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            actions = tf.one_hot(action, self.n_action) # (10, 3)\n",
        "\n",
        "            state = state.reshape(1,10,3)\n",
        "            prev_action = prev_action.reshape(1,10,1)\n",
        "            prev_reward = prev_reward.reshape(1,10,1)\n",
        "\n",
        "            pai, v_theta  = self.model([state, prev_action, prev_reward])\n",
        "\n",
        "            pai = tf.reshape(pai, [10,3]) # (10, 3)\n",
        "\n",
        "            actions = tf.cast(actions, tf.float32)\n",
        "\n",
        "            pais = tf.reduce_sum(actions * pai, axis=1, keepdims=True)\n",
        "\n",
        "            mu = self._reshape_and_cast(mu, 3)\n",
        "            ratio = tf.math.divide_no_nan(pais, mu)\n",
        "            rhoi = ci = tf.minimum(1.0, ratio)\n",
        "\n",
        "            n_num, _ = ratio.shape\n",
        "\n",
        "            rhoi = self._reshape_and_cast(rhoi,3)\n",
        "            ci = self._reshape_and_cast(ci,3)\n",
        "            reward = self._reshape_and_cast(reward,1)\n",
        "            v_next = self._reshape_and_cast(v_next,1)\n",
        "            v = self._reshape_and_cast(v,1)\n",
        "            v_theta = self._reshape_and_cast(v_theta,1)\n",
        "\n",
        "            b4_delta_v = (reward + self.gamma * v_next - v)\n",
        "            b4_delta_v  =  tf.cast(b4_delta_v, tf.float32)\n",
        "            delta_v = tf.multiply(rhoi, b4_delta_v)\n",
        "            delta_v = self._reshape_and_cast(delta_v,3)\n",
        "            v_trace =v + self._sigma(ci, delta_v, n_num)\n",
        "            total_loss = self._compute_baseline_loss(v_trace - v_theta)\n",
        "            total_loss += self._compute_policy_gradient_loss(pai, actions, delta_v)\n",
        "            total_loss += self._compute_entropy_loss(pai)\n",
        "\n",
        "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "    def _reshape_and_cast(self, x, num):\n",
        "        x = tf.reshape(x, [10,num])\n",
        "        x  =  tf.cast(x, tf.float32)\n",
        "        return x\n",
        "\n",
        "    def _infinite_product(self, x, max_num):\n",
        "        num = tf.ones([3, ], tf.float32)\n",
        "        for i in range(max_num):\n",
        "            num *= x[i]   \n",
        "        return num\n",
        "\n",
        "    def _sigma(self, x, delta_v, b):\n",
        "        \n",
        "        num = 0.0\n",
        "        for i in range(b):\n",
        "            num += pow(self.gamma, i) * self._infinite_product(x, i) * delta_v[i]\n",
        "        return tf.cast(num, tf.float32)\n",
        "\n",
        "    def _compute_baseline_loss(self, advantages):\n",
        "        return .5 * tf.reduce_sum(tf.square(advantages))\n",
        "\n",
        "    def _compute_policy_gradient_loss(self, logits, actions, advantages):\n",
        "        cross_entropy = tf.losses.categorical_crossentropy(y_true=actions, y_pred=logits)\n",
        "        cross_entropy = tf.reshape(cross_entropy, [10,1])\n",
        "        advantages = tf.stop_gradient(advantages)\n",
        "        policy_gradient_loss_per_timestep = cross_entropy * advantages\n",
        "        return tf.reduce_sum(policy_gradient_loss_per_timestep)\n",
        "\n",
        "    def _compute_entropy_loss(self, logits):\n",
        "        log_policy = tf.math.log(logits)\n",
        "        entropy_per_timestep = tf.reduce_sum(-logits * log_policy, axis=-1)\n",
        "        return -tf.reduce_sum(entropy_per_timestep)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcPU3_nDFvro"
      },
      "source": [
        "class Learner(Critic):\n",
        "    def __init__(self):\n",
        "\n",
        "        conv_filter = 12\n",
        "        units = 28\n",
        "        look_back = 10\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "\n",
        "        input1_ = Input(shape=(look_back, 3))\n",
        "        input2_ = Input(shape=(look_back, 1))\n",
        "        input3_ = Input(shape=(look_back, 1))\n",
        "\n",
        "        x = Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\")(input1_)\n",
        "        x = MaxPool1D(pool_size=1, padding='same')(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        combined = concatenate([x, input2_, input3_],axis=-1)\n",
        "        common = LSTM(units, return_sequences=True)(combined)\n",
        "        common = Dense(units, kernel_initializer='random_uniform')(common)\n",
        "        common = Activation(\"relu\")(common)\n",
        "\n",
        "        actor  = Dense(3, activation=\"softmax\")(common)\n",
        "        critic = Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model([input1_, input2_, input3_], [actor, critic])\n",
        "        model.compile(loss = \"mean_absolute_error\", optimizer=opt)\n",
        "        model.summary()\n",
        "        #dot_img_file = './f\"{name}_model.png\"'\n",
        "        #tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
        "        self.model = model\n",
        "        super().__init__(model)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        length = memory.max_length_memory()\n",
        "        for i in range(length):\n",
        "            min = i\n",
        "            max = (i + 10)\n",
        "            state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu = memory.get_experiences(min, max)\n",
        "            self.valuenetwork(state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, learner):\n",
        "\n",
        "        self.learner = learner\n",
        "        self.model = clone_model(learner.model)\n",
        "        self.n_action = 3\n",
        "        self.state_arr = np.array([])\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.array([])\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def reset(self):\n",
        "        self.state_arr = np.empty((0,3), int)\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.empty((0,3), int)\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def policynetwork(self, state, prev_action, prev_reward):\n",
        "\n",
        "        if len(self.state_arr) == 10:\n",
        "            self.state_arr[0:-1] = self.state_arr[1:]\n",
        "            self.p_action_arr[0:-1] = self.p_action_arr[1:]\n",
        "            self.p_reward_arr[0:-1] = self.p_reward_arr[1:]\n",
        "            self.state_arr[-1] = state\n",
        "            self.p_action_arr[-1] = prev_action\n",
        "            self.p_reward_arr[-1] = prev_reward\n",
        "            tmp_state = copy.deepcopy(self.state_arr)\n",
        "            tmp_action = copy.deepcopy(self.p_action_arr)\n",
        "            tmp_reward = copy.deepcopy(self.p_reward_arr)\n",
        "            tmp_state = tmp_state.reshape(1,10,3)\n",
        "            tmp_action = tmp_action.reshape(1,10,1)\n",
        "            tmp_reward = tmp_reward.reshape(1,10,1)\n",
        "            act_p, v = self.model([tmp_state, tmp_action, tmp_reward]) # [-1.03245259 -0.55189404  0.87892511] 1 0\n",
        "            v_np = v.numpy()\n",
        "            p_np = act_p.numpy()\n",
        "            one_hot_actions = tf.one_hot([0,1,2], 3)\n",
        "            act_p = p_np[0][9]\n",
        "            mu = tf.reduce_sum(one_hot_actions * act_p, axis=1)\n",
        "            return np.random.choice(3, p=p_np[0][9]), v_np[0][9][0], mu.numpy()\n",
        "\n",
        "        self.state_arr = np.append(self.state_arr, np.array([state]), axis=0)\n",
        "        self.p_action_arr = np.append(self.p_action_arr, np.array([prev_action]))\n",
        "        self.p_reward_arr = np.append(self.p_reward_arr, np.array([prev_reward]))\n",
        "        return 1, 1, [0.0, 1.0, 0.0]\n",
        "\n",
        "    def policynetwork_next(self, next_state, action, reward):\n",
        "\n",
        "        if len(self.next_state_arr) == 10:\n",
        "            self.next_state_arr[0:-1] = self.next_state_arr[1:]\n",
        "            self.action_arr[0:-1] = self.action_arr[1:]\n",
        "            self.reward_arr[0:-1] = self.reward_arr[1:]\n",
        "            self.next_state_arr[-1] = next_state\n",
        "            self.action_arr[-1] = action\n",
        "            self.reward_arr[-1] = reward\n",
        "\n",
        "            tmp_n_state = copy.deepcopy(self.next_state_arr)\n",
        "            tmp_action = copy.deepcopy(self.action_arr)\n",
        "            tmp_reward = copy.deepcopy(self.reward_arr)\n",
        "            tmp_n_state = tmp_n_state.reshape(1,10,3)\n",
        "            tmp_action = tmp_action.reshape(1,10,1)\n",
        "            tmp_reward = tmp_reward.reshape(1,10,1)\n",
        "\n",
        "            _, v_next = self.model([tmp_n_state, tmp_action, tmp_reward])\n",
        "            v_next_np = v_next.numpy()\n",
        "            return v_next_np[0][9][0]\n",
        "\n",
        "        self.next_state_arr = np.append(self.next_state_arr, np.array([next_state]), axis=0)\n",
        "        self.action_arr = np.append(self.action_arr, np.array([action]))\n",
        "        self.reward_arr = np.append(self.reward_arr, np.array([reward]))\n",
        "        return 1.0\n",
        "\n",
        "    def load(self, name):\n",
        "        self.learner.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.learner.save(name)\n",
        "\n",
        "    def integration(self):\n",
        "        self.model = clone_model(self.learner.model)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        self.learner.placement(memory)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4-NrrtJBQWj"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : np.ndarray = np.empty((0,3), int)\n",
        "    next_state : np.ndarray = np.empty((0,3), int)\n",
        "    prev_action : np.ndarray = np.array([])\n",
        "    action : np.ndarray = np.array([])\n",
        "    prev_reward : np.ndarray = np.array([])\n",
        "    reward : np.ndarray = np.array([])\n",
        "    done : np.ndarray = np.array([])\n",
        "    v : np.ndarray = np.array([])\n",
        "    v_next : np.ndarray = np.array([])\n",
        "    mu : np.ndarray = np.empty((0,3), int)\n",
        "    minibatch_size : int = 64\n",
        "\n",
        "    def append_experiences(self, state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu):\n",
        "        self.state = np.append(self.state, np.array([state]), axis=0)\n",
        "        self.next_state = np.append(self.next_state, np.array([next_state]), axis=0)\n",
        "        self.prev_action = np.append(self.prev_action, np.array(prev_action))\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.prev_reward = np.append(self.prev_reward, np.array(prev_reward))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        self.done = np.append(self.done, np.array(done))\n",
        "        self.v = np.append(self.v, np.array(v))\n",
        "        self.v_next = np.append(self.v_next, np.array(v_next))\n",
        "        self.mu = np.append(self.mu, np.array([mu]), axis=0)\n",
        "\n",
        "    def max_length_memory(self):\n",
        "        max_len = len(self.state)\n",
        "        max_len = int(max_len) - 11\n",
        "\n",
        "        return max_len\n",
        "\n",
        "    def get_experiences(self, min, max):\n",
        "        state, next_state, mu = np.empty((0,3), int), np.empty((0,3), int), np.empty((0,3), int)\n",
        "        prev_action, action, prev_reward, reward, done, v, v_next = np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
        "        for i in range(min, max):\n",
        "            state = np.append(state, np.array([self.state[i]]), axis=0)\n",
        "            next_state = np.append(next_state, self.action[i])\n",
        "            prev_action = np.append(prev_action, self.prev_action[i])\n",
        "            action = np.append(action, self.action[i])\n",
        "            prev_reward = np.append(prev_reward, self.prev_reward[i])\n",
        "            reward = np.append(reward, self.reward[i])\n",
        "            done = np.append(done, self.done[i])\n",
        "            v = np.append(v, self.v[i])\n",
        "            v_next = np.append(v_next, self.v_next[i])\n",
        "            mu = np.append(mu, np.array([self.mu[i]]), axis=0)\n",
        "\n",
        "        return state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, num, mdl_dir, name, batch_size = 10, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "        \n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            self.actor.reset()\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory()\n",
        "            prev_action = 1\n",
        "            prev_reward = 0\n",
        "            i = 0\n",
        "    \n",
        "            while not done:\n",
        "                action, v, mu = self.actor.policynetwork(state, prev_action, prev_reward)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "                v_next = self.actor.policynetwork_next(next_state, action, reward)\n",
        "\n",
        "                if (i > self.batch_size) and (self.mode == 'train'):\n",
        "                    memory.append_experiences(state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu)\n",
        "\n",
        "                state = next_state\n",
        "                prev_action = action\n",
        "                prev_reward = reward\n",
        "                i += 1\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                self.actor.placement(memory)\n",
        "                self.actor.integration()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1977f1e0-4cb9-455b-9886-cf8795b538b1"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 25\n",
        "batch_size = 10\n",
        "learner = Learner()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "    actor = Actor(learner)\n",
        "    main = Main(env, actor, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 10, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 10, 12)       48          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 10, 12)       0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 10, 12)       0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 14)       0           activation[0][0]                 \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 10, 28)       4816        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10, 28)       812         lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 10, 28)       0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10, 3)        87          activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10, 1)        29          activation_1[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,792\n",
            "Trainable params: 5,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/25 RapTime: 0:01:17.490125 FixedProfit: 1374388 TradeTimes: 145 TradeWin: 88\n",
            "Episode: 1/25 RapTime: 0:01:17.783259 FixedProfit: 1535609 TradeTimes: 140 TradeWin: 83\n",
            "Episode: 1/25 RapTime: 0:01:17.844292 FixedProfit: 1403251 TradeTimes: 148 TradeWin: 93\n",
            "Episode: 1/25 RapTime: 0:01:18.019429 FixedProfit: 1209455 TradeTimes: 144 TradeWin: 83\n",
            "Episode: 2/25 RapTime: 0:01:13.843317 FixedProfit: 1608577 TradeTimes: 141 TradeWin: 85\n",
            "Episode: 2/25 RapTime: 0:01:14.441452 FixedProfit: 1649210 TradeTimes: 147 TradeWin: 94\n",
            "Episode: 2/25 RapTime: 0:01:14.378164 FixedProfit: 1341907 TradeTimes: 150 TradeWin: 93\n",
            "Episode: 2/25 RapTime: 0:01:14.606998 FixedProfit: 1876764 TradeTimes: 142 TradeWin: 97\n",
            "Episode: 3/25 RapTime: 0:01:14.527805 FixedProfit: 990190 TradeTimes: 149 TradeWin: 85\n",
            "Episode: 3/25 RapTime: 0:01:14.418370 FixedProfit: 1288446 TradeTimes: 153 TradeWin: 101\n",
            "Episode: 3/25 RapTime: 0:01:14.605352 FixedProfit: 1135954 TradeTimes: 147 TradeWin: 94\n",
            "Episode: 3/25 RapTime: 0:01:14.406343 FixedProfit: 997979 TradeTimes: 139 TradeWin: 79\n",
            "Episode: 4/25 RapTime: 0:01:13.800843 FixedProfit: 1358659 TradeTimes: 153 TradeWin: 87\n",
            "Episode: 4/25 RapTime: 0:01:13.929704 FixedProfit: 988017 TradeTimes: 144 TradeWin: 83\n",
            "Episode: 4/25 RapTime: 0:01:14.024094 FixedProfit: 967887 TradeTimes: 156 TradeWin: 88\n",
            "Episode: 4/25 RapTime: 0:01:14.435142 FixedProfit: 1297256 TradeTimes: 155 TradeWin: 88\n",
            "Episode: 5/25 RapTime: 0:01:14.113406 FixedProfit: 1549670 TradeTimes: 130 TradeWin: 80\n",
            "Episode: 5/25 RapTime: 0:01:14.159765 FixedProfit: 1733426 TradeTimes: 156 TradeWin: 99\n",
            "Episode: 5/25 RapTime: 0:01:14.597474 FixedProfit: 1626699 TradeTimes: 158 TradeWin: 103\n",
            "Episode: 5/25 RapTime: 0:01:14.443501 FixedProfit: 889592 TradeTimes: 154 TradeWin: 88\n",
            "Episode: 6/25 RapTime: 0:01:13.778127 FixedProfit: 1359771 TradeTimes: 143 TradeWin: 96\n",
            "Episode: 6/25 RapTime: 0:01:13.639995 FixedProfit: 1503950 TradeTimes: 143 TradeWin: 89\n",
            "Episode: 6/25 RapTime: 0:01:14.300755 FixedProfit: 1204222 TradeTimes: 165 TradeWin: 100\n",
            "Episode: 6/25 RapTime: 0:01:14.064614 FixedProfit: 1383004 TradeTimes: 154 TradeWin: 106\n",
            "Episode: 7/25 RapTime: 0:01:14.227039 FixedProfit: 1080715 TradeTimes: 138 TradeWin: 79\n",
            "Episode: 7/25 RapTime: 0:01:14.095541 FixedProfit: 1268880 TradeTimes: 137 TradeWin: 87\n",
            "Episode: 7/25 RapTime: 0:01:14.139664 FixedProfit: 1892710 TradeTimes: 141 TradeWin: 92\n",
            "Episode: 7/25 RapTime: 0:01:14.518833 FixedProfit: 1000785 TradeTimes: 155 TradeWin: 87\n",
            "Episode: 8/25 RapTime: 0:01:13.770699 FixedProfit: 1326951 TradeTimes: 149 TradeWin: 96\n",
            "Episode: 8/25 RapTime: 0:01:13.743527 FixedProfit: 1582988 TradeTimes: 157 TradeWin: 107\n",
            "Episode: 8/25 RapTime: 0:01:14.004930 FixedProfit: 996448 TradeTimes: 138 TradeWin: 81\n",
            "Episode: 8/25 RapTime: 0:01:14.132418 FixedProfit: 1339640 TradeTimes: 145 TradeWin: 89\n",
            "Episode: 9/25 RapTime: 0:01:13.964176 FixedProfit: 1312993 TradeTimes: 160 TradeWin: 104\n",
            "Episode: 9/25 RapTime: 0:01:14.072511 FixedProfit: 1009567 TradeTimes: 145 TradeWin: 88\n",
            "Episode: 9/25 RapTime: 0:01:14.364932 FixedProfit: 1565998 TradeTimes: 140 TradeWin: 91\n",
            "Episode: 9/25 RapTime: 0:01:14.459511 FixedProfit: 1186056 TradeTimes: 161 TradeWin: 104\n",
            "Episode: 10/25 RapTime: 0:01:13.589636 FixedProfit: 1367484 TradeTimes: 137 TradeWin: 87\n",
            "Episode: 10/25 RapTime: 0:01:13.888845 FixedProfit: 1674079 TradeTimes: 152 TradeWin: 96\n",
            "Episode: 10/25 RapTime: 0:01:13.877868 FixedProfit: 1391806 TradeTimes: 145 TradeWin: 85\n",
            "Episode: 10/25 RapTime: 0:01:13.859713 FixedProfit: 1053692 TradeTimes: 163 TradeWin: 105\n",
            "Episode: 11/25 RapTime: 0:01:13.695410 FixedProfit: 1080693 TradeTimes: 142 TradeWin: 87\n",
            "Episode: 11/25 RapTime: 0:01:14.292774 FixedProfit: 1363051 TradeTimes: 146 TradeWin: 87\n",
            "Episode: 11/25 RapTime: 0:01:13.662513 FixedProfit: 1510025 TradeTimes: 144 TradeWin: 90\n",
            "Episode: 11/25 RapTime: 0:01:14.127426 FixedProfit: 1104006 TradeTimes: 149 TradeWin: 86\n",
            "Episode: 12/25 RapTime: 0:01:13.907806 FixedProfit: 1152006 TradeTimes: 154 TradeWin: 95\n",
            "Episode: 12/25 RapTime: 0:01:13.791457 FixedProfit: 1141717 TradeTimes: 140 TradeWin: 88\n",
            "Episode: 12/25 RapTime: 0:01:14.079512 FixedProfit: 851851 TradeTimes: 147 TradeWin: 81\n",
            "Episode: 12/25 RapTime: 0:01:14.028612 FixedProfit: 1268333 TradeTimes: 147 TradeWin: 92\n",
            "Episode: 13/25 RapTime: 0:01:13.860109 FixedProfit: 1143499 TradeTimes: 145 TradeWin: 84\n",
            "Episode: 13/25 RapTime: 0:01:13.898017 FixedProfit: 1567589 TradeTimes: 146 TradeWin: 85\n",
            "Episode: 13/25 RapTime: 0:01:14.099636 FixedProfit: 1032554 TradeTimes: 132 TradeWin: 81\n",
            "Episode: 13/25 RapTime: 0:01:14.009583 FixedProfit: 1403974 TradeTimes: 150 TradeWin: 86\n",
            "Episode: 14/25 RapTime: 0:01:13.562579 FixedProfit: 749367 TradeTimes: 141 TradeWin: 74\n",
            "Episode: 14/25 RapTime: 0:01:13.600445 FixedProfit: 1072115 TradeTimes: 149 TradeWin: 81\n",
            "Episode: 14/25 RapTime: 0:01:13.496173 FixedProfit: 1698699 TradeTimes: 140 TradeWin: 88\n",
            "Episode: 14/25 RapTime: 0:01:13.771989 FixedProfit: 1239310 TradeTimes: 153 TradeWin: 96\n",
            "Episode: 15/25 RapTime: 0:01:13.633839 FixedProfit: 985950 TradeTimes: 153 TradeWin: 87\n",
            "Episode: 15/25 RapTime: 0:01:13.407503 FixedProfit: 1008233 TradeTimes: 159 TradeWin: 95\n",
            "Episode: 15/25 RapTime: 0:01:13.440809 FixedProfit: 863236 TradeTimes: 142 TradeWin: 87\n",
            "Episode: 15/25 RapTime: 0:01:13.528957 FixedProfit: 1685645 TradeTimes: 138 TradeWin: 85\n",
            "Episode: 16/25 RapTime: 0:01:13.247474 FixedProfit: 1236928 TradeTimes: 144 TradeWin: 85\n",
            "Episode: 16/25 RapTime: 0:01:13.417219 FixedProfit: 954175 TradeTimes: 147 TradeWin: 84\n",
            "Episode: 16/25 RapTime: 0:01:13.363433 FixedProfit: 1171108 TradeTimes: 146 TradeWin: 86\n",
            "Episode: 16/25 RapTime: 0:01:13.157745 FixedProfit: 1485918 TradeTimes: 149 TradeWin: 91\n",
            "Episode: 17/25 RapTime: 0:01:13.902780 FixedProfit: 1087562 TradeTimes: 140 TradeWin: 85\n",
            "Episode: 17/25 RapTime: 0:01:13.715434 FixedProfit: 1414869 TradeTimes: 154 TradeWin: 101\n",
            "Episode: 17/25 RapTime: 0:01:14.031535 FixedProfit: 1249450 TradeTimes: 140 TradeWin: 85\n",
            "Episode: 17/25 RapTime: 0:01:13.855038 FixedProfit: 975501 TradeTimes: 160 TradeWin: 90\n",
            "Episode: 18/25 RapTime: 0:01:12.781891 FixedProfit: 1043080 TradeTimes: 145 TradeWin: 85\n",
            "Episode: 18/25 RapTime: 0:01:12.891449 FixedProfit: 1032991 TradeTimes: 157 TradeWin: 89\n",
            "Episode: 18/25 RapTime: 0:01:12.460846 FixedProfit: 905359 TradeTimes: 158 TradeWin: 89\n",
            "Episode: 18/25 RapTime: 0:01:13.031226 FixedProfit: 1469220 TradeTimes: 151 TradeWin: 91\n",
            "Episode: 19/25 RapTime: 0:01:12.232324 FixedProfit: 1706112 TradeTimes: 134 TradeWin: 79\n",
            "Episode: 19/25 RapTime: 0:01:12.380015 FixedProfit: 1139092 TradeTimes: 150 TradeWin: 89\n",
            "Episode: 19/25 RapTime: 0:01:12.370267 FixedProfit: 953069 TradeTimes: 153 TradeWin: 92\n",
            "Episode: 19/25 RapTime: 0:01:12.870107 FixedProfit: 1299886 TradeTimes: 153 TradeWin: 94\n",
            "Episode: 20/25 RapTime: 0:01:11.982239 FixedProfit: 844055 TradeTimes: 143 TradeWin: 82\n",
            "Episode: 20/25 RapTime: 0:01:12.063084 FixedProfit: 1462302 TradeTimes: 147 TradeWin: 86\n",
            "Episode: 20/25 RapTime: 0:01:12.009721 FixedProfit: 1138455 TradeTimes: 144 TradeWin: 88\n",
            "Episode: 20/25 RapTime: 0:01:11.936583 FixedProfit: 1049950 TradeTimes: 158 TradeWin: 98\n",
            "Episode: 21/25 RapTime: 0:01:12.197907 FixedProfit: 1090671 TradeTimes: 145 TradeWin: 88\n",
            "Episode: 21/25 RapTime: 0:01:12.344240 FixedProfit: 1555210 TradeTimes: 151 TradeWin: 96\n",
            "Episode: 21/25 RapTime: 0:01:12.472259 FixedProfit: 1627997 TradeTimes: 144 TradeWin: 92\n",
            "Episode: 21/25 RapTime: 0:01:12.320709 FixedProfit: 1362080 TradeTimes: 147 TradeWin: 95\n",
            "Episode: 22/25 RapTime: 0:01:12.981531 FixedProfit: 1213947 TradeTimes: 164 TradeWin: 99\n",
            "Episode: 22/25 RapTime: 0:01:12.966744 FixedProfit: 1363802 TradeTimes: 149 TradeWin: 85\n",
            "Episode: 22/25 RapTime: 0:01:13.252105 FixedProfit: 1302943 TradeTimes: 150 TradeWin: 95\n",
            "Episode: 22/25 RapTime: 0:01:13.022855 FixedProfit: 1153429 TradeTimes: 156 TradeWin: 92\n",
            "Episode: 23/25 RapTime: 0:01:13.240636 FixedProfit: 1531081 TradeTimes: 140 TradeWin: 90\n",
            "Episode: 23/25 RapTime: 0:01:13.212524 FixedProfit: 1109642 TradeTimes: 158 TradeWin: 93\n",
            "Episode: 23/25 RapTime: 0:01:12.979325 FixedProfit: 1417204 TradeTimes: 140 TradeWin: 82\n",
            "Episode: 23/25 RapTime: 0:01:13.208920 FixedProfit: 1489312 TradeTimes: 149 TradeWin: 103\n",
            "Episode: 24/25 RapTime: 0:01:11.879103 FixedProfit: 1050932 TradeTimes: 151 TradeWin: 91\n",
            "Episode: 24/25 RapTime: 0:01:12.149864 FixedProfit: 783810 TradeTimes: 154 TradeWin: 91\n",
            "Episode: 24/25 RapTime: 0:01:12.277766 FixedProfit: 1160587 TradeTimes: 141 TradeWin: 84\n",
            "Episode: 24/25 RapTime: 0:01:12.940694 FixedProfit: 1104652 TradeTimes: 155 TradeWin: 97\n",
            "Episode: 25/25 RapTime: 0:01:12.228044 FixedProfit: 1460964 TradeTimes: 156 TradeWin: 102\n",
            "Episode: 25/25 RapTime: 0:01:11.924070 FixedProfit: 988401 TradeTimes: 145 TradeWin: 92\n",
            "Episode: 25/25 RapTime: 0:01:11.073772 FixedProfit: 1125850 TradeTimes: 151 TradeWin: 86\n",
            "Episode: 25/25 RapTime: 0:01:09.060082 FixedProfit: 1073976 TradeTimes: 148 TradeWin: 90\n"
          ]
        }
      ]
    }
  ]
}