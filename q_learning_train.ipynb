{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q_learning_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN/C4Cq3mbvXeO674VtIwhg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/q_learning_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "6fb961f1-3117-4957-b9e2-ac816e316fcf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'qlearning_train.csv'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU())\n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        q = self.model.predict(state)  \n",
        "        next_q = self.model.predict(next_state)\n",
        "        target = np.copy(q)\n",
        "        if done:\n",
        "            target[:, action] = reward\n",
        "        else:\n",
        "            target[:, action] = reward + self.gamma*np.max(next_q, axis=1)\n",
        "        self.model.train_on_batch(state, target)\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent(Brain):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.predict(state)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "        return np.argmax(act_values[0])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent):\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "\n",
        "        if mode == 'test':\n",
        "            with open('{}/qlearning.pkl'.format(mdl_dir), 'rb') as f:\n",
        "                self.scaler = pickle.load(f)\n",
        "            self.agent.epsilon = 0.01\n",
        "            self.agent.load('{}/qlearning.h5'.format(mdl_dir))\n",
        "\n",
        "\n",
        "    def play_game(episodes_times = 1000, mode = 'test'):\n",
        "        if mode == 'test':\n",
        "            df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "        else:\n",
        "            df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "        for episode in range(episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if mode == 'train':\n",
        "                    self.agent.train(state, action, reward, next_state, done)\n",
        "                \n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            else:\n",
        "                record = pd.Series(info['cur_revenue'], index=df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "\n",
        "            state = next_state\n",
        "            df_rec = df_rec.append(record, ignore_index=True)\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.agent.save('{}/qlearning.h5'.format(mdl_dir))\n",
        "            with open('{}/qlearning.pkl'.format(mdl_dir), 'wb') as f:\n",
        "                pickle.dump(scaler, f)\n",
        "\n",
        "        return df_rec\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "e6300eea-85a9-4635-fe7b-f50edae1b2de"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "mode = 'train'\n",
        "agent = Agent()\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "main = Main(env, agent)\n",
        "\n",
        "df_rec = main.play_game(eepisodes_times = episodes_times, mode = mode)\n",
        "\n",
        "df_rec.to_csv(csv_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/100 RapTime: 0:00:55.049109 FixedProfit: 1146402\n",
            "Episode: 2/100 RapTime: 0:00:54.127810 FixedProfit: 980572\n",
            "Episode: 3/100 RapTime: 0:00:54.353401 FixedProfit: 1003250\n",
            "Episode: 4/100 RapTime: 0:00:54.745210 FixedProfit: 1134670\n",
            "Episode: 5/100 RapTime: 0:00:54.352333 FixedProfit: 1093769\n",
            "Episode: 6/100 RapTime: 0:00:54.805559 FixedProfit: 1052409\n",
            "Episode: 7/100 RapTime: 0:00:54.500806 FixedProfit: 1218577\n",
            "Episode: 8/100 RapTime: 0:00:54.266726 FixedProfit: 1179975\n",
            "Episode: 9/100 RapTime: 0:00:54.359593 FixedProfit: 1080393\n",
            "Episode: 10/100 RapTime: 0:00:54.451343 FixedProfit: 1060915\n",
            "Episode: 11/100 RapTime: 0:00:54.105170 FixedProfit: 1086532\n",
            "Episode: 12/100 RapTime: 0:00:54.186136 FixedProfit: 972449\n",
            "Episode: 13/100 RapTime: 0:00:53.832725 FixedProfit: 835966\n",
            "Episode: 14/100 RapTime: 0:00:53.836015 FixedProfit: 1161063\n",
            "Episode: 15/100 RapTime: 0:00:54.384369 FixedProfit: 926698\n",
            "Episode: 16/100 RapTime: 0:00:53.929015 FixedProfit: 971515\n",
            "Episode: 17/100 RapTime: 0:00:54.157262 FixedProfit: 824770\n",
            "Episode: 18/100 RapTime: 0:00:54.293449 FixedProfit: 1018231\n",
            "Episode: 19/100 RapTime: 0:00:53.751790 FixedProfit: 1011329\n",
            "Episode: 20/100 RapTime: 0:00:53.884711 FixedProfit: 1140151\n",
            "Episode: 21/100 RapTime: 0:00:54.240771 FixedProfit: 1347149\n",
            "Episode: 22/100 RapTime: 0:00:55.837012 FixedProfit: 1036025\n",
            "Episode: 23/100 RapTime: 0:00:54.356274 FixedProfit: 1119888\n",
            "Episode: 24/100 RapTime: 0:00:53.725958 FixedProfit: 1102075\n",
            "Episode: 25/100 RapTime: 0:00:55.297356 FixedProfit: 953817\n",
            "Episode: 26/100 RapTime: 0:00:55.625213 FixedProfit: 1127627\n",
            "Episode: 27/100 RapTime: 0:00:54.180239 FixedProfit: 1056610\n",
            "Episode: 28/100 RapTime: 0:00:53.857369 FixedProfit: 1203946\n",
            "Episode: 29/100 RapTime: 0:00:54.512068 FixedProfit: 1185028\n",
            "Episode: 30/100 RapTime: 0:00:54.241718 FixedProfit: 919405\n",
            "Episode: 31/100 RapTime: 0:00:54.146385 FixedProfit: 1043825\n",
            "Episode: 32/100 RapTime: 0:00:53.947065 FixedProfit: 1023397\n",
            "Episode: 33/100 RapTime: 0:00:54.017438 FixedProfit: 938851\n",
            "Episode: 34/100 RapTime: 0:00:54.004866 FixedProfit: 1176934\n",
            "Episode: 35/100 RapTime: 0:00:53.489715 FixedProfit: 1174584\n",
            "Episode: 36/100 RapTime: 0:00:53.272806 FixedProfit: 1113025\n",
            "Episode: 37/100 RapTime: 0:00:54.047390 FixedProfit: 946724\n",
            "Episode: 38/100 RapTime: 0:00:53.536569 FixedProfit: 1229619\n",
            "Episode: 39/100 RapTime: 0:00:52.755465 FixedProfit: 1024324\n",
            "Episode: 40/100 RapTime: 0:00:53.098754 FixedProfit: 1098026\n",
            "Episode: 41/100 RapTime: 0:00:53.287205 FixedProfit: 779763\n",
            "Episode: 42/100 RapTime: 0:00:53.022153 FixedProfit: 1257203\n",
            "Episode: 43/100 RapTime: 0:00:53.580670 FixedProfit: 1065617\n",
            "Episode: 44/100 RapTime: 0:00:53.055849 FixedProfit: 1185287\n",
            "Episode: 45/100 RapTime: 0:00:53.304003 FixedProfit: 1053695\n",
            "Episode: 46/100 RapTime: 0:00:53.047865 FixedProfit: 978536\n",
            "Episode: 47/100 RapTime: 0:00:52.724496 FixedProfit: 944011\n",
            "Episode: 48/100 RapTime: 0:00:53.685202 FixedProfit: 1138834\n",
            "Episode: 49/100 RapTime: 0:00:52.793040 FixedProfit: 1082224\n",
            "Episode: 50/100 RapTime: 0:00:53.225958 FixedProfit: 1190054\n",
            "Episode: 51/100 RapTime: 0:00:53.265065 FixedProfit: 1106231\n",
            "Episode: 52/100 RapTime: 0:00:53.272462 FixedProfit: 1027788\n",
            "Episode: 53/100 RapTime: 0:00:53.234738 FixedProfit: 1220381\n",
            "Episode: 54/100 RapTime: 0:00:52.741746 FixedProfit: 1277375\n",
            "Episode: 55/100 RapTime: 0:00:53.899442 FixedProfit: 1234682\n",
            "Episode: 56/100 RapTime: 0:00:52.830207 FixedProfit: 842815\n",
            "Episode: 57/100 RapTime: 0:00:52.872571 FixedProfit: 958795\n",
            "Episode: 58/100 RapTime: 0:00:53.421433 FixedProfit: 1203959\n",
            "Episode: 59/100 RapTime: 0:00:53.244865 FixedProfit: 1017134\n",
            "Episode: 60/100 RapTime: 0:00:53.657827 FixedProfit: 1208270\n",
            "Episode: 61/100 RapTime: 0:00:54.958232 FixedProfit: 1209705\n",
            "Episode: 62/100 RapTime: 0:00:55.339454 FixedProfit: 1229987\n",
            "Episode: 63/100 RapTime: 0:00:54.299405 FixedProfit: 1253448\n",
            "Episode: 64/100 RapTime: 0:00:54.714654 FixedProfit: 987192\n",
            "Episode: 65/100 RapTime: 0:00:54.452860 FixedProfit: 1159066\n",
            "Episode: 66/100 RapTime: 0:00:54.543595 FixedProfit: 1008740\n",
            "Episode: 67/100 RapTime: 0:00:55.760907 FixedProfit: 1041078\n",
            "Episode: 68/100 RapTime: 0:00:55.315037 FixedProfit: 1147810\n",
            "Episode: 69/100 RapTime: 0:00:55.556077 FixedProfit: 1186350\n",
            "Episode: 70/100 RapTime: 0:00:54.614973 FixedProfit: 1152459\n",
            "Episode: 71/100 RapTime: 0:00:54.220454 FixedProfit: 1144925\n",
            "Episode: 72/100 RapTime: 0:00:54.041092 FixedProfit: 1148926\n",
            "Episode: 73/100 RapTime: 0:00:53.464348 FixedProfit: 935133\n",
            "Episode: 74/100 RapTime: 0:00:53.293046 FixedProfit: 1125071\n",
            "Episode: 75/100 RapTime: 0:00:52.455221 FixedProfit: 1051827\n",
            "Episode: 76/100 RapTime: 0:00:52.834456 FixedProfit: 839559\n",
            "Episode: 77/100 RapTime: 0:00:54.849362 FixedProfit: 1291904\n",
            "Episode: 78/100 RapTime: 0:00:57.069326 FixedProfit: 945758\n",
            "Episode: 79/100 RapTime: 0:00:57.229088 FixedProfit: 977049\n",
            "Episode: 80/100 RapTime: 0:00:54.756764 FixedProfit: 1115750\n",
            "Episode: 81/100 RapTime: 0:00:55.951637 FixedProfit: 1099051\n",
            "Episode: 82/100 RapTime: 0:00:57.963960 FixedProfit: 899867\n",
            "Episode: 83/100 RapTime: 0:00:57.625770 FixedProfit: 1058417\n",
            "Episode: 84/100 RapTime: 0:00:57.387027 FixedProfit: 959862\n",
            "Episode: 85/100 RapTime: 0:00:54.899385 FixedProfit: 988378\n",
            "Episode: 86/100 RapTime: 0:00:54.841427 FixedProfit: 1259859\n",
            "Episode: 87/100 RapTime: 0:00:57.264280 FixedProfit: 870961\n",
            "Episode: 88/100 RapTime: 0:00:57.328376 FixedProfit: 1282303\n",
            "Episode: 89/100 RapTime: 0:00:56.748525 FixedProfit: 1516766\n",
            "Episode: 90/100 RapTime: 0:00:55.731516 FixedProfit: 911456\n",
            "Episode: 91/100 RapTime: 0:00:54.658639 FixedProfit: 1004076\n",
            "Episode: 92/100 RapTime: 0:00:55.142204 FixedProfit: 979909\n",
            "Episode: 93/100 RapTime: 0:00:57.697052 FixedProfit: 1214625\n",
            "Episode: 94/100 RapTime: 0:00:55.953963 FixedProfit: 1183647\n",
            "Episode: 95/100 RapTime: 0:00:53.895885 FixedProfit: 1058601\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}