{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "impala_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/impala_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "71a30eb9-b1df-4799-c411-553d46d35835"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPool1D, Activation, concatenate\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'test'\n",
        "name = 'impala'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m48th46c8otj"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.n_action = 3\n",
        "        self.gamma = 0.9\n",
        "        self.alfa = 0.5\n",
        "        self.beta = 0.00025\n",
        "\n",
        "    def valuenetwork(self, state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            actions = tf.one_hot(action, self.n_action) # (10, 3)\n",
        "\n",
        "            state = state.reshape(1,10,3)\n",
        "            prev_action = prev_action.reshape(1,10,1)\n",
        "            prev_reward = prev_reward.reshape(1,10,1)\n",
        "\n",
        "            pai, v_theta  = self.model([state, prev_action, prev_reward])\n",
        "\n",
        "            pai = tf.reshape(pai, [10,3]) # (10, 3)\n",
        "\n",
        "            actions = tf.cast(actions, tf.float32)\n",
        "\n",
        "            pais = tf.reduce_sum(actions * pai, axis=1, keepdims=True)\n",
        "\n",
        "            mu = self._reshape_and_cast(mu, 3)\n",
        "            ratio = tf.math.divide_no_nan(pais, mu)\n",
        "            rhoi = ci = tf.minimum(1.0, ratio)\n",
        "\n",
        "            n_num, _ = ratio.shape\n",
        "\n",
        "            rhoi = self._reshape_and_cast(rhoi,3)\n",
        "            ci = self._reshape_and_cast(ci,3)\n",
        "            reward = self._reshape_and_cast(reward,1)\n",
        "            v_next = self._reshape_and_cast(v_next,1)\n",
        "            v = self._reshape_and_cast(v,1)\n",
        "            v_theta = self._reshape_and_cast(v_theta,1)\n",
        "\n",
        "            b4_delta_v = (reward + self.gamma * v_next - v)\n",
        "            b4_delta_v  =  tf.cast(b4_delta_v, tf.float32)\n",
        "            delta_v = tf.multiply(rhoi, b4_delta_v)\n",
        "            delta_v = self._reshape_and_cast(delta_v,3)\n",
        "            v_trace =v + self._sigma(ci, delta_v, n_num)\n",
        "            total_loss = self._compute_baseline_loss(v_trace - v_theta)\n",
        "            total_loss += self._compute_policy_gradient_loss(pai, actions, delta_v)\n",
        "            total_loss += self._compute_entropy_loss(pai)\n",
        "\n",
        "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "    def _reshape_and_cast(self, x, num):\n",
        "        x = tf.reshape(x, [10,num])\n",
        "        x  =  tf.cast(x, tf.float32)\n",
        "        return x\n",
        "\n",
        "    def _infinite_product(self, x, max_num):\n",
        "        num = tf.ones([3, ], tf.float32)\n",
        "        for i in range(max_num):\n",
        "            num *= x[i]   \n",
        "        return num\n",
        "\n",
        "    def _sigma(self, x, delta_v, b):\n",
        "        \n",
        "        num = 0.0\n",
        "        for i in range(b):\n",
        "            num += pow(self.gamma, i) * self._infinite_product(x, i) * delta_v[i]\n",
        "        return tf.cast(num, tf.float32)\n",
        "\n",
        "    def _compute_baseline_loss(self, advantages):\n",
        "        return .5 * tf.reduce_sum(tf.square(advantages))\n",
        "\n",
        "    def _compute_policy_gradient_loss(self, logits, actions, advantages):\n",
        "        cross_entropy = tf.losses.categorical_crossentropy(y_true=actions, y_pred=logits)\n",
        "        cross_entropy = tf.reshape(cross_entropy, [10,1])\n",
        "        advantages = tf.stop_gradient(advantages)\n",
        "        policy_gradient_loss_per_timestep = cross_entropy * advantages\n",
        "        return tf.reduce_sum(policy_gradient_loss_per_timestep)\n",
        "\n",
        "    def _compute_entropy_loss(self, logits):\n",
        "        log_policy = tf.math.log(logits)\n",
        "        entropy_per_timestep = tf.reduce_sum(-logits * log_policy, axis=-1)\n",
        "        return -tf.reduce_sum(entropy_per_timestep)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcPU3_nDFvro"
      },
      "source": [
        "class Learner(Critic):\n",
        "    def __init__(self):\n",
        "\n",
        "        conv_filter = 12\n",
        "        units = 28\n",
        "        look_back = 10\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "\n",
        "        input1_ = Input(shape=(look_back, 3))\n",
        "        input2_ = Input(shape=(look_back, 1))\n",
        "        input3_ = Input(shape=(look_back, 1))\n",
        "\n",
        "        x = Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\")(input1_)\n",
        "        x = MaxPool1D(pool_size=1, padding='same')(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        combined = concatenate([x, input2_, input3_],axis=-1)\n",
        "        common = LSTM(units, return_sequences=True)(combined)\n",
        "        common = Dense(units, kernel_initializer='random_uniform')(common)\n",
        "        common = Activation(\"relu\")(common)\n",
        "\n",
        "        actor  = Dense(3, activation=\"softmax\")(common)\n",
        "        critic = Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model([input1_, input2_, input3_], [actor, critic])\n",
        "        model.compile(loss = \"mean_absolute_error\", optimizer=opt)\n",
        "        model.summary()\n",
        "        #dot_img_file = './f\"{name}_model.png\"'\n",
        "        #tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
        "        self.model = model\n",
        "        super().__init__(model)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        length = memory.max_length_memory()\n",
        "        for i in range(length):\n",
        "            min = i\n",
        "            max = (i + 10)\n",
        "            state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu = memory.get_experiences(min, max)\n",
        "            self.valuenetwork(state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, learner):\n",
        "\n",
        "        self.learner = learner\n",
        "        self.model = clone_model(learner.model)\n",
        "        self.n_action = 3\n",
        "        self.state_arr = np.array([])\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.array([])\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def reset(self):\n",
        "        self.state_arr = np.empty((0,3), int)\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.empty((0,3), int)\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def policynetwork(self, state, prev_action, prev_reward):\n",
        "\n",
        "        if len(self.state_arr) == 10:\n",
        "            self.state_arr[0:-1] = self.state_arr[1:]\n",
        "            self.p_action_arr[0:-1] = self.p_action_arr[1:]\n",
        "            self.p_reward_arr[0:-1] = self.p_reward_arr[1:]\n",
        "            self.state_arr[-1] = state\n",
        "            self.p_action_arr[-1] = prev_action\n",
        "            self.p_reward_arr[-1] = prev_reward\n",
        "            tmp_state = copy.deepcopy(self.state_arr)\n",
        "            tmp_action = copy.deepcopy(self.p_action_arr)\n",
        "            tmp_reward = copy.deepcopy(self.p_reward_arr)\n",
        "            tmp_state = tmp_state.reshape(1,10,3)\n",
        "            tmp_action = tmp_action.reshape(1,10,1)\n",
        "            tmp_reward = tmp_reward.reshape(1,10,1)\n",
        "            act_p, v = self.model([tmp_state, tmp_action, tmp_reward]) # [-1.03245259 -0.55189404  0.87892511] 1 0\n",
        "            v_np = v.numpy()\n",
        "            p_np = act_p.numpy()\n",
        "            one_hot_actions = tf.one_hot([0,1,2], 3)\n",
        "            act_p = p_np[0][9]\n",
        "            mu = tf.reduce_sum(one_hot_actions * act_p, axis=1)\n",
        "            return np.random.choice(3, p=p_np[0][9]), v_np[0][9][0], mu.numpy()\n",
        "\n",
        "        self.state_arr = np.append(self.state_arr, np.array([state]), axis=0)\n",
        "        self.p_action_arr = np.append(self.p_action_arr, np.array([prev_action]))\n",
        "        self.p_reward_arr = np.append(self.p_reward_arr, np.array([prev_reward]))\n",
        "        return 1, 1, [0.0, 1.0, 0.0]\n",
        "\n",
        "    def policynetwork_next(self, next_state, action, reward):\n",
        "\n",
        "        if len(self.next_state_arr) == 10:\n",
        "            self.next_state_arr[0:-1] = self.next_state_arr[1:]\n",
        "            self.action_arr[0:-1] = self.action_arr[1:]\n",
        "            self.reward_arr[0:-1] = self.reward_arr[1:]\n",
        "            self.next_state_arr[-1] = next_state\n",
        "            self.action_arr[-1] = action\n",
        "            self.reward_arr[-1] = reward\n",
        "\n",
        "            tmp_n_state = copy.deepcopy(self.next_state_arr)\n",
        "            tmp_action = copy.deepcopy(self.action_arr)\n",
        "            tmp_reward = copy.deepcopy(self.reward_arr)\n",
        "            tmp_n_state = tmp_n_state.reshape(1,10,3)\n",
        "            tmp_action = tmp_action.reshape(1,10,1)\n",
        "            tmp_reward = tmp_reward.reshape(1,10,1)\n",
        "\n",
        "            _, v_next = self.model([tmp_n_state, tmp_action, tmp_reward])\n",
        "            v_next_np = v_next.numpy()\n",
        "            return v_next_np[0][9][0]\n",
        "\n",
        "        self.next_state_arr = np.append(self.next_state_arr, np.array([next_state]), axis=0)\n",
        "        self.action_arr = np.append(self.action_arr, np.array([action]))\n",
        "        self.reward_arr = np.append(self.reward_arr, np.array([reward]))\n",
        "        return 1.0\n",
        "\n",
        "    def load(self, name):\n",
        "        self.learner.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.learner.save(name)\n",
        "\n",
        "    def integration(self):\n",
        "        self.model = clone_model(self.learner.model)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        self.learner.placement(memory)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4-NrrtJBQWj"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : np.ndarray = np.empty((0,3), int)\n",
        "    next_state : np.ndarray = np.empty((0,3), int)\n",
        "    prev_action : np.ndarray = np.array([])\n",
        "    action : np.ndarray = np.array([])\n",
        "    prev_reward : np.ndarray = np.array([])\n",
        "    reward : np.ndarray = np.array([])\n",
        "    done : np.ndarray = np.array([])\n",
        "    v : np.ndarray = np.array([])\n",
        "    v_next : np.ndarray = np.array([])\n",
        "    mu : np.ndarray = np.empty((0,3), int)\n",
        "    minibatch_size : int = 64\n",
        "\n",
        "    def append_experiences(self, state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu):\n",
        "        self.state = np.append(self.state, np.array([state]), axis=0)\n",
        "        self.next_state = np.append(self.next_state, np.array([next_state]), axis=0)\n",
        "        self.prev_action = np.append(self.prev_action, np.array(prev_action))\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.prev_reward = np.append(self.prev_reward, np.array(prev_reward))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        self.done = np.append(self.done, np.array(done))\n",
        "        self.v = np.append(self.v, np.array(v))\n",
        "        self.v_next = np.append(self.v_next, np.array(v_next))\n",
        "        self.mu = np.append(self.mu, np.array([mu]), axis=0)\n",
        "\n",
        "    def max_length_memory(self):\n",
        "        max_len = len(self.state)\n",
        "        max_len = int(max_len) - 11\n",
        "\n",
        "        return max_len\n",
        "\n",
        "    def get_experiences(self, min, max):\n",
        "        state, next_state, mu = np.empty((0,3), int), np.empty((0,3), int), np.empty((0,3), int)\n",
        "        prev_action, action, prev_reward, reward, done, v, v_next = np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
        "        for i in range(min, max):\n",
        "            state = np.append(state, np.array([self.state[i]]), axis=0)\n",
        "            next_state = np.append(next_state, self.action[i])\n",
        "            prev_action = np.append(prev_action, self.prev_action[i])\n",
        "            action = np.append(action, self.action[i])\n",
        "            prev_reward = np.append(prev_reward, self.prev_reward[i])\n",
        "            reward = np.append(reward, self.reward[i])\n",
        "            done = np.append(done, self.done[i])\n",
        "            v = np.append(v, self.v[i])\n",
        "            v_next = np.append(v_next, self.v_next[i])\n",
        "            mu = np.append(mu, np.array([self.mu[i]]), axis=0)\n",
        "\n",
        "        return state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, num, mdl_dir, name, batch_size = 10, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "        \n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            self.actor.reset()\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory()\n",
        "            prev_action = 1\n",
        "            prev_reward = 0\n",
        "            i = 0\n",
        "    \n",
        "            while not done:\n",
        "                action, v, mu = self.actor.policynetwork(state, prev_action, prev_reward)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "                v_next = self.actor.policynetwork_next(next_state, action, reward)\n",
        "\n",
        "                if (i > self.batch_size) and (self.mode == 'train'):\n",
        "                    memory.append_experiences(state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu)\n",
        "\n",
        "                state = next_state\n",
        "                prev_action = action\n",
        "                prev_reward = reward\n",
        "                i += 1\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                self.actor.placement(memory)\n",
        "                self.actor.integration()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6776671c-a992-4c8c-d971-3445b4ef8663"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 25\n",
        "batch_size = 10\n",
        "learner = Learner()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "    actor = Actor(learner)\n",
        "    main = Main(env, actor, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 10, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 10, 12)       48          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 10, 12)       0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 10, 12)       0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 14)       0           activation[0][0]                 \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 10, 28)       4816        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10, 28)       812         lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 10, 28)       0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10, 3)        87          activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10, 1)        29          activation_1[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,792\n",
            "Trainable params: 5,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/25 RapTime: 0:01:17.035002 FixedProfit: 894860 TradeTimes: 141 TradeWin: 88\n",
            "Episode: 1/25 RapTime: 0:01:17.073601 FixedProfit: 1477529 TradeTimes: 130 TradeWin: 85\n",
            "Episode: 1/25 RapTime: 0:01:17.433389 FixedProfit: 1109455 TradeTimes: 150 TradeWin: 87\n",
            "Episode: 1/25 RapTime: 0:01:17.456742 FixedProfit: 1512022 TradeTimes: 165 TradeWin: 100\n",
            "Episode: 2/25 RapTime: 0:01:13.196219 FixedProfit: 1775503 TradeTimes: 160 TradeWin: 107\n",
            "Episode: 2/25 RapTime: 0:01:13.365507 FixedProfit: 1221087 TradeTimes: 144 TradeWin: 89\n",
            "Episode: 2/25 RapTime: 0:01:13.214034 FixedProfit: 1354630 TradeTimes: 133 TradeWin: 79\n",
            "Episode: 2/25 RapTime: 0:01:13.311662 FixedProfit: 1149300 TradeTimes: 152 TradeWin: 86\n",
            "Episode: 3/25 RapTime: 0:01:13.402675 FixedProfit: 1438774 TradeTimes: 152 TradeWin: 92\n",
            "Episode: 3/25 RapTime: 0:01:13.266297 FixedProfit: 1121444 TradeTimes: 150 TradeWin: 82\n",
            "Episode: 3/25 RapTime: 0:01:13.408159 FixedProfit: 1058207 TradeTimes: 135 TradeWin: 82\n",
            "Episode: 3/25 RapTime: 0:01:13.626973 FixedProfit: 1243490 TradeTimes: 150 TradeWin: 92\n",
            "Episode: 4/25 RapTime: 0:01:13.035363 FixedProfit: 1733866 TradeTimes: 156 TradeWin: 96\n",
            "Episode: 4/25 RapTime: 0:01:13.141212 FixedProfit: 1237068 TradeTimes: 143 TradeWin: 86\n",
            "Episode: 4/25 RapTime: 0:01:12.767898 FixedProfit: 1079502 TradeTimes: 150 TradeWin: 85\n",
            "Episode: 4/25 RapTime: 0:01:12.824326 FixedProfit: 1441673 TradeTimes: 149 TradeWin: 79\n",
            "Episode: 5/25 RapTime: 0:01:12.711876 FixedProfit: 1498712 TradeTimes: 148 TradeWin: 95\n",
            "Episode: 5/25 RapTime: 0:01:12.592653 FixedProfit: 1596220 TradeTimes: 147 TradeWin: 91\n",
            "Episode: 5/25 RapTime: 0:01:13.065175 FixedProfit: 1952603 TradeTimes: 149 TradeWin: 100\n",
            "Episode: 5/25 RapTime: 0:01:13.290893 FixedProfit: 1540300 TradeTimes: 149 TradeWin: 94\n",
            "Episode: 6/25 RapTime: 0:01:12.527025 FixedProfit: 1550608 TradeTimes: 154 TradeWin: 86\n",
            "Episode: 6/25 RapTime: 0:01:12.485793 FixedProfit: 1110207 TradeTimes: 142 TradeWin: 83\n",
            "Episode: 6/25 RapTime: 0:01:12.875415 FixedProfit: 1211077 TradeTimes: 145 TradeWin: 91\n",
            "Episode: 6/25 RapTime: 0:01:12.752199 FixedProfit: 983264 TradeTimes: 139 TradeWin: 91\n",
            "Episode: 7/25 RapTime: 0:01:12.960933 FixedProfit: 996501 TradeTimes: 139 TradeWin: 87\n",
            "Episode: 7/25 RapTime: 0:01:12.878032 FixedProfit: 1162643 TradeTimes: 142 TradeWin: 90\n",
            "Episode: 7/25 RapTime: 0:01:13.168162 FixedProfit: 1082993 TradeTimes: 143 TradeWin: 88\n",
            "Episode: 7/25 RapTime: 0:01:13.203143 FixedProfit: 1039801 TradeTimes: 143 TradeWin: 80\n",
            "Episode: 8/25 RapTime: 0:01:12.383542 FixedProfit: 850439 TradeTimes: 141 TradeWin: 74\n",
            "Episode: 8/25 RapTime: 0:01:12.672612 FixedProfit: 1461894 TradeTimes: 139 TradeWin: 88\n",
            "Episode: 8/25 RapTime: 0:01:13.065588 FixedProfit: 1060717 TradeTimes: 131 TradeWin: 78\n",
            "Episode: 8/25 RapTime: 0:01:12.993417 FixedProfit: 1320833 TradeTimes: 126 TradeWin: 81\n",
            "Episode: 9/25 RapTime: 0:01:13.143927 FixedProfit: 1149992 TradeTimes: 144 TradeWin: 95\n",
            "Episode: 9/25 RapTime: 0:01:12.459695 FixedProfit: 1387008 TradeTimes: 143 TradeWin: 91\n",
            "Episode: 9/25 RapTime: 0:01:12.635719 FixedProfit: 1609099 TradeTimes: 150 TradeWin: 90\n",
            "Episode: 9/25 RapTime: 0:01:12.841890 FixedProfit: 1224567 TradeTimes: 140 TradeWin: 81\n",
            "Episode: 10/25 RapTime: 0:01:12.751478 FixedProfit: 1640932 TradeTimes: 148 TradeWin: 93\n",
            "Episode: 10/25 RapTime: 0:01:12.525738 FixedProfit: 1592444 TradeTimes: 152 TradeWin: 96\n",
            "Episode: 10/25 RapTime: 0:01:12.952134 FixedProfit: 1544186 TradeTimes: 139 TradeWin: 90\n",
            "Episode: 10/25 RapTime: 0:01:12.771617 FixedProfit: 1971250 TradeTimes: 151 TradeWin: 95\n",
            "Episode: 11/25 RapTime: 0:01:12.634135 FixedProfit: 1204905 TradeTimes: 143 TradeWin: 85\n",
            "Episode: 11/25 RapTime: 0:01:12.398538 FixedProfit: 1148940 TradeTimes: 147 TradeWin: 87\n",
            "Episode: 11/25 RapTime: 0:01:12.425011 FixedProfit: 1252512 TradeTimes: 147 TradeWin: 97\n",
            "Episode: 11/25 RapTime: 0:01:12.633667 FixedProfit: 900054 TradeTimes: 138 TradeWin: 80\n",
            "Episode: 12/25 RapTime: 0:01:12.427565 FixedProfit: 860009 TradeTimes: 149 TradeWin: 84\n",
            "Episode: 12/25 RapTime: 0:01:12.543056 FixedProfit: 1005927 TradeTimes: 150 TradeWin: 81\n",
            "Episode: 12/25 RapTime: 0:01:13.017288 FixedProfit: 1143252 TradeTimes: 129 TradeWin: 79\n",
            "Episode: 12/25 RapTime: 0:01:12.600084 FixedProfit: 922047 TradeTimes: 145 TradeWin: 80\n",
            "Episode: 13/25 RapTime: 0:01:12.136774 FixedProfit: 1062379 TradeTimes: 139 TradeWin: 85\n",
            "Episode: 13/25 RapTime: 0:01:12.206696 FixedProfit: 1058011 TradeTimes: 142 TradeWin: 90\n",
            "Episode: 13/25 RapTime: 0:01:12.553327 FixedProfit: 1038971 TradeTimes: 150 TradeWin: 96\n",
            "Episode: 13/25 RapTime: 0:01:12.633479 FixedProfit: 906970 TradeTimes: 143 TradeWin: 82\n",
            "Episode: 14/25 RapTime: 0:01:12.416392 FixedProfit: 1304080 TradeTimes: 152 TradeWin: 89\n",
            "Episode: 14/25 RapTime: 0:01:12.511918 FixedProfit: 2038181 TradeTimes: 153 TradeWin: 103\n",
            "Episode: 14/25 RapTime: 0:01:12.584957 FixedProfit: 1207929 TradeTimes: 141 TradeWin: 90\n",
            "Episode: 14/25 RapTime: 0:01:12.549940 FixedProfit: 1069688 TradeTimes: 144 TradeWin: 93\n",
            "Episode: 15/25 RapTime: 0:01:11.921471 FixedProfit: 1615847 TradeTimes: 151 TradeWin: 104\n",
            "Episode: 15/25 RapTime: 0:01:12.009799 FixedProfit: 1152246 TradeTimes: 144 TradeWin: 88\n",
            "Episode: 15/25 RapTime: 0:01:12.085671 FixedProfit: 1061036 TradeTimes: 148 TradeWin: 79\n",
            "Episode: 15/25 RapTime: 0:01:12.258906 FixedProfit: 1895225 TradeTimes: 146 TradeWin: 86\n",
            "Episode: 16/25 RapTime: 0:01:12.449090 FixedProfit: 1203434 TradeTimes: 141 TradeWin: 87\n",
            "Episode: 16/25 RapTime: 0:01:12.425236 FixedProfit: 1263140 TradeTimes: 143 TradeWin: 85\n",
            "Episode: 16/25 RapTime: 0:01:12.804988 FixedProfit: 1365675 TradeTimes: 148 TradeWin: 86\n",
            "Episode: 16/25 RapTime: 0:01:12.631416 FixedProfit: 1761910 TradeTimes: 143 TradeWin: 90\n",
            "Episode: 17/25 RapTime: 0:01:11.799862 FixedProfit: 1240760 TradeTimes: 144 TradeWin: 81\n",
            "Episode: 17/25 RapTime: 0:01:12.081639 FixedProfit: 1073716 TradeTimes: 143 TradeWin: 88\n",
            "Episode: 17/25 RapTime: 0:01:12.206267 FixedProfit: 1690161 TradeTimes: 148 TradeWin: 95\n",
            "Episode: 17/25 RapTime: 0:01:12.232487 FixedProfit: 1424974 TradeTimes: 149 TradeWin: 95\n",
            "Episode: 18/25 RapTime: 0:01:12.306219 FixedProfit: 892204 TradeTimes: 158 TradeWin: 91\n",
            "Episode: 18/25 RapTime: 0:01:12.090083 FixedProfit: 1116219 TradeTimes: 137 TradeWin: 81\n",
            "Episode: 18/25 RapTime: 0:01:12.208329 FixedProfit: 1607438 TradeTimes: 148 TradeWin: 97\n",
            "Episode: 18/25 RapTime: 0:01:12.017001 FixedProfit: 1308620 TradeTimes: 127 TradeWin: 83\n",
            "Episode: 19/25 RapTime: 0:01:12.006953 FixedProfit: 1079209 TradeTimes: 151 TradeWin: 94\n",
            "Episode: 19/25 RapTime: 0:01:11.766338 FixedProfit: 1134213 TradeTimes: 129 TradeWin: 74\n",
            "Episode: 19/25 RapTime: 0:01:12.027525 FixedProfit: 1367438 TradeTimes: 147 TradeWin: 89\n",
            "Episode: 19/25 RapTime: 0:01:12.308682 FixedProfit: 1039046 TradeTimes: 144 TradeWin: 94\n",
            "Episode: 20/25 RapTime: 0:01:12.356626 FixedProfit: 1629409 TradeTimes: 135 TradeWin: 86\n",
            "Episode: 20/25 RapTime: 0:01:12.133558 FixedProfit: 1333292 TradeTimes: 155 TradeWin: 94\n",
            "Episode: 20/25 RapTime: 0:01:12.388718 FixedProfit: 1478449 TradeTimes: 136 TradeWin: 90\n",
            "Episode: 20/25 RapTime: 0:01:12.370973 FixedProfit: 1013435 TradeTimes: 147 TradeWin: 96\n",
            "Episode: 21/25 RapTime: 0:01:11.311592 FixedProfit: 1333552 TradeTimes: 141 TradeWin: 88\n",
            "Episode: 21/25 RapTime: 0:01:11.833626 FixedProfit: 830473 TradeTimes: 148 TradeWin: 89\n",
            "Episode: 21/25 RapTime: 0:01:11.586123 FixedProfit: 1348251 TradeTimes: 144 TradeWin: 83\n",
            "Episode: 21/25 RapTime: 0:01:11.538727 FixedProfit: 1001517 TradeTimes: 132 TradeWin: 79\n",
            "Episode: 22/25 RapTime: 0:01:11.377332 FixedProfit: 1224568 TradeTimes: 150 TradeWin: 96\n",
            "Episode: 22/25 RapTime: 0:01:11.033470 FixedProfit: 1734407 TradeTimes: 147 TradeWin: 101\n",
            "Episode: 22/25 RapTime: 0:01:11.238243 FixedProfit: 1621889 TradeTimes: 146 TradeWin: 97\n",
            "Episode: 22/25 RapTime: 0:01:11.356745 FixedProfit: 1073481 TradeTimes: 137 TradeWin: 90\n",
            "Episode: 23/25 RapTime: 0:01:10.489359 FixedProfit: 1215124 TradeTimes: 144 TradeWin: 77\n",
            "Episode: 23/25 RapTime: 0:01:10.103903 FixedProfit: 1501672 TradeTimes: 153 TradeWin: 100\n",
            "Episode: 23/25 RapTime: 0:01:10.254687 FixedProfit: 1071812 TradeTimes: 149 TradeWin: 88\n",
            "Episode: 23/25 RapTime: 0:01:10.118347 FixedProfit: 1295547 TradeTimes: 142 TradeWin: 90\n",
            "Episode: 24/25 RapTime: 0:01:10.167074 FixedProfit: 1536538 TradeTimes: 142 TradeWin: 89\n",
            "Episode: 24/25 RapTime: 0:01:10.263670 FixedProfit: 1327238 TradeTimes: 148 TradeWin: 89\n",
            "Episode: 24/25 RapTime: 0:01:10.121102 FixedProfit: 1137958 TradeTimes: 152 TradeWin: 88\n",
            "Episode: 24/25 RapTime: 0:01:10.351089 FixedProfit: 1519473 TradeTimes: 129 TradeWin: 90\n",
            "Episode: 25/25 RapTime: 0:01:10.669427 FixedProfit: 1321864 TradeTimes: 142 TradeWin: 98\n",
            "Episode: 25/25 RapTime: 0:01:10.899358 FixedProfit: 1210679 TradeTimes: 155 TradeWin: 90\n",
            "Episode: 25/25 RapTime: 0:01:10.023181 FixedProfit: 1204804 TradeTimes: 152 TradeWin: 88\n",
            "Episode: 25/25 RapTime: 0:01:08.800581 FixedProfit: 1598188 TradeTimes: 146 TradeWin: 95\n"
          ]
        }
      ]
    }
  ]
}