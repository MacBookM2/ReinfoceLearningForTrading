{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "impala_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/impala_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "4052a8d1-db87-4f04-deb1-0a5081dc6782"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'train'\n",
        "name = 'impala'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class Learner(Critic):\n",
        "    def __init__(self):\n",
        "\n",
        "        conv_filter = 4\n",
        "        units = 8\n",
        "        batch_size = 8\n",
        "        look_back = 10\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "\n",
        "        input1_ = Input(shape=(batch_size, 1))\n",
        "        input2_ = keras.layers.Input(shape=(1,))\n",
        "        input3_ = keras.layers.Input(shape=(1,))\n",
        "\n",
        "        x = Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\")(input1_)\n",
        "        #x = Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\",batch_input_shape=(None, look_back, 1))\n",
        "        x = MaxPool1D(pool_size=1, padding='same')\n",
        "        x = Activation(\"relu\")\n",
        "\n",
        "        combined = concatenate([x.output, input2_, input3_])\n",
        "        common = LSTM(units, return_sequences=True)(combined)\n",
        "        common = Dense(1, kernel_initializer='random_uniform')\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model([input1_, input2_, input3_], [actor, critic])\n",
        "        mastermodel.compile(loss = \"mean_absolute_error\", optimizer=opt)\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "        super().__init__(mastermodel)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        length = memory.max_length_memory()\n",
        "        for i in range(length):\n",
        "            min = i * 10\n",
        "            max = (i + 1) * 10\n",
        "            state, action, reward, next_state, done, v, v_next, mu = memory.get_experiences(min, max)\n",
        "            self.valuenetwork(state, action, reward, next_state, done, v, v_next, mu)\n",
        "\n",
        "    def layering(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m48th46c8otj"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model):\n",
        "        self.model = model\n",
        "        self.n_action = 3\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def valuenetwork(self, state, action, reward, next_state, done, v, v_next, mu):\n",
        "\n",
        "        # V-trace\n",
        "        onehot_actions = tf.one_hot(action, self.n_action)\n",
        "\n",
        "        pai, v      = self.model(state.reshape((1,-1)))\n",
        "\n",
        "        pai_probs = tf.nn.softmax(pai)\n",
        "        pais = tf.reduce_sum(onehot_actions * pai_probs, axis=1, keepdims=True)\n",
        "\n",
        "        ratio = tf.math.divide_no_nan(pais, mu)\n",
        "        rhoi = ci = tf.minimum(1.0, ratio)\n",
        "\n",
        "        _, n_num =ratio.shape\n",
        "        delta_v = roi * (reward + self.gamma*v_next - v)\n",
        "        v_trace =vs + _sigma(x, delta_v, self.gamma, n_num)\n",
        "\n",
        "    def _infinite_product(x, max_num):\n",
        "        num = 1.0\n",
        "        for i in range(max_num):\n",
        "            num *= x[i]   \n",
        "        return num\n",
        "\n",
        "    def _sigma(x, delta_v, gamma, b):\n",
        "        num = 0.0\n",
        "        for i in range(b):\n",
        "            num += pow(gamma, i) * _infinite_product(x, i) * delta_v[i]\n",
        "        return num\n",
        "\n",
        "    def _theta_loss_grad(self, v_trace, v):\n",
        "\n",
        "        vtheta = copy.copy(v)\n",
        "\n",
        "        with tf.GradientTape() as t:\n",
        "            t.watch(v)\n",
        "            # どうやって偏微分するのか\n",
        "\n",
        "        dy_dx = t.gradient(y, v)\n",
        "    return (v_trace - vtheta) * dy_dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        conv_filter = 4\n",
        "        units = 8\n",
        "        batch_size = 8\n",
        "        look_back = 10\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "\n",
        "        input1_ = Input(shape=(batch_size, 1))\n",
        "        input2_ = keras.layers.Input(shape=(1,))\n",
        "        input3_ = keras.layers.Input(shape=(1,))\n",
        "\n",
        "        x = Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\")(input1_)\n",
        "        x = MaxPool1D(pool_size=1, padding='same')\n",
        "        x = Activation(\"relu\")\n",
        "\n",
        "        combined = concatenate([x.output, input2_, input3_])\n",
        "        common = LSTM(units, return_sequences=True)(combined)\n",
        "        common = Dense(1, kernel_initializer='random_uniform')\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model([input1_, input2_, input3_], [actor, critic])\n",
        "        model.compile(loss = \"mean_absolute_error\", optimizer=opt)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self, learner):\n",
        "\n",
        "        self.learner = learner\n",
        "        self.model = learner.mastermodel\n",
        "        self.n_action = 3\n",
        "        self.state_arr = np.array([])\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.array([])\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def reset(self):\n",
        "        self.state_arr = np.empty((0,3), int)\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.empty((0,3), int)\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def policynetwork(self, state, prev_action, prev_reward):\n",
        "\n",
        "        if len(self.state_arr) == 10:\n",
        "            self.state_arr[0:-1] = self.state_arr[1:]\n",
        "            self.p_action_arr[0:-1] = self.p_action_arr[1:]\n",
        "            self.p_reward_arr[0:-1] = self.p_reward_arr[1:]\n",
        "            self.state_arr[-1] = state\n",
        "            self.p_action_arr[-1] = prev_action\n",
        "            self.p_reward_arr[-1] = prev_reward\n",
        "        else:\n",
        "            self.state_arr = np.append(self.state_arr, np.array([state]), axis=0)\n",
        "            self.p_action_arr = np.append(self.p_action_arr, np.array([prev_action]))\n",
        "            self.p_reward_arr = np.append(self.p_reward_arr, np.array([prev_reward]))\n",
        "\n",
        "        act_p, v = self.model(self.next_state_arr, self.action_arr, self.reward_arr)\n",
        "        one_hot_actions = tf.one_hot([0,1,2], 3)\n",
        "        act_p = tf.nn.softmax(act_p)\n",
        "        mu = tf.reduce_sum(one_hot_actions * act_p, axis=1)\n",
        "        return np.random.choice(3, p=act_p[9].numpy()), v, mu.numpy()\n",
        "\n",
        "    def policynetwork_next(self, next_state, action, reward):\n",
        "\n",
        "        if len(self.next_state_arr) == 10:\n",
        "            self.next_state_arr[0:-1] = self.next_state_arr[1:]\n",
        "            self.action_arr[0:-1] = self.action_arr[1:]\n",
        "            self.reward_arr[0:-1] = self.reward_arr[1:]\n",
        "            self.next_state_arr[-1] = next_state\n",
        "            self.action_arr[-1] = action\n",
        "            self.reward_arr[-1] = reward\n",
        "        else:\n",
        "            self.next_state_arr = np.append(self.next_state_arr, np.array([next_state]), axis=0)\n",
        "            self.action_arr = np.append(self.action_arr, np.array([action]))\n",
        "            self.reward_arr = np.append(self.reward_arr, np.array([reward]))\n",
        "        \n",
        "        _, v_next = self.model(self.state_arr, self.p_action_arr, self.p_reward_arr)\n",
        "        return v_next\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        self.learner.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.learner.save(name)\n",
        "\n",
        "    def integration(self, model):\n",
        "        self.learner.layering(self.model)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        self.learner.placement(memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4-NrrtJBQWj"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : np.ndarray = np.empty((0,3), int)\n",
        "    action : np.ndarray = np.array([])\n",
        "    reward : np.ndarray = np.array([])\n",
        "    next_state : np.ndarray = np.empty((0,3), int)\n",
        "    done : np.ndarray = np.array([])\n",
        "    v : np.ndarray = np.array([])\n",
        "    v_next : np.ndarray = np.array([])\n",
        "    mu : np.ndarray = np.empty((0,3), int)\n",
        "    minibatch_size : int = 64\n",
        "\n",
        "    def append_experiences(self, state, action, reward, next_state, done, v, v_next, mu):\n",
        "        self.state = np.append(self.state, np.array([state]), axis=0)\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        self.next_state = np.append(self.next_state, np.array([next_state]), axis=0)\n",
        "        self.done = np.append(self.done, np.array(done))\n",
        "        v = v.numpy()\n",
        "        v = v.values\n",
        "        self.v = np.append(self.v, np.array(v))\n",
        "        v_next = v_next.numpy()\n",
        "        v_next = v_next.values\n",
        "        self.v_next = np.append(self.v_next, np.array(v_next))\n",
        "        self.mu = np.append(self.mu, np.array([mu]), axis=0)\n",
        "\n",
        "    def max_length_memory(self):\n",
        "        \n",
        "        max_len = len(self.state)\n",
        "        max_len = int(max_len/10)\n",
        "\n",
        "        return max_len\n",
        "\n",
        "    def get_experiences(self, min, max):\n",
        "        state, next_state, mu = np.empty((0,3), int), np.empty((0,3), int), np.empty((0,3), int)\n",
        "        action, reward, done, v, v_next = np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
        "        for i in range(min, (max + 1)):\n",
        "            state = np.append(state, np.array([self.state[i]]), axis=0)\n",
        "            action = np.append(action, self.action[i])\n",
        "            reward = np.append(reward, self.reward[i])\n",
        "            next_state = np.append(next_state, np.array([self.next_state[i]]), axis=0)\n",
        "            done = np.append(done, self.done[i])\n",
        "            v = np.append(v, self.v[i])\n",
        "            v_next = np.append(v_next, self.v_next[i])\n",
        "            mu = np.append(mu, np.array([self.mu[i]]), axis=0)\n",
        "\n",
        "        return state, action, reward, next_state, done, v, v_next, mu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, num, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "        \n",
        "        self.actor.integration()\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            self.actor = reset()\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory()\n",
        "            prev_action = 1\n",
        "            prev_reward = 0\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action, v, mu = self.actor.policynetwork(state, prev_action, prev_reward)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "                v_next = self.actor.policynetwork_next(next_state, action, reward)\n",
        "                memory.append_experiences(state, action, reward, next_state, done, v, v_next, mu)\n",
        "\n",
        "                state = next_state\n",
        "                prev_action = action\n",
        "                prev_reward = reward\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                self.actor.placement(memory)\n",
        "                self.actor.integration()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcaac04-3ed9-4354-e01e-76035350f431"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 50\n",
        "batch_size = 32\n",
        "Learner = Learner()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    actor = Actor(Learner)\n",
        "    main = Main(env, actor, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:00:07.546968 FixedProfit: 1169989\n",
            "Episode: 1/50 RapTime: 0:00:07.612042 FixedProfit: 1059467\n",
            "Episode: 1/50 RapTime: 0:00:07.615279 FixedProfit: 1158603\n",
            "Episode: 1/50 RapTime: 0:00:07.632192 FixedProfit: 1165455\n",
            "Episode: 2/50 RapTime: 0:00:05.927232 FixedProfit: 1016367\n",
            "Episode: 2/50 RapTime: 0:00:06.025182 FixedProfit: 1276579\n",
            "Episode: 2/50 RapTime: 0:00:05.948696 FixedProfit: 1201259\n",
            "Episode: 2/50 RapTime: 0:00:05.986488 FixedProfit: 984656\n",
            "Episode: 3/50 RapTime: 0:00:06.130633 FixedProfit: 1169294\n",
            "Episode: 3/50 RapTime: 0:00:06.145895 FixedProfit: 885612\n",
            "Episode: 3/50 RapTime: 0:00:06.158131 FixedProfit: 1420326\n",
            "Episode: 3/50 RapTime: 0:00:06.148910 FixedProfit: 1269692\n",
            "Episode: 4/50 RapTime: 0:00:06.073014 FixedProfit: 1026756\n",
            "Episode: 4/50 RapTime: 0:00:06.115491 FixedProfit: 936162\n",
            "Episode: 4/50 RapTime: 0:00:06.120120 FixedProfit: 1076578\n",
            "Episode: 4/50 RapTime: 0:00:06.138792 FixedProfit: 1197165\n",
            "Episode: 5/50 RapTime: 0:00:06.154226 FixedProfit: 1212076\n",
            "Episode: 5/50 RapTime: 0:00:06.266450 FixedProfit: 1122122\n",
            "Episode: 5/50 RapTime: 0:00:06.269729 FixedProfit: 1625372\n",
            "Episode: 5/50 RapTime: 0:00:06.290006 FixedProfit: 1197165\n",
            "Episode: 6/50 RapTime: 0:00:06.158766 FixedProfit: 1074539\n",
            "Episode: 6/50 RapTime: 0:00:06.163094 FixedProfit: 1045249\n",
            "Episode: 6/50 RapTime: 0:00:06.162303 FixedProfit: 1298897\n",
            "Episode: 6/50 RapTime: 0:00:06.196075 FixedProfit: 1197165\n",
            "Episode: 7/50 RapTime: 0:00:06.219585 FixedProfit: 1138189\n",
            "Episode: 7/50 RapTime: 0:00:06.240118 FixedProfit: 1176741\n",
            "Episode: 7/50 RapTime: 0:00:06.246839 FixedProfit: 977041\n",
            "Episode: 7/50 RapTime: 0:00:06.236636 FixedProfit: 1197165\n",
            "Episode: 8/50 RapTime: 0:00:06.041817 FixedProfit: 1224014\n",
            "Episode: 8/50 RapTime: 0:00:06.106552 FixedProfit: 1218163\n",
            "Episode: 8/50 RapTime: 0:00:06.048483 FixedProfit: 1195052\n",
            "Episode: 8/50 RapTime: 0:00:06.102722 FixedProfit: 1197165\n",
            "Episode: 9/50 RapTime: 0:00:05.993672 FixedProfit: 1144284\n",
            "Episode: 9/50 RapTime: 0:00:05.983082 FixedProfit: 1109722\n",
            "Episode: 9/50 RapTime: 0:00:06.050194 FixedProfit: 1029899\n",
            "Episode: 9/50 RapTime: 0:00:06.002228 FixedProfit: 1197165\n",
            "Episode: 10/50 RapTime: 0:00:06.015348 FixedProfit: 1101549\n",
            "Episode: 10/50 RapTime: 0:00:06.022349 FixedProfit: 1221590\n",
            "Episode: 10/50 RapTime: 0:00:06.027200 FixedProfit: 1056139\n",
            "Episode: 10/50 RapTime: 0:00:06.059197 FixedProfit: 1197165\n",
            "Episode: 11/50 RapTime: 0:00:06.062198 FixedProfit: 993236\n",
            "Episode: 11/50 RapTime: 0:00:06.078155 FixedProfit: 979895\n",
            "Episode: 11/50 RapTime: 0:00:06.072121 FixedProfit: 1047003\n",
            "Episode: 11/50 RapTime: 0:00:06.042640 FixedProfit: 1197165\n",
            "Episode: 12/50 RapTime: 0:00:06.011496 FixedProfit: 1089737\n",
            "Episode: 12/50 RapTime: 0:00:06.048210 FixedProfit: 1182831\n",
            "Episode: 12/50 RapTime: 0:00:06.042010 FixedProfit: 1170386\n",
            "Episode: 12/50 RapTime: 0:00:06.064099 FixedProfit: 1197165\n",
            "Episode: 13/50 RapTime: 0:00:06.026756 FixedProfit: 1210904\n",
            "Episode: 13/50 RapTime: 0:00:06.088239 FixedProfit: 958649\n",
            "Episode: 13/50 RapTime: 0:00:06.040379 FixedProfit: 1233376\n",
            "Episode: 13/50 RapTime: 0:00:06.052856 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:05.944771 FixedProfit: 1093307\n",
            "Episode: 14/50 RapTime: 0:00:05.940498 FixedProfit: 1219652\n",
            "Episode: 14/50 RapTime: 0:00:06.004286 FixedProfit: 1182110\n",
            "Episode: 14/50 RapTime: 0:00:05.983681 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:06.156052 FixedProfit: 1091974\n",
            "Episode: 15/50 RapTime: 0:00:06.133122 FixedProfit: 1042793\n",
            "Episode: 15/50 RapTime: 0:00:06.179996 FixedProfit: 1116506\n",
            "Episode: 15/50 RapTime: 0:00:06.146232 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:06.360817 FixedProfit: 1233203\n",
            "Episode: 16/50 RapTime: 0:00:06.332426 FixedProfit: 956502\n",
            "Episode: 16/50 RapTime: 0:00:06.366972 FixedProfit: 1088828\n",
            "Episode: 16/50 RapTime: 0:00:06.341361 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:06.363333 FixedProfit: 940442\n",
            "Episode: 17/50 RapTime: 0:00:06.358656 FixedProfit: 961987\n",
            "Episode: 17/50 RapTime: 0:00:06.288780 FixedProfit: 1019982\n",
            "Episode: 17/50 RapTime: 0:00:06.377729 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:06.145711 FixedProfit: 1036579\n",
            "Episode: 18/50 RapTime: 0:00:06.225719 FixedProfit: 1006064\n",
            "Episode: 18/50 RapTime: 0:00:06.278436 FixedProfit: 1049857\n",
            "Episode: 18/50 RapTime: 0:00:06.220177 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:06.001748 FixedProfit: 1051717\n",
            "Episode: 19/50 RapTime: 0:00:06.068594 FixedProfit: 1166106\n",
            "Episode: 19/50 RapTime: 0:00:05.966148 FixedProfit: 959199\n",
            "Episode: 19/50 RapTime: 0:00:06.006401 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:06.061467 FixedProfit: 1046759\n",
            "Episode: 20/50 RapTime: 0:00:06.099119 FixedProfit: 1271377\n",
            "Episode: 20/50 RapTime: 0:00:06.141025 FixedProfit: 1027075\n",
            "Episode: 20/50 RapTime: 0:00:06.121142 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:06.341789 FixedProfit: 1158959\n",
            "Episode: 21/50 RapTime: 0:00:06.292098 FixedProfit: 1068032\n",
            "Episode: 21/50 RapTime: 0:00:06.300734 FixedProfit: 978498\n",
            "Episode: 21/50 RapTime: 0:00:06.195991 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:06.169159 FixedProfit: 1047075\n",
            "Episode: 22/50 RapTime: 0:00:06.168447 FixedProfit: 930156\n",
            "Episode: 22/50 RapTime: 0:00:06.242809 FixedProfit: 998871\n",
            "Episode: 22/50 RapTime: 0:00:06.191690 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:06.042264 FixedProfit: 1178661\n",
            "Episode: 23/50 RapTime: 0:00:06.004584 FixedProfit: 1128292\n",
            "Episode: 23/50 RapTime: 0:00:05.985177 FixedProfit: 1203884\n",
            "Episode: 23/50 RapTime: 0:00:05.992424 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:06.016131 FixedProfit: 1034592\n",
            "Episode: 24/50 RapTime: 0:00:06.069196 FixedProfit: 1140418\n",
            "Episode: 24/50 RapTime: 0:00:06.048961 FixedProfit: 933008\n",
            "Episode: 24/50 RapTime: 0:00:06.052715 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:06.018964 FixedProfit: 910735\n",
            "Episode: 25/50 RapTime: 0:00:06.041542 FixedProfit: 930733\n",
            "Episode: 25/50 RapTime: 0:00:06.012744 FixedProfit: 864340\n",
            "Episode: 25/50 RapTime: 0:00:06.045250 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:06.247580 FixedProfit: 1003332\n",
            "Episode: 26/50 RapTime: 0:00:06.304110 FixedProfit: 1092560\n",
            "Episode: 26/50 RapTime: 0:00:06.261823 FixedProfit: 1109810\n",
            "Episode: 26/50 RapTime: 0:00:06.289948 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:06.278431 FixedProfit: 1046846\n",
            "Episode: 27/50 RapTime: 0:00:06.238470 FixedProfit: 957322\n",
            "Episode: 27/50 RapTime: 0:00:06.253275 FixedProfit: 1016478\n",
            "Episode: 27/50 RapTime: 0:00:06.231033 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:06.304103 FixedProfit: 1198688\n",
            "Episode: 28/50 RapTime: 0:00:06.234365 FixedProfit: 1044496\n",
            "Episode: 28/50 RapTime: 0:00:06.293816 FixedProfit: 1021163\n",
            "Episode: 28/50 RapTime: 0:00:06.312497 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:06.165134 FixedProfit: 1106325\n",
            "Episode: 29/50 RapTime: 0:00:06.273868 FixedProfit: 1044213\n",
            "Episode: 29/50 RapTime: 0:00:06.248123 FixedProfit: 1030029\n",
            "Episode: 29/50 RapTime: 0:00:06.247923 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:06.061009 FixedProfit: 1186987\n",
            "Episode: 30/50 RapTime: 0:00:06.004254 FixedProfit: 1131280\n",
            "Episode: 30/50 RapTime: 0:00:06.012797 FixedProfit: 1002018\n",
            "Episode: 30/50 RapTime: 0:00:06.016938 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:05.970952 FixedProfit: 1044980\n",
            "Episode: 31/50 RapTime: 0:00:06.014458 FixedProfit: 1033206\n",
            "Episode: 31/50 RapTime: 0:00:06.040093 FixedProfit: 1120835\n",
            "Episode: 31/50 RapTime: 0:00:06.021186 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:05.974943 FixedProfit: 947334\n",
            "Episode: 32/50 RapTime: 0:00:05.976389 FixedProfit: 1144366\n",
            "Episode: 32/50 RapTime: 0:00:06.003759 FixedProfit: 1102774\n",
            "Episode: 32/50 RapTime: 0:00:06.018398 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:06.081148 FixedProfit: 1005201\n",
            "Episode: 33/50 RapTime: 0:00:06.105395 FixedProfit: 1077988\n",
            "Episode: 33/50 RapTime: 0:00:06.094022 FixedProfit: 1184228\n",
            "Episode: 33/50 RapTime: 0:00:06.062720 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:06.031155 FixedProfit: 1089937\n",
            "Episode: 34/50 RapTime: 0:00:06.056914 FixedProfit: 1108669\n",
            "Episode: 34/50 RapTime: 0:00:06.009061 FixedProfit: 986440\n",
            "Episode: 34/50 RapTime: 0:00:05.989184 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:06.008224 FixedProfit: 1096085\n",
            "Episode: 35/50 RapTime: 0:00:05.969641 FixedProfit: 1226039\n",
            "Episode: 35/50 RapTime: 0:00:06.009435 FixedProfit: 1004989\n",
            "Episode: 35/50 RapTime: 0:00:06.063872 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:06.044734 FixedProfit: 1062099\n",
            "Episode: 36/50 RapTime: 0:00:06.000120 FixedProfit: 1207684\n",
            "Episode: 36/50 RapTime: 0:00:06.072183 FixedProfit: 1145951\n",
            "Episode: 36/50 RapTime: 0:00:06.082743 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:06.273463 FixedProfit: 1062976\n",
            "Episode: 37/50 RapTime: 0:00:06.294004 FixedProfit: 1062210\n",
            "Episode: 37/50 RapTime: 0:00:06.281766 FixedProfit: 1338207\n",
            "Episode: 37/50 RapTime: 0:00:06.294189 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:06.302453 FixedProfit: 1222641\n",
            "Episode: 38/50 RapTime: 0:00:06.246620 FixedProfit: 1174202\n",
            "Episode: 38/50 RapTime: 0:00:06.263313 FixedProfit: 1056618\n",
            "Episode: 38/50 RapTime: 0:00:06.315318 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:06.276275 FixedProfit: 1030793\n",
            "Episode: 39/50 RapTime: 0:00:06.299213 FixedProfit: 1224489\n",
            "Episode: 39/50 RapTime: 0:00:06.279152 FixedProfit: 1033939\n",
            "Episode: 39/50 RapTime: 0:00:06.265943 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:06.216292 FixedProfit: 1024018\n",
            "Episode: 40/50 RapTime: 0:00:06.242918 FixedProfit: 1094841\n",
            "Episode: 40/50 RapTime: 0:00:06.283772 FixedProfit: 1029721\n",
            "Episode: 40/50 RapTime: 0:00:06.185122 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:06.041603 FixedProfit: 1131363\n",
            "Episode: 41/50 RapTime: 0:00:06.055879 FixedProfit: 1143148\n",
            "Episode: 41/50 RapTime: 0:00:06.010271 FixedProfit: 1305262\n",
            "Episode: 41/50 RapTime: 0:00:06.062106 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:06.269861 FixedProfit: 1126572\n",
            "Episode: 42/50 RapTime: 0:00:06.220044 FixedProfit: 1066024\n",
            "Episode: 42/50 RapTime: 0:00:06.304565 FixedProfit: 950685\n",
            "Episode: 42/50 RapTime: 0:00:06.271574 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:06.319481 FixedProfit: 1155597\n",
            "Episode: 43/50 RapTime: 0:00:06.307429 FixedProfit: 1022351\n",
            "Episode: 43/50 RapTime: 0:00:06.287736 FixedProfit: 1094823\n",
            "Episode: 43/50 RapTime: 0:00:06.316724 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:06.210318 FixedProfit: 1016361\n",
            "Episode: 44/50 RapTime: 0:00:06.236043 FixedProfit: 1025467\n",
            "Episode: 44/50 RapTime: 0:00:06.225516 FixedProfit: 1031381\n",
            "Episode: 44/50 RapTime: 0:00:06.262329 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:06.213146 FixedProfit: 856844\n",
            "Episode: 45/50 RapTime: 0:00:06.173240 FixedProfit: 1368489\n",
            "Episode: 45/50 RapTime: 0:00:06.196098 FixedProfit: 1113462\n",
            "Episode: 45/50 RapTime: 0:00:06.110188 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:05.982355 FixedProfit: 1199678\n",
            "Episode: 46/50 RapTime: 0:00:06.054445 FixedProfit: 1120923\n",
            "Episode: 46/50 RapTime: 0:00:06.046399 FixedProfit: 1259253\n",
            "Episode: 46/50 RapTime: 0:00:06.052358 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:06.014772 FixedProfit: 1253011\n",
            "Episode: 47/50 RapTime: 0:00:06.059454 FixedProfit: 1236552\n",
            "Episode: 47/50 RapTime: 0:00:06.049246 FixedProfit: 1106895\n",
            "Episode: 47/50 RapTime: 0:00:06.117065 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:06.022596 FixedProfit: 1057050\n",
            "Episode: 48/50 RapTime: 0:00:06.041410 FixedProfit: 1105059\n",
            "Episode: 48/50 RapTime: 0:00:06.046250 FixedProfit: 1089261\n",
            "Episode: 48/50 RapTime: 0:00:06.060787 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:06.044802 FixedProfit: 1126484\n",
            "Episode: 49/50 RapTime: 0:00:06.045181 FixedProfit: 1179134\n",
            "Episode: 49/50 RapTime: 0:00:06.065722 FixedProfit: 1244871\n",
            "Episode: 49/50 RapTime: 0:00:06.030036 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:06.071813 FixedProfit: 1056234\n",
            "Episode: 50/50 RapTime: 0:00:06.697154 FixedProfit: 1343338\n",
            "Episode: 50/50 RapTime: 0:00:06.630865 FixedProfit: 1044218\n",
            "Episode: 50/50 RapTime: 0:00:06.444966 FixedProfit: 1197165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}