{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddqn_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN1PyjwiTCj9r09SdeqsFO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/ddqn_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "42fc16f9-5b96-469c-9ead-50257b4d063c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "mode = 'test'\n",
        "name = 'ddqn'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        model1 = Sequential()\n",
        "        model1.add(Dense(3, input_shape=(3,)))\n",
        "        model1.add(ReLU()) \n",
        "        model1.add(Dense(3))\n",
        "        model1.add(ReLU()) \n",
        "        model1.add(Dense(3))\n",
        "        model1.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        model1.summary()\n",
        "        self.model1 = model1\n",
        "\n",
        "        model2 = Sequential()\n",
        "        model2.add(Dense(3, input_shape=(3,)))\n",
        "        model2.add(ReLU()) \n",
        "        model2.add(Dense(3))\n",
        "        model2.add(ReLU()) \n",
        "        model2.add(Dense(3))\n",
        "        model2.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        model2.summary()\n",
        "        self.model2 = model2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent(Brain, ReplayMemory):\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        Brain.__init__(self)\n",
        "        ReplayMemory.__init__(self, max_size, batch_size)\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state,s_flag=12):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self._predict(state,s_flag)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "        return np.argmax(act_values)\n",
        "\n",
        "    def _predict(self, state, s_flag = 12):\n",
        "        values = None\n",
        "        q1 = self.model1.predict(state)\n",
        "        q2 = self.model2.predict(state)\n",
        "        if s_flag == 12:\n",
        "            values = np.array([q1[0,a] + q2[0,a] for a in range(3)])\n",
        "        elif s_flag == 11:\n",
        "            values = np.array([q1[0,a] + q1[0,a] for a in range(3)])\n",
        "        else:\n",
        "            values = np.array([q2[0,a] + q2[0,a] for a in range(3)])\n",
        "        return values\n",
        "\n",
        "    def replay(self, s_flag):\n",
        "        if self.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "\n",
        "        next_act_values = self._predict(next_states,s_flag)\n",
        "        next_action = np.argmax(next_act_values)\n",
        "\n",
        "        if s_flag == 11:\n",
        "            q = self.model1.predict(states)\n",
        "            next_q = self.model2.predict(next_states)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, actions] = rewards + (1 - done) * self.gamma*np.max(next_q, axis=1)\n",
        "            self.model1.train_on_batch(states, target)\n",
        "        else:\n",
        "            q = self.model2.predict(states)  \n",
        "            next_q = self.model1.predict(next_states)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, actions] = rewards + (1 - done) * self.gamma*np.max(next_q, axis=1)\n",
        "            self.model2.train_on_batch(states, target)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.model1.load_weights(name)\n",
        "        self.model2.load_weights(name2)\n",
        "\n",
        "    def save(self, name, name2):\n",
        "        self.model1.save_weights(name)\n",
        "        self.model2.save_weights(name2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                s_flag = 12\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    rand = np.random.random()\n",
        "                    if rand <= 0.5:\n",
        "                        s_flag = 11\n",
        "                    else:\n",
        "                        s_flag = 22\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                    self.agent.replay(s_flag)                \n",
        "            play_time = datetime.now() - start_time\n",
        "\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        self.agent.save('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYFNVDDQz9X9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45cf36db-3400-45ea-cc23-9e241fd3bab5"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "agent = Agent(max_size, batch_size)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:01:12.324450 FixedProfit: 1526988 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 2/100 RapTime: 0:01:12.849642 FixedProfit: 1425746 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 3/100 RapTime: 0:01:10.422986 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/100 RapTime: 0:01:10.367112 FixedProfit: 1581449 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 5/100 RapTime: 0:01:10.164928 FixedProfit: 1549570 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 6/100 RapTime: 0:01:10.351166 FixedProfit: 1530418 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 7/100 RapTime: 0:01:10.379281 FixedProfit: 1537669 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 8/100 RapTime: 0:01:10.551606 FixedProfit: 1531134 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 9/100 RapTime: 0:01:09.934082 FixedProfit: 1586194 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 10/100 RapTime: 0:01:10.067149 FixedProfit: 1575456 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 11/100 RapTime: 0:01:09.957351 FixedProfit: 1545746 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 12/100 RapTime: 0:01:10.595729 FixedProfit: 1586213 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 13/100 RapTime: 0:01:12.034989 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/100 RapTime: 0:01:11.379332 FixedProfit: 1556883 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 15/100 RapTime: 0:01:11.564755 FixedProfit: 1557259 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 16/100 RapTime: 0:01:11.217610 FixedProfit: 1545770 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 17/100 RapTime: 0:01:10.961181 FixedProfit: 1557674 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 18/100 RapTime: 0:01:11.250135 FixedProfit: 1572685 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 19/100 RapTime: 0:01:12.619710 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/100 RapTime: 0:01:12.631690 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/100 RapTime: 0:01:12.100862 FixedProfit: 1552837 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 22/100 RapTime: 0:01:12.301064 FixedProfit: 1549594 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 23/100 RapTime: 0:01:11.800648 FixedProfit: 1471111 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 24/100 RapTime: 0:01:10.691271 FixedProfit: 1541693 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 25/100 RapTime: 0:01:11.256870 FixedProfit: 1536607 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 26/100 RapTime: 0:01:11.282153 FixedProfit: 1533314 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 27/100 RapTime: 0:01:11.175526 FixedProfit: 1540058 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 28/100 RapTime: 0:01:10.614793 FixedProfit: 1534982 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 29/100 RapTime: 0:01:11.125881 FixedProfit: 1531028 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 30/100 RapTime: 0:01:10.807827 FixedProfit: 1520767 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 31/100 RapTime: 0:01:11.834407 FixedProfit: 1552320 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 32/100 RapTime: 0:01:12.179065 FixedProfit: 1534469 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 33/100 RapTime: 0:01:12.332414 FixedProfit: 1508261 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 34/100 RapTime: 0:01:11.846752 FixedProfit: 1526060 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 35/100 RapTime: 0:01:12.298499 FixedProfit: 1549236 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 36/100 RapTime: 0:01:09.671217 FixedProfit: 1506752 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 37/100 RapTime: 0:01:09.040120 FixedProfit: 1536838 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 38/100 RapTime: 0:01:09.674062 FixedProfit: 1587732 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 39/100 RapTime: 0:01:09.174797 FixedProfit: 1518499 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 40/100 RapTime: 0:01:09.300701 FixedProfit: 1449983 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 41/100 RapTime: 0:01:09.144733 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 42/100 RapTime: 0:01:08.764563 FixedProfit: 1538156 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 43/100 RapTime: 0:01:09.134500 FixedProfit: 1548278 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 44/100 RapTime: 0:01:09.120801 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 45/100 RapTime: 0:01:09.258626 FixedProfit: 1550566 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 46/100 RapTime: 0:01:08.870124 FixedProfit: 1523116 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 47/100 RapTime: 0:01:08.906718 FixedProfit: 1539065 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 48/100 RapTime: 0:01:09.251250 FixedProfit: 1606672 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 49/100 RapTime: 0:01:08.716872 FixedProfit: 1521601 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 50/100 RapTime: 0:01:10.204229 FixedProfit: 1529825 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 51/100 RapTime: 0:01:10.388522 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 52/100 RapTime: 0:01:09.561288 FixedProfit: 1549141 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 53/100 RapTime: 0:01:09.980635 FixedProfit: 1534171 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 54/100 RapTime: 0:01:09.928483 FixedProfit: 1545438 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 55/100 RapTime: 0:01:09.392855 FixedProfit: 1534786 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 56/100 RapTime: 0:01:11.943458 FixedProfit: 1500310 TradeTimes: 8 TradeWin: 5\n",
            "Episode: 57/100 RapTime: 0:01:13.997679 FixedProfit: 1533364 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 58/100 RapTime: 0:01:14.264356 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 59/100 RapTime: 0:01:14.618816 FixedProfit: 1555353 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 60/100 RapTime: 0:01:14.268863 FixedProfit: 1560672 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 61/100 RapTime: 0:01:14.520497 FixedProfit: 1534339 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 62/100 RapTime: 0:01:13.798624 FixedProfit: 1526050 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 63/100 RapTime: 0:01:14.673242 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 64/100 RapTime: 0:01:14.034310 FixedProfit: 1575565 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 65/100 RapTime: 0:01:13.700678 FixedProfit: 1545537 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 66/100 RapTime: 0:01:13.747553 FixedProfit: 1559063 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 67/100 RapTime: 0:01:14.034411 FixedProfit: 1545471 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 68/100 RapTime: 0:01:13.373276 FixedProfit: 1514282 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 69/100 RapTime: 0:01:13.887834 FixedProfit: 1517912 TradeTimes: 8 TradeWin: 7\n",
            "Episode: 70/100 RapTime: 0:01:13.925850 FixedProfit: 1580024 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 71/100 RapTime: 0:01:14.120074 FixedProfit: 1489398 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 72/100 RapTime: 0:01:13.806748 FixedProfit: 1451834 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 73/100 RapTime: 0:01:13.997576 FixedProfit: 1541954 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 74/100 RapTime: 0:01:14.908677 FixedProfit: 1526066 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 75/100 RapTime: 0:01:15.122364 FixedProfit: 1540623 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 76/100 RapTime: 0:01:15.355605 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 77/100 RapTime: 0:01:15.516832 FixedProfit: 1550498 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 78/100 RapTime: 0:01:15.538828 FixedProfit: 1520166 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 79/100 RapTime: 0:01:15.505056 FixedProfit: 1542086 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 80/100 RapTime: 0:01:15.375652 FixedProfit: 1548441 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 81/100 RapTime: 0:01:16.135230 FixedProfit: 1553231 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 82/100 RapTime: 0:01:15.305768 FixedProfit: 1496894 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 83/100 RapTime: 0:01:15.199424 FixedProfit: 1559912 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 84/100 RapTime: 0:01:15.771184 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 85/100 RapTime: 0:01:15.057653 FixedProfit: 1531225 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 86/100 RapTime: 0:01:13.006119 FixedProfit: 1545965 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 87/100 RapTime: 0:01:09.254657 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 88/100 RapTime: 0:01:10.008024 FixedProfit: 1526744 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 89/100 RapTime: 0:01:09.035202 FixedProfit: 1553490 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 90/100 RapTime: 0:01:09.745019 FixedProfit: 1530005 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 91/100 RapTime: 0:01:09.450762 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 92/100 RapTime: 0:01:09.850845 FixedProfit: 1525082 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 93/100 RapTime: 0:01:09.737869 FixedProfit: 1544961 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 94/100 RapTime: 0:01:14.720672 FixedProfit: 1541627 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 95/100 RapTime: 0:01:15.375453 FixedProfit: 1544172 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 96/100 RapTime: 0:01:15.446745 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 97/100 RapTime: 0:01:14.581920 FixedProfit: 1555488 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 98/100 RapTime: 0:01:15.316239 FixedProfit: 1512393 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 99/100 RapTime: 0:01:15.495619 FixedProfit: 1553969 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 100/100 RapTime: 0:01:16.318882 FixedProfit: 1534797 TradeTimes: 5 TradeWin: 4\n"
          ]
        }
      ]
    }
  ]
}