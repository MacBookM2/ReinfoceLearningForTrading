{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a3c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "35c76564-d2e9-49c5-ab43-9fe106274fe0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'a3c_train.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m51Mu4xy9-Nj"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class MasterBrain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model(input_, [actor, critic])\n",
        "        mastermodel.compile(optimizer=Adam(lr=lr))\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "def play_game(env, actor, critic, scaler, episodes_times = 25, batch_size = 32, mode = 'train'):\n",
        "\n",
        "    experiences = []\n",
        "    episode_rewards = []\n",
        "    actor.layering()\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        state = state.flatten()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        while not done:\n",
        "            action = actor.policynetwork(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            next_state = next_state.flatten()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if mode == 'train':\n",
        "                experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                if len(experiences) == batch_size:\n",
        "                    critic.valuenetwork(experiences)\n",
        "                    experiences = []\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            actor.integration()\n",
        "            actor.layering()\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d743226-9575-4f39-bf9b-b6834a6b9615"
      },
      "source": [
        "initial_money=1000000\n",
        "mode = 'train'\n",
        "episodes_times = 50\n",
        "batch_size = 32\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "if mode == 'test':\n",
        "    masterbrain.load(f'{models_folder}/a3c_model.h5')\n",
        "\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "        print(row, file=f)\n",
        "else:\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit'\n",
        "        print(row, file=f)\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    e = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    a = Actor(brain)\n",
        "    c = Critic(model)\n",
        "    s = make_scaler(e)\n",
        "    arr = [e,a,c,s]\n",
        "    envs.append(arr)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: play_game(env[0],env[1],env[2],env[3], episodes_times, batch_size, mode)\n",
        "        datas.append(executor.submit(job))\n",
        "\n",
        "if mode == 'train':\n",
        "    masterbrain.save(f'{models_folder}/a3c_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:00:08.084508 FixedProfit: 1101512\n",
            "Episode: 1/50 RapTime: 0:00:08.088872 FixedProfit: 1234984\n",
            "Episode: 1/50 RapTime: 0:00:08.115249 FixedProfit: 1090468\n",
            "Episode: 1/50 RapTime: 0:00:08.193785 FixedProfit: 1099184\n",
            "Episode: 2/50 RapTime: 0:00:07.730413 FixedProfit: 1237192\n",
            "Episode: 2/50 RapTime: 0:00:07.743759 FixedProfit: 1071207\n",
            "Episode: 2/50 RapTime: 0:00:07.785845 FixedProfit: 1052417\n",
            "Episode: 2/50 RapTime: 0:00:07.765116 FixedProfit: 1027880\n",
            "Episode: 3/50 RapTime: 0:00:07.741085 FixedProfit: 1096991\n",
            "Episode: 3/50 RapTime: 0:00:07.744628 FixedProfit: 1195983\n",
            "Episode: 3/50 RapTime: 0:00:07.715663 FixedProfit: 1096999\n",
            "Episode: 3/50 RapTime: 0:00:07.694652 FixedProfit: 986949\n",
            "Episode: 4/50 RapTime: 0:00:07.867383 FixedProfit: 1078965\n",
            "Episode: 4/50 RapTime: 0:00:07.875128 FixedProfit: 953647\n",
            "Episode: 4/50 RapTime: 0:00:07.889753 FixedProfit: 1156785\n",
            "Episode: 4/50 RapTime: 0:00:07.969694 FixedProfit: 1059767\n",
            "Episode: 5/50 RapTime: 0:00:07.982766 FixedProfit: 1266256\n",
            "Episode: 5/50 RapTime: 0:00:08.006551 FixedProfit: 1274735\n",
            "Episode: 5/50 RapTime: 0:00:07.961826 FixedProfit: 1209265\n",
            "Episode: 5/50 RapTime: 0:00:08.032386 FixedProfit: 944621\n",
            "Episode: 6/50 RapTime: 0:00:07.785535 FixedProfit: 963197\n",
            "Episode: 6/50 RapTime: 0:00:07.800331 FixedProfit: 1287650\n",
            "Episode: 6/50 RapTime: 0:00:07.816737 FixedProfit: 1169120\n",
            "Episode: 6/50 RapTime: 0:00:07.796727 FixedProfit: 1183314\n",
            "Episode: 7/50 RapTime: 0:00:07.704557 FixedProfit: 1096006\n",
            "Episode: 7/50 RapTime: 0:00:07.809316 FixedProfit: 1119698\n",
            "Episode: 7/50 RapTime: 0:00:07.687705 FixedProfit: 1047984\n",
            "Episode: 7/50 RapTime: 0:00:07.834955 FixedProfit: 1221842\n",
            "Episode: 8/50 RapTime: 0:00:07.811676 FixedProfit: 1166143\n",
            "Episode: 8/50 RapTime: 0:00:07.753117 FixedProfit: 1126386\n",
            "Episode: 8/50 RapTime: 0:00:07.811558 FixedProfit: 977714\n",
            "Episode: 8/50 RapTime: 0:00:07.821713 FixedProfit: 1070863\n",
            "Episode: 9/50 RapTime: 0:00:07.814846 FixedProfit: 1038840\n",
            "Episode: 9/50 RapTime: 0:00:07.873736 FixedProfit: 1023818\n",
            "Episode: 9/50 RapTime: 0:00:07.877482 FixedProfit: 1094851\n",
            "Episode: 9/50 RapTime: 0:00:07.915883 FixedProfit: 1377175\n",
            "Episode: 10/50 RapTime: 0:00:07.635519 FixedProfit: 1133709\n",
            "Episode: 10/50 RapTime: 0:00:07.728689 FixedProfit: 1290441\n",
            "Episode: 10/50 RapTime: 0:00:07.678881 FixedProfit: 1114054\n",
            "Episode: 10/50 RapTime: 0:00:07.656537 FixedProfit: 986571\n",
            "Episode: 11/50 RapTime: 0:00:07.765597 FixedProfit: 998246\n",
            "Episode: 11/50 RapTime: 0:00:07.795278 FixedProfit: 1134839\n",
            "Episode: 11/50 RapTime: 0:00:07.729603 FixedProfit: 1067734\n",
            "Episode: 11/50 RapTime: 0:00:07.773785 FixedProfit: 1097243\n",
            "Episode: 12/50 RapTime: 0:00:07.709992 FixedProfit: 1095375\n",
            "Episode: 12/50 RapTime: 0:00:07.638823 FixedProfit: 1067847\n",
            "Episode: 12/50 RapTime: 0:00:07.674636 FixedProfit: 1197165\n",
            "Episode: 12/50 RapTime: 0:00:07.627392 FixedProfit: 1108940\n",
            "Episode: 13/50 RapTime: 0:00:07.519196 FixedProfit: 1055571\n",
            "Episode: 13/50 RapTime: 0:00:07.516844 FixedProfit: 1197165\n",
            "Episode: 13/50 RapTime: 0:00:07.525565 FixedProfit: 1208128\n",
            "Episode: 13/50 RapTime: 0:00:07.573531 FixedProfit: 1061618\n",
            "Episode: 14/50 RapTime: 0:00:07.556630 FixedProfit: 1404535\n",
            "Episode: 14/50 RapTime: 0:00:07.550397 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:07.650594 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:07.584479 FixedProfit: 915722\n",
            "Episode: 15/50 RapTime: 0:00:07.452728 FixedProfit: 1118408\n",
            "Episode: 15/50 RapTime: 0:00:07.446795 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:07.521769 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:07.513596 FixedProfit: 1346554\n",
            "Episode: 16/50 RapTime: 0:00:07.544117 FixedProfit: 948407\n",
            "Episode: 16/50 RapTime: 0:00:07.521376 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:07.556867 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:07.583311 FixedProfit: 988008\n",
            "Episode: 17/50 RapTime: 0:00:07.710099 FixedProfit: 1203227\n",
            "Episode: 17/50 RapTime: 0:00:07.701115 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:07.734185 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:07.694570 FixedProfit: 1073174\n",
            "Episode: 18/50 RapTime: 0:00:07.634554 FixedProfit: 1132538\n",
            "Episode: 18/50 RapTime: 0:00:07.570906 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:07.576188 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:07.558696 FixedProfit: 1037154\n",
            "Episode: 19/50 RapTime: 0:00:07.560603 FixedProfit: 1127197\n",
            "Episode: 19/50 RapTime: 0:00:07.586624 FixedProfit: 1208128\n",
            "Episode: 19/50 RapTime: 0:00:07.572600 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:07.587635 FixedProfit: 1188279\n",
            "Episode: 20/50 RapTime: 0:00:07.461568 FixedProfit: 1043134\n",
            "Episode: 20/50 RapTime: 0:00:07.491943 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:07.525630 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:07.474578 FixedProfit: 959094\n",
            "Episode: 21/50 RapTime: 0:00:07.652153 FixedProfit: 1197744\n",
            "Episode: 21/50 RapTime: 0:00:07.715322 FixedProfit: 1208128\n",
            "Episode: 21/50 RapTime: 0:00:07.680819 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:07.622162 FixedProfit: 1113656\n",
            "Episode: 22/50 RapTime: 0:00:07.621161 FixedProfit: 1401195\n",
            "Episode: 22/50 RapTime: 0:00:07.582688 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:07.604903 FixedProfit: 1077076\n",
            "Episode: 22/50 RapTime: 0:00:07.679056 FixedProfit: 1092498\n",
            "Episode: 23/50 RapTime: 0:00:07.573006 FixedProfit: 1018379\n",
            "Episode: 23/50 RapTime: 0:00:07.509363 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:07.583357 FixedProfit: 1073127\n",
            "Episode: 23/50 RapTime: 0:00:07.527657 FixedProfit: 1119247\n",
            "Episode: 24/50 RapTime: 0:00:08.014516 FixedProfit: 1208128\n",
            "Episode: 24/50 RapTime: 0:00:08.062260 FixedProfit: 1105111\n",
            "Episode: 24/50 RapTime: 0:00:08.078997 FixedProfit: 1032877\n",
            "Episode: 24/50 RapTime: 0:00:08.065075 FixedProfit: 1195955\n",
            "Episode: 25/50 RapTime: 0:00:07.627359 FixedProfit: 1208128\n",
            "Episode: 25/50 RapTime: 0:00:07.633816 FixedProfit: 1213991\n",
            "Episode: 25/50 RapTime: 0:00:07.643903 FixedProfit: 1191954\n",
            "Episode: 25/50 RapTime: 0:00:07.658096 FixedProfit: 1146959\n",
            "Episode: 26/50 RapTime: 0:00:07.767798 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:07.740802 FixedProfit: 1357811\n",
            "Episode: 26/50 RapTime: 0:00:07.753131 FixedProfit: 1208128\n",
            "Episode: 26/50 RapTime: 0:00:07.780862 FixedProfit: 1151363\n",
            "Episode: 27/50 RapTime: 0:00:07.698384 FixedProfit: 1081324\n",
            "Episode: 27/50 RapTime: 0:00:07.717700 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:07.670219 FixedProfit: 1192980\n",
            "Episode: 27/50 RapTime: 0:00:07.723443 FixedProfit: 1157429\n",
            "Episode: 28/50 RapTime: 0:00:07.671466 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:07.728971 FixedProfit: 1257204\n",
            "Episode: 28/50 RapTime: 0:00:07.692700 FixedProfit: 1175255\n",
            "Episode: 28/50 RapTime: 0:00:07.712203 FixedProfit: 1143932\n",
            "Episode: 29/50 RapTime: 0:00:07.704360 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:07.708548 FixedProfit: 1018395\n",
            "Episode: 29/50 RapTime: 0:00:07.742928 FixedProfit: 1192980\n",
            "Episode: 29/50 RapTime: 0:00:07.712650 FixedProfit: 1124617\n",
            "Episode: 30/50 RapTime: 0:00:07.635382 FixedProfit: 1085196\n",
            "Episode: 30/50 RapTime: 0:00:07.674875 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:07.565384 FixedProfit: 1183321\n",
            "Episode: 30/50 RapTime: 0:00:07.667290 FixedProfit: 1138885\n",
            "Episode: 31/50 RapTime: 0:00:07.593960 FixedProfit: 1220289\n",
            "Episode: 31/50 RapTime: 0:00:07.648533 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:07.536233 FixedProfit: 1140322\n",
            "Episode: 31/50 RapTime: 0:00:07.579032 FixedProfit: 1183747\n",
            "Episode: 32/50 RapTime: 0:00:07.476307 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:07.519991 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:07.489961 FixedProfit: 1212780\n",
            "Episode: 32/50 RapTime: 0:00:07.497092 FixedProfit: 1000000\n",
            "Episode: 33/50 RapTime: 0:00:07.398653 FixedProfit: 1189423\n",
            "Episode: 33/50 RapTime: 0:00:07.504173 FixedProfit: 1199612\n",
            "Episode: 33/50 RapTime: 0:00:07.465109 FixedProfit: 1155645\n",
            "Episode: 33/50 RapTime: 0:00:07.438148 FixedProfit: 1000000\n",
            "Episode: 34/50 RapTime: 0:00:07.546023 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:07.542698 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:07.511364 FixedProfit: 1208352\n",
            "Episode: 34/50 RapTime: 0:00:07.439857 FixedProfit: 1000000\n",
            "Episode: 35/50 RapTime: 0:00:07.462268 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:07.385280 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:07.431209 FixedProfit: 1185935\n",
            "Episode: 35/50 RapTime: 0:00:07.415387 FixedProfit: 1000000\n",
            "Episode: 36/50 RapTime: 0:00:07.490424 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:07.494205 FixedProfit: 1191271\n",
            "Episode: 36/50 RapTime: 0:00:07.498142 FixedProfit: 1192980\n",
            "Episode: 36/50 RapTime: 0:00:07.495499 FixedProfit: 1000000\n",
            "Episode: 37/50 RapTime: 0:00:07.426140 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:07.414824 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:07.367525 FixedProfit: 1190689\n",
            "Episode: 37/50 RapTime: 0:00:07.473444 FixedProfit: 1000000\n",
            "Episode: 38/50 RapTime: 0:00:07.602909 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:07.537923 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:07.559834 FixedProfit: 1195955\n",
            "Episode: 38/50 RapTime: 0:00:07.583280 FixedProfit: 1000000\n",
            "Episode: 39/50 RapTime: 0:00:07.619794 FixedProfit: 1197165Episode: 39/50 RapTime: 0:00:07.683294 FixedProfit: 1197165\n",
            "\n",
            "Episode: 39/50 RapTime: 0:00:07.580267 FixedProfit: 1198139\n",
            "Episode: 39/50 RapTime: 0:00:07.666130 FixedProfit: 1000000\n",
            "Episode: 40/50 RapTime: 0:00:07.465404 FixedProfit: 1211557\n",
            "Episode: 40/50 RapTime: 0:00:07.508550 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:07.451721 FixedProfit: 1152015\n",
            "Episode: 40/50 RapTime: 0:00:07.483556 FixedProfit: 1000000\n",
            "Episode: 41/50 RapTime: 0:00:07.481527 FixedProfit: 1173059\n",
            "Episode: 41/50 RapTime: 0:00:07.557924 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:07.541118 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:07.489576 FixedProfit: 1000000\n",
            "Episode: 42/50 RapTime: 0:00:07.479567 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:07.512095 FixedProfit: 1128428\n",
            "Episode: 42/50 RapTime: 0:00:07.494819 FixedProfit: 1203544\n",
            "Episode: 42/50 RapTime: 0:00:07.478067 FixedProfit: 1000000\n",
            "Episode: 43/50 RapTime: 0:00:07.484646 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:07.480554 FixedProfit: 1196834\n",
            "Episode: 43/50 RapTime: 0:00:07.495405 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:07.510429 FixedProfit: 1000000\n",
            "Episode: 44/50 RapTime: 0:00:07.537933 FixedProfit: 1196192\n",
            "Episode: 44/50 RapTime: 0:00:07.561918 FixedProfit: 1208128\n",
            "Episode: 44/50 RapTime: 0:00:07.538007 FixedProfit: 1212474\n",
            "Episode: 44/50 RapTime: 0:00:07.608075 FixedProfit: 1000000\n",
            "Episode: 45/50 RapTime: 0:00:07.786865 FixedProfit: 1196192\n",
            "Episode: 45/50 RapTime: 0:00:07.805397 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:07.886183 FixedProfit: 1212780\n",
            "Episode: 45/50 RapTime: 0:00:07.874609 FixedProfit: 1000000\n",
            "Episode: 46/50 RapTime: 0:00:07.796996 FixedProfit: 1194143\n",
            "Episode: 46/50 RapTime: 0:00:07.769965 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:07.707040 FixedProfit: 1183321\n",
            "Episode: 46/50 RapTime: 0:00:07.775256 FixedProfit: 1000000\n",
            "Episode: 47/50 RapTime: 0:00:07.624756 FixedProfit: 1191954\n",
            "Episode: 47/50 RapTime: 0:00:07.603552 FixedProfit: 1190689\n",
            "Episode: 47/50 RapTime: 0:00:07.623761 FixedProfit: 1198139\n",
            "Episode: 47/50 RapTime: 0:00:07.606167 FixedProfit: 1000000\n",
            "Episode: 48/50 RapTime: 0:00:07.586399 FixedProfit: 1196834\n",
            "Episode: 48/50 RapTime: 0:00:07.666238 FixedProfit: 1170818\n",
            "Episode: 48/50 RapTime: 0:00:07.692697 FixedProfit: 1194143\n",
            "Episode: 48/50 RapTime: 0:00:07.632251 FixedProfit: 1000000\n",
            "Episode: 49/50 RapTime: 0:00:07.580112 FixedProfit: 1196192\n",
            "Episode: 49/50 RapTime: 0:00:07.595303 FixedProfit: 1197032\n",
            "Episode: 49/50 RapTime: 0:00:07.624513 FixedProfit: 1195955\n",
            "Episode: 49/50 RapTime: 0:00:07.559644 FixedProfit: 1000000\n",
            "Episode: 50/50 RapTime: 0:00:07.473332 FixedProfit: 1162485\n",
            "Episode: 50/50 RapTime: 0:00:07.452512 FixedProfit: 1191954\n",
            "Episode: 50/50 RapTime: 0:00:07.472721 FixedProfit: 1194143\n",
            "Episode: 50/50 RapTime: 0:00:07.213819 FixedProfit: 1000000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}