{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ape-X_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNZK+ciV+WSP2Zk5dgBc+Ow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/Ape_X_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NIXg6mTzk0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c739ce8-e883-4fda-f53f-abbfa0ad9f93"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import Dense, ReLU, Input, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from statistics import mean\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from queue import Queue\n",
        "import time\n",
        "\n",
        "\n",
        "mode = 'train'\n",
        "name = 'apex'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNPHarYI9zGY"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        learning_rate = 0.00001\n",
        "        neurons_per_layer = 24\n",
        "\n",
        "        input = Input(shape=(3,))\n",
        "        common = Dense(neurons_per_layer*2, activation='relu')(input)\n",
        "        common = Dense(neurons_per_layer*4, activation='relu')(common)\n",
        "\n",
        "        common = Dense(4, activation='linear')(common)\n",
        "        output = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - 0.0*K.mean(a[:, 1:], keepdims=True),output_shape=(3,))(common)\n",
        "\n",
        "        model = keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "        optimizer = Adam(learning_rate=learning_rate, clipvalue=40)\n",
        "        model.compile(loss=Huber(), optimizer=optimizer)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvtDknjO2juu"
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, model1, model2):\n",
        "\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        self.gamma  = 0.99\n",
        "\n",
        "    def learn(self, q):\n",
        "        while True:\n",
        "            val = q.get()\n",
        "            if (val == 'done_prosess'):\n",
        "                break\n",
        "            s_flag = 11 if np.random.random() <= 0.5 else 22\n",
        "\n",
        "            states, next_states, actions, rewards, done = val['state'], val['next_state'], val['act'], val['reward'], val['done']\n",
        "\n",
        "            next_act_values = self.model1.predict(next_states,s_flag)\n",
        "            next_action =np.argmax(next_act_values)\n",
        "\n",
        "            if s_flag == 11:\n",
        "                q = self.model1.predict(states)  \n",
        "                next_q = self.model2.predict(next_states)\n",
        "                target = np.copy(q)\n",
        "\n",
        "                target[:, actions] = rewards + (1 - done) * self.gamma * np.max(next_q, axis=1)\n",
        "                self.model1.train_on_batch(states, target)\n",
        "            else:\n",
        "                q = self.model2.predict(states)  \n",
        "                next_q = self.model1.predict(next_states)\n",
        "                target = np.copy(q)\n",
        "\n",
        "                target[:, actions] = rewards + (1 - done) * self.gamma * np.max(next_q, axis=1)\n",
        "                self.model2.train_on_batch(states, target)\n",
        "            q.task_done()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZLHqi3CpnxI"
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.tderrors_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def findall(self):\n",
        "        return self.states_memory,self.next_states_memory,self.acts_memory,self.rewards_memory,self.done_memory,self.tderrors_memory\n",
        "\n",
        "    #def update_memory_tderror(self, val):\n",
        "    #    self.tderrors_memory = val\n",
        "\n",
        "    def replace_tderror(self, val):\n",
        "        np.put(self.tderrors_memory, range(val.shape[0]), val)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, model1, model2, memory, epsilon, batch_size=32):\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        self.memory = memory\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon_min = 0.01\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state, s_flag=12):\n",
        "        ran_num = np.random.rand()\n",
        "        if (ran_num <= self.epsilon) and (ran_num > self.epsilon_min):\n",
        "            return np.random.choice(3)\n",
        "        act_values = self._predict(state,s_flag)\n",
        "        return np.argmax(act_values)\n",
        "\n",
        "    def _predict(self, state, s_flag = 12):\n",
        "        values = None\n",
        "        q1 = self.model1.predict(state)\n",
        "        q2 = self.model2.predict(state)\n",
        "        if s_flag == 12:\n",
        "            values = q1 + q2\n",
        "        elif s_flag == 11:\n",
        "            values = q1 + q1\n",
        "        else:\n",
        "            values = q2 + q2\n",
        "        return values\n",
        "\n",
        "    def replay(self):\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        return m_batch\n",
        "\n",
        "    def pioritized_experience_replay(self):\n",
        "\n",
        "        sum_ab_tderror = self._absolute_tderror()\n",
        "        td_list = np.random.uniform(0, sum_ab_tderror, self.batch_size)\n",
        "        td_list = np.sort(td_list)\n",
        "\n",
        "        num_np = np.array([], dtype=np.int16)\n",
        "        i, sum_ab_tderror_tmp = 0, 0\n",
        "        states, next_states, actions, rewards, done, tderror = self.memory.findall()\n",
        "        for item in td_list:\n",
        "            while sum_ab_tderror_tmp < item:\n",
        "                sum_ab_tderror_tmp += abs(tderror[i]) + 0.0001\n",
        "                i += 1\n",
        "            num_np = np.append(num_np, i)\n",
        "\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [states[num_np],next_states[num_np],\n",
        "                 actions[num_np],rewards[num_np],\n",
        "                 done[num_np]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def tderror(self):\n",
        "        states, next_states, acts, rewards, done, tderrors = self.memory.findall()\n",
        "        \n",
        "        n2_rewards = rewards[2:-1]\n",
        "        next_rewards = rewards[1:-2]\n",
        "        n3_action = acts[3:]\n",
        "        n3_next_states = next_states[3:]\n",
        "\n",
        "        states = states[:-3]\n",
        "        next_states = next_states[:-3]\n",
        "        acts = acts[:-3]\n",
        "        rewards = rewards[:-3]\n",
        "        done = done[:-3]\n",
        "\n",
        "        #self.memory.cntr = (self.memory.cntr - 3) if (self.memory.cntr - 3) > 0 else (self.memory.cntr + max_size - 4)\n",
        "        #self.memory.size = (self.memory.size - 3) if (self.memory.size - 3) > 0 else (self.memory.size + max_size - 4)\n",
        "\n",
        "        #self.memory.cntr = self.memory.cntr - 3\n",
        "        #self.memory.size = self.memory.size - 3\n",
        "\n",
        "        next_action = np.argmax(self._predict(n3_next_states), axis = 1)\n",
        "        next_action = next_action.reshape(-1)\n",
        "\n",
        "        acts = acts.reshape(-1)\n",
        "\n",
        "        acts_one = np.eye(3)[acts]\n",
        "        next_action_one = np.eye(3)[next_action]\n",
        "        n3_q = np.max(self._predict(n3_next_states) * next_action_one, axis = 1)\n",
        "\n",
        "        target = rewards + self.gamma * next_rewards + (self.gamma ** 2) * n2_rewards + (self.gamma ** 3) * n3_q\n",
        "        tderror = target - np.max(self._predict(states) * acts_one, axis = 1)\n",
        "        #self.memory.update_memory_tderror(tderror)\n",
        "        self.memory.replace_tderror(tderror)\n",
        "\n",
        "    def _absolute_tderror(self):\n",
        "        absolute_tderror = 0\n",
        "        tderror = self.memory.tderrors_memory\n",
        "        for i in range(0, (len(tderror)-1)):\n",
        "            absolute_tderror += abs(tderror[i]) + 0.0001\n",
        "        return absolute_tderror\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.model1.load_weights(name)\n",
        "        self.model2.load_weights(name2)\n",
        "\n",
        "    def save(self, name, name2):\n",
        "        self.model1.save_weights(name)\n",
        "        self.model2.save_weights(name2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, i, term, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "        self.agent_num      = i\n",
        "        self.term           = term\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self, q):\n",
        "\n",
        "        #total_reward = [0]\n",
        "        total_reward = [1000000]\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                s_flag = 12\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                    if (self.agent.memory.size > self.agent.memory.batch_size) and (self.agent.memory.cntr % 100 == 0):\n",
        "                        self.agent.tderror()\n",
        "                        m_batch = None\n",
        "                        if mean(total_reward) < 1050000:\n",
        "                            m_batch = self.agent.replay()\n",
        "                        else:\n",
        "                            m_batch = self.agent.pioritized_experience_replay()\n",
        "                        q.put(m_batch) \n",
        "                        self.agent.tderror()\n",
        "              \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                total_reward.append(info['cur_revenue'])\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "            time.sleep(1)\n",
        "            self.term[self.agent_num] = True\n",
        "            if self.term.all():\n",
        "                q.put(\"done_prosess\")\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        _ = env.reset()\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, str(self.agent_num)), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        if self.agent_num == 0:\n",
        "            self.agent.load('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        if self.agent_num == 0:\n",
        "            self.agent.save('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYFNVDDQz9X9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61772aef-1e02-469f-81da-b0b7243e7201"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "max_size = 5000\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "\n",
        "brain1 = Brain()\n",
        "model1 = brain1.model\n",
        "brain2 = Brain()\n",
        "model2 = brain2.model\n",
        "\n",
        "learner = Learner(model1, model2)\n",
        "memory = Memory(max_size, batch_size)\n",
        "\n",
        "thread_num = 4\n",
        "epsilon_list = np.array([])\n",
        "epsilon = 0.4\n",
        "alfa = 7\n",
        "for i in range(thread_num):\n",
        "    num = epsilon ** (1 + i * alfa / (thread_num - 1))\n",
        "    epsilon_list = np.append(epsilon_list, num)\n",
        "\n",
        "workers = []\n",
        "term = np.full((thread_num,), False)\n",
        "for i in range(thread_num):\n",
        "    epsilon = epsilon_list[i]\n",
        "    agent = Agent(model1, model2, memory, epsilon, batch_size)\n",
        "    main  = Main(env, agent, mdl_dir, name, i, term, episodes_times, mode)\n",
        "    workers.append(main)\n",
        "\n",
        "data = []\n",
        "q = Queue()\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    for i, w in enumerate(workers):\n",
        "        print(f'start worker_{i}')\n",
        "        def job1(): return w.play_game(q,)\n",
        "        data.append(executor.submit(job1))\n",
        "\n",
        "    print('start learner')\n",
        "    def job2(): return learner.learn(q,)\n",
        "    data.append(executor.submit(job2))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 48)                192       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 96)                4704      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 388       \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 5,284\n",
            "Trainable params: 5,284\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 48)                192       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 96)                4704      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 388       \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 5,284\n",
            "Trainable params: 5,284\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "start worker_0\n",
            "start worker_1\n",
            "start worker_2\n",
            "start worker_3\n",
            "start learner\n",
            "Episode: 1/100 RapTime: 0:00:55.817103 FixedProfit: 1125893\n",
            "Episode: 1/100 RapTime: 0:01:50.856868 FixedProfit: 1122586\n",
            "Episode: 1/100 RapTime: 0:02:45.345851 FixedProfit: 1121903\n",
            "Episode: 2/100 RapTime: 0:00:55.028176 FixedProfit: 1067539\n",
            "Episode: 3/100 RapTime: 0:00:56.388377 FixedProfit: 1206792\n",
            "Episode: 4/100 RapTime: 0:00:59.509724 FixedProfit: 1242719\n",
            "Episode: 1/100 RapTime: 0:06:32.304146 FixedProfit: 1212263\n",
            "Episode: 2/100 RapTime: 0:06:31.243977 FixedProfit: 1154646\n",
            "Episode: 3/100 RapTime: 0:00:55.702302 FixedProfit: 1264767\n",
            "Episode: 5/100 RapTime: 0:03:39.695500 FixedProfit: 1198864\n",
            "Episode: 2/100 RapTime: 0:08:15.868802 FixedProfit: 1171024\n",
            "Episode: 6/100 RapTime: 0:01:41.000649 FixedProfit: 1154353\n",
            "Episode: 7/100 RapTime: 0:00:50.889903 FixedProfit: 1184767\n",
            "Episode: 3/100 RapTime: 0:02:29.032577 FixedProfit: 1238437\n",
            "Episode: 4/100 RapTime: 0:00:50.526776 FixedProfit: 1373366\n",
            "Episode: 8/100 RapTime: 0:02:26.105017 FixedProfit: 1132561\n",
            "Episode: 4/100 RapTime: 0:06:43.397078 FixedProfit: 1132852\n",
            "Episode: 9/100 RapTime: 0:01:42.371063 FixedProfit: 1296223\n",
            "Episode: 10/100 RapTime: 0:00:51.262039 FixedProfit: 1196077\n",
            "Episode: 11/100 RapTime: 0:00:50.052050 FixedProfit: 1181969\n",
            "Episode: 12/100 RapTime: 0:00:49.945405 FixedProfit: 1189402\n",
            "Episode: 13/100 RapTime: 0:00:49.449503 FixedProfit: 1146422\n",
            "Episode: 14/100 RapTime: 0:00:50.739311 FixedProfit: 1228791\n",
            "Episode: 5/100 RapTime: 0:05:51.929051 FixedProfit: 1212049\n",
            "Episode: 15/100 RapTime: 0:01:40.139395 FixedProfit: 1302099\n",
            "Episode: 6/100 RapTime: 0:01:38.801795 FixedProfit: 1174544\n",
            "Episode: 7/100 RapTime: 0:00:44.992290 FixedProfit: 1182743\n",
            "Episode: 16/100 RapTime: 0:02:18.151313 FixedProfit: 1127880\n",
            "Episode: 17/100 RapTime: 0:00:42.120976 FixedProfit: 1188356\n",
            "Episode: 18/100 RapTime: 0:00:45.079221 FixedProfit: 1198527\n",
            "Episode: 19/100 RapTime: 0:00:46.047961 FixedProfit: 1240831\n",
            "Episode: 20/100 RapTime: 0:00:44.151541 FixedProfit: 1043650\n",
            "Episode: 21/100 RapTime: 0:00:44.346591 FixedProfit: 1212486\n",
            "Episode: 22/100 RapTime: 0:00:45.929070 FixedProfit: 1070861\n",
            "Episode: 23/100 RapTime: 0:00:43.481038 FixedProfit: 1210468\n",
            "Episode: 24/100 RapTime: 0:00:44.276019 FixedProfit: 1014909\n",
            "Episode: 25/100 RapTime: 0:00:47.424775 FixedProfit: 1172135\n",
            "Episode: 26/100 RapTime: 0:00:45.672151 FixedProfit: 1205207\n",
            "Episode: 27/100 RapTime: 0:00:45.226092 FixedProfit: 1310008\n",
            "Episode: 28/100 RapTime: 0:00:46.199072 FixedProfit: 1360950\n",
            "Episode: 29/100 RapTime: 0:00:46.382229 FixedProfit: 1028327\n",
            "Episode: 30/100 RapTime: 0:00:44.431998 FixedProfit: 1247197\n",
            "Episode: 31/100 RapTime: 0:00:43.272157 FixedProfit: 1284820\n",
            "Episode: 32/100 RapTime: 0:00:44.317382 FixedProfit: 1222856\n",
            "Episode: 33/100 RapTime: 0:00:46.529397 FixedProfit: 1157546\n",
            "Episode: 34/100 RapTime: 0:00:43.512862 FixedProfit: 1325797\n",
            "Episode: 35/100 RapTime: 0:00:45.171976 FixedProfit: 1203407\n",
            "Episode: 36/100 RapTime: 0:00:46.272095 FixedProfit: 1248980\n",
            "Episode: 37/100 RapTime: 0:00:46.307494 FixedProfit: 1230257\n",
            "Episode: 38/100 RapTime: 0:00:44.643961 FixedProfit: 1295628\n",
            "Episode: 39/100 RapTime: 0:00:45.331261 FixedProfit: 1475962\n",
            "Episode: 40/100 RapTime: 0:00:48.090236 FixedProfit: 1210248\n",
            "Episode: 41/100 RapTime: 0:00:43.668234 FixedProfit: 1236431\n",
            "Episode: 42/100 RapTime: 0:00:42.131958 FixedProfit: 1205430\n",
            "Episode: 43/100 RapTime: 0:00:44.917685 FixedProfit: 1327013\n",
            "Episode: 44/100 RapTime: 0:00:47.518394 FixedProfit: 1127316\n",
            "Episode: 45/100 RapTime: 0:00:43.701655 FixedProfit: 1285797\n",
            "Episode: 46/100 RapTime: 0:00:44.191081 FixedProfit: 1166774\n",
            "Episode: 47/100 RapTime: 0:00:43.981030 FixedProfit: 1190695\n",
            "Episode: 48/100 RapTime: 0:00:46.741823 FixedProfit: 1095261\n",
            "Episode: 49/100 RapTime: 0:00:43.994504 FixedProfit: 1293508\n",
            "Episode: 50/100 RapTime: 0:00:46.054863 FixedProfit: 1316568\n",
            "Episode: 51/100 RapTime: 0:00:46.186073 FixedProfit: 1219290\n",
            "Episode: 52/100 RapTime: 0:00:44.151806 FixedProfit: 1233311\n",
            "Episode: 53/100 RapTime: 0:00:43.001427 FixedProfit: 1155999\n",
            "Episode: 54/100 RapTime: 0:00:45.274536 FixedProfit: 1220824\n",
            "Episode: 55/100 RapTime: 0:00:47.976031 FixedProfit: 1079141\n",
            "Episode: 56/100 RapTime: 0:00:42.573500 FixedProfit: 1133865\n",
            "Episode: 57/100 RapTime: 0:00:46.453723 FixedProfit: 1224154\n",
            "Episode: 58/100 RapTime: 0:00:45.988300 FixedProfit: 1146368\n",
            "Episode: 59/100 RapTime: 0:00:46.589233 FixedProfit: 1284034\n",
            "Episode: 60/100 RapTime: 0:00:43.940811 FixedProfit: 1128691\n",
            "Episode: 61/100 RapTime: 0:00:45.594543 FixedProfit: 1095395\n",
            "Episode: 62/100 RapTime: 0:00:42.118241 FixedProfit: 1039859\n",
            "Episode: 63/100 RapTime: 0:00:46.164741 FixedProfit: 1144906\n",
            "Episode: 64/100 RapTime: 0:00:44.110595 FixedProfit: 1134155\n",
            "Episode: 65/100 RapTime: 0:00:43.983453 FixedProfit: 1374343\n",
            "Episode: 66/100 RapTime: 0:00:46.403608 FixedProfit: 1103392\n",
            "Episode: 67/100 RapTime: 0:00:45.243183 FixedProfit: 1094925\n",
            "Episode: 68/100 RapTime: 0:00:45.482412 FixedProfit: 1094755\n",
            "Episode: 69/100 RapTime: 0:00:44.443546 FixedProfit: 1190551\n",
            "Episode: 70/100 RapTime: 0:00:44.562345 FixedProfit: 1303083\n",
            "Episode: 71/100 RapTime: 0:00:43.251465 FixedProfit: 1259764\n",
            "Episode: 72/100 RapTime: 0:00:43.396027 FixedProfit: 1030130\n",
            "Episode: 73/100 RapTime: 0:00:43.954520 FixedProfit: 1153011\n",
            "Episode: 74/100 RapTime: 0:00:46.901007 FixedProfit: 1084457\n",
            "Episode: 75/100 RapTime: 0:00:45.245433 FixedProfit: 1165991\n",
            "Episode: 76/100 RapTime: 0:00:46.712824 FixedProfit: 1259910\n",
            "Episode: 77/100 RapTime: 0:00:46.519063 FixedProfit: 1084991\n",
            "Episode: 78/100 RapTime: 0:00:47.253214 FixedProfit: 1295705\n",
            "Episode: 79/100 RapTime: 0:00:46.480305 FixedProfit: 1182894\n",
            "Episode: 80/100 RapTime: 0:00:46.222921 FixedProfit: 1140871\n",
            "Episode: 81/100 RapTime: 0:00:46.947532 FixedProfit: 1136899\n",
            "Episode: 82/100 RapTime: 0:00:46.427410 FixedProfit: 1059900\n",
            "Episode: 83/100 RapTime: 0:00:43.847328 FixedProfit: 1136817\n",
            "Episode: 84/100 RapTime: 0:00:47.587383 FixedProfit: 1218245\n",
            "Episode: 85/100 RapTime: 0:00:47.306171 FixedProfit: 1028384\n",
            "Episode: 86/100 RapTime: 0:00:45.984400 FixedProfit: 1208293\n",
            "Episode: 87/100 RapTime: 0:00:48.367207 FixedProfit: 1195779\n",
            "Episode: 88/100 RapTime: 0:00:47.990136 FixedProfit: 1315977\n",
            "Episode: 89/100 RapTime: 0:00:45.458651 FixedProfit: 1088154\n",
            "Episode: 90/100 RapTime: 0:00:46.097043 FixedProfit: 1193814\n",
            "Episode: 91/100 RapTime: 0:00:45.671020 FixedProfit: 1031104\n",
            "Episode: 92/100 RapTime: 0:00:45.468061 FixedProfit: 1097089\n",
            "Episode: 93/100 RapTime: 0:00:48.993515 FixedProfit: 1206002\n",
            "Episode: 94/100 RapTime: 0:00:46.454797 FixedProfit: 1120271\n",
            "Episode: 95/100 RapTime: 0:00:47.624085 FixedProfit: 1219526\n",
            "Episode: 96/100 RapTime: 0:00:49.248484 FixedProfit: 1108391\n",
            "Episode: 97/100 RapTime: 0:00:44.573730 FixedProfit: 1217688\n",
            "Episode: 98/100 RapTime: 0:00:45.569520 FixedProfit: 1126726\n",
            "Episode: 99/100 RapTime: 0:00:46.479023 FixedProfit: 1138009\n",
            "Episode: 100/100 RapTime: 0:00:47.159560 FixedProfit: 1059629\n"
          ]
        }
      ]
    }
  ]
}