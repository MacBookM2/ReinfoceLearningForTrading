{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddqn_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNv1x0Bmp/34hgyfmJ+slyh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/ddqn_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "69fb3592-4abf-4f5f-9ec7-5b86bda4f2f8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "mode = 'train'\n",
        "name = 'ddqn'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        model1 = Sequential()\n",
        "        model1.add(Dense(3, input_shape=(3,)))\n",
        "        model1.add(ReLU()) \n",
        "        model1.add(Dense(3))\n",
        "        model1.add(ReLU()) \n",
        "        model1.add(Dense(3))\n",
        "        model1.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        model1.summary()\n",
        "        self.model1 = model1\n",
        "\n",
        "        model2 = Sequential()\n",
        "        model2.add(Dense(3, input_shape=(3,)))\n",
        "        model2.add(ReLU()) \n",
        "        model2.add(Dense(3))\n",
        "        model2.add(ReLU()) \n",
        "        model2.add(Dense(3))\n",
        "        model2.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        model2.summary()\n",
        "        self.model2 = model2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent(Brain, ReplayMemory):\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        Brain.__init__(self)\n",
        "        ReplayMemory.__init__(self, max_size, batch_size)\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state,s_flag=12):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.predict(state,s_flag)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def predict(self, state, s_flag = 12):\n",
        "        values = None\n",
        "        q1 = self.model1.predict(state)\n",
        "        q2 = self.model2.predict(state)\n",
        "        if s_flag == 12:\n",
        "            values = np.array([q1[0,a] + q2[0,a] for a in range(2)])\n",
        "        elif s_flag == 11:\n",
        "            values = np.array([q1[0,a] + q1[0,a] for a in range(2)])\n",
        "        else:\n",
        "            values = np.array([q2[0,a] + q2[0,a] for a in range(2)])\n",
        "        return values\n",
        "\n",
        "    def replay(self, s_flag):\n",
        "        if self.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "\n",
        "        '''\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "        target_full = self.model.predict(states)\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "        '''\n",
        "\n",
        "        next_act_values = self.model1.predict(next_states,s_flag)\n",
        "        next_action =np.argmax(next_act_values)\n",
        "\n",
        "        if s_flag == 11:\n",
        "            q = self.model1.predict(states)  \n",
        "            next_q = self.model2.predict(next_states)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, actions] = rewards + (1 - done) * self.gamma*np.max(next_q, axis=1)\n",
        "            self.model1.train_on_batch(states, target)\n",
        "        else:\n",
        "            q = self.model2.predict(states)  \n",
        "            next_q = self.model1.predict(next_states)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, actions] = rewards + (1 - done) * self.gamma*np.max(next_q, axis=1)\n",
        "            self.model2.train_on_batch(states, target)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.model1.load_weights(name)\n",
        "        self.model2.load_weights(name2)\n",
        "\n",
        "    def save(self, name, name2):\n",
        "        self.model1.save_weights(name)\n",
        "        self.model2.save_weights(name2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                s_flag = 12\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    rand = np.random.random()\n",
        "                    if rand <= 0.5:\n",
        "                        s_flag = 11\n",
        "                    else:\n",
        "                        s_flag = 22\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                    self.agent.replay(s_flag)                \n",
        "            play_time = datetime.now() - start_time\n",
        "\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        self.agent.save('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "7f741605-4203-4816-f893-33056d430f6a"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "agent = Agent(max_size, batch_size)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/200 RapTime: 0:01:25.587990 FixedProfit: 1216612\n",
            "Episode: 2/200 RapTime: 0:01:37.772598 FixedProfit: 1200220\n",
            "Episode: 3/200 RapTime: 0:01:37.833684 FixedProfit: 1205416\n",
            "Episode: 4/200 RapTime: 0:01:38.204933 FixedProfit: 1197325\n",
            "Episode: 5/200 RapTime: 0:01:38.124658 FixedProfit: 1214084\n",
            "Episode: 6/200 RapTime: 0:01:38.573146 FixedProfit: 1186904\n",
            "Episode: 7/200 RapTime: 0:01:38.466613 FixedProfit: 1197165\n",
            "Episode: 8/200 RapTime: 0:01:38.140354 FixedProfit: 1219530\n",
            "Episode: 9/200 RapTime: 0:01:38.337495 FixedProfit: 1185640\n",
            "Episode: 10/200 RapTime: 0:01:38.319236 FixedProfit: 1200921\n",
            "Episode: 11/200 RapTime: 0:01:38.408151 FixedProfit: 1199342\n",
            "Episode: 12/200 RapTime: 0:01:38.241269 FixedProfit: 1192489\n",
            "Episode: 13/200 RapTime: 0:01:38.023514 FixedProfit: 1184599\n",
            "Episode: 14/200 RapTime: 0:01:38.460299 FixedProfit: 1198416\n",
            "Episode: 15/200 RapTime: 0:01:38.234323 FixedProfit: 1180145\n",
            "Episode: 16/200 RapTime: 0:01:38.390207 FixedProfit: 1199553\n",
            "Episode: 17/200 RapTime: 0:01:39.796733 FixedProfit: 1210790\n",
            "Episode: 18/200 RapTime: 0:01:40.407542 FixedProfit: 1187129\n",
            "Episode: 19/200 RapTime: 0:01:39.214903 FixedProfit: 1204234\n",
            "Episode: 20/200 RapTime: 0:01:38.627041 FixedProfit: 1205878\n",
            "Episode: 21/200 RapTime: 0:01:38.860431 FixedProfit: 1199369\n",
            "Episode: 22/200 RapTime: 0:01:38.740900 FixedProfit: 1214122\n",
            "Episode: 23/200 RapTime: 0:01:39.201941 FixedProfit: 1243736\n",
            "Episode: 24/200 RapTime: 0:01:38.929023 FixedProfit: 1207916\n",
            "Episode: 25/200 RapTime: 0:01:38.969929 FixedProfit: 1195383\n",
            "Episode: 26/200 RapTime: 0:01:38.768762 FixedProfit: 1199709\n",
            "Episode: 27/200 RapTime: 0:01:38.950239 FixedProfit: 1207764\n",
            "Episode: 28/200 RapTime: 0:01:38.716052 FixedProfit: 1193249\n",
            "Episode: 29/200 RapTime: 0:01:38.948335 FixedProfit: 1198517\n",
            "Episode: 30/200 RapTime: 0:01:40.820258 FixedProfit: 1189400\n",
            "Episode: 31/200 RapTime: 0:01:43.931119 FixedProfit: 1198241\n",
            "Episode: 32/200 RapTime: 0:01:41.262845 FixedProfit: 1187127\n",
            "Episode: 33/200 RapTime: 0:01:40.858730 FixedProfit: 1200953\n",
            "Episode: 34/200 RapTime: 0:01:40.625889 FixedProfit: 1187907\n",
            "Episode: 35/200 RapTime: 0:01:39.796293 FixedProfit: 1224968\n",
            "Episode: 36/200 RapTime: 0:01:40.624387 FixedProfit: 1205390\n",
            "Episode: 37/200 RapTime: 0:01:40.192699 FixedProfit: 1189883\n",
            "Episode: 38/200 RapTime: 0:01:40.608480 FixedProfit: 1205590\n",
            "Episode: 39/200 RapTime: 0:01:39.769364 FixedProfit: 1197165\n",
            "Episode: 40/200 RapTime: 0:01:39.180958 FixedProfit: 1182770\n",
            "Episode: 41/200 RapTime: 0:01:39.135335 FixedProfit: 1204355\n",
            "Episode: 42/200 RapTime: 0:01:39.696198 FixedProfit: 1193695\n",
            "Episode: 43/200 RapTime: 0:01:39.696110 FixedProfit: 1152527\n",
            "Episode: 44/200 RapTime: 0:01:35.582935 FixedProfit: 1197165\n",
            "Episode: 45/200 RapTime: 0:01:34.215773 FixedProfit: 1197165\n",
            "Episode: 46/200 RapTime: 0:01:33.487259 FixedProfit: 1167631\n",
            "Episode: 47/200 RapTime: 0:01:33.025336 FixedProfit: 1200895\n",
            "Episode: 48/200 RapTime: 0:01:33.791347 FixedProfit: 1197165\n",
            "Episode: 49/200 RapTime: 0:01:33.490598 FixedProfit: 1207534\n",
            "Episode: 50/200 RapTime: 0:01:33.395974 FixedProfit: 1195790\n",
            "Episode: 51/200 RapTime: 0:01:33.856614 FixedProfit: 1203085\n",
            "Episode: 52/200 RapTime: 0:01:34.531300 FixedProfit: 1198604\n",
            "Episode: 53/200 RapTime: 0:01:33.804851 FixedProfit: 1192449\n",
            "Episode: 54/200 RapTime: 0:01:34.013834 FixedProfit: 1206120\n",
            "Episode: 55/200 RapTime: 0:01:36.556016 FixedProfit: 1200835\n",
            "Episode: 56/200 RapTime: 0:01:33.327592 FixedProfit: 1197384\n",
            "Episode: 57/200 RapTime: 0:01:33.591047 FixedProfit: 1211310\n",
            "Episode: 58/200 RapTime: 0:01:34.573434 FixedProfit: 1187132\n",
            "Episode: 59/200 RapTime: 0:01:33.557985 FixedProfit: 1181108\n",
            "Episode: 60/200 RapTime: 0:01:32.983217 FixedProfit: 1193960\n",
            "Episode: 61/200 RapTime: 0:01:33.420697 FixedProfit: 1182631\n",
            "Episode: 62/200 RapTime: 0:01:33.250805 FixedProfit: 1172245\n",
            "Episode: 63/200 RapTime: 0:01:33.042663 FixedProfit: 1197165\n",
            "Episode: 64/200 RapTime: 0:01:33.204825 FixedProfit: 1188809\n",
            "Episode: 65/200 RapTime: 0:01:33.609053 FixedProfit: 1199510\n",
            "Episode: 66/200 RapTime: 0:01:33.447203 FixedProfit: 1177871\n",
            "Episode: 67/200 RapTime: 0:01:33.018294 FixedProfit: 1185214\n",
            "Episode: 68/200 RapTime: 0:01:33.468437 FixedProfit: 1188833\n",
            "Episode: 69/200 RapTime: 0:01:33.551881 FixedProfit: 1197165\n",
            "Episode: 70/200 RapTime: 0:01:33.292460 FixedProfit: 1187043\n",
            "Episode: 71/200 RapTime: 0:01:33.442539 FixedProfit: 1230967\n",
            "Episode: 72/200 RapTime: 0:01:33.746180 FixedProfit: 1192483\n",
            "Episode: 73/200 RapTime: 0:01:33.364293 FixedProfit: 1217204\n",
            "Episode: 74/200 RapTime: 0:01:35.470821 FixedProfit: 1188316\n",
            "Episode: 75/200 RapTime: 0:01:34.113372 FixedProfit: 1202597\n",
            "Episode: 76/200 RapTime: 0:01:33.677999 FixedProfit: 1180543\n",
            "Episode: 77/200 RapTime: 0:01:35.056327 FixedProfit: 1164261\n",
            "Episode: 78/200 RapTime: 0:01:36.586592 FixedProfit: 1213311\n",
            "Episode: 79/200 RapTime: 0:01:33.890660 FixedProfit: 1201720\n",
            "Episode: 80/200 RapTime: 0:01:33.599819 FixedProfit: 1202603\n",
            "Episode: 81/200 RapTime: 0:01:33.965162 FixedProfit: 1203264\n",
            "Episode: 82/200 RapTime: 0:01:33.566668 FixedProfit: 1193634\n",
            "Episode: 83/200 RapTime: 0:01:33.515016 FixedProfit: 1197165\n",
            "Episode: 84/200 RapTime: 0:01:33.575102 FixedProfit: 1197706\n",
            "Episode: 85/200 RapTime: 0:01:33.610396 FixedProfit: 1200689\n",
            "Episode: 86/200 RapTime: 0:01:33.284273 FixedProfit: 1191823\n",
            "Episode: 87/200 RapTime: 0:01:34.812408 FixedProfit: 1206126\n",
            "Episode: 88/200 RapTime: 0:01:34.422529 FixedProfit: 1174914\n",
            "Episode: 89/200 RapTime: 0:01:34.816915 FixedProfit: 1198070\n",
            "Episode: 90/200 RapTime: 0:01:34.166830 FixedProfit: 1205249\n",
            "Episode: 91/200 RapTime: 0:01:34.059249 FixedProfit: 1193982\n",
            "Episode: 92/200 RapTime: 0:01:34.401496 FixedProfit: 1232914\n",
            "Episode: 93/200 RapTime: 0:01:34.324003 FixedProfit: 1186312\n",
            "Episode: 94/200 RapTime: 0:01:34.845630 FixedProfit: 1197165\n",
            "Episode: 95/200 RapTime: 0:01:34.746544 FixedProfit: 1244931\n",
            "Episode: 96/200 RapTime: 0:01:34.471631 FixedProfit: 1172739\n",
            "Episode: 97/200 RapTime: 0:01:34.730724 FixedProfit: 1198957\n",
            "Episode: 98/200 RapTime: 0:01:33.855300 FixedProfit: 1185155\n",
            "Episode: 99/200 RapTime: 0:01:34.108081 FixedProfit: 1170548\n",
            "Episode: 100/200 RapTime: 0:01:34.405288 FixedProfit: 1163105\n",
            "Episode: 101/200 RapTime: 0:01:34.123156 FixedProfit: 1189304\n",
            "Episode: 102/200 RapTime: 0:01:34.496754 FixedProfit: 1207868\n",
            "Episode: 103/200 RapTime: 0:01:34.342924 FixedProfit: 1189400\n",
            "Episode: 104/200 RapTime: 0:01:34.171196 FixedProfit: 1194561\n",
            "Episode: 105/200 RapTime: 0:01:34.278505 FixedProfit: 1205017\n",
            "Episode: 106/200 RapTime: 0:01:34.195673 FixedProfit: 1166482\n",
            "Episode: 107/200 RapTime: 0:01:34.156618 FixedProfit: 1177155\n",
            "Episode: 108/200 RapTime: 0:01:33.711576 FixedProfit: 1164346\n",
            "Episode: 109/200 RapTime: 0:01:33.715693 FixedProfit: 1205507\n",
            "Episode: 110/200 RapTime: 0:01:33.880414 FixedProfit: 1196592\n",
            "Episode: 111/200 RapTime: 0:01:34.255749 FixedProfit: 1208778\n",
            "Episode: 112/200 RapTime: 0:01:33.882452 FixedProfit: 1183516\n",
            "Episode: 113/200 RapTime: 0:01:33.968252 FixedProfit: 1197165\n",
            "Episode: 114/200 RapTime: 0:01:33.925626 FixedProfit: 1165186\n",
            "Episode: 115/200 RapTime: 0:01:34.035724 FixedProfit: 1197165\n",
            "Episode: 116/200 RapTime: 0:01:33.899473 FixedProfit: 1194477\n",
            "Episode: 117/200 RapTime: 0:01:33.875294 FixedProfit: 1200533\n",
            "Episode: 118/200 RapTime: 0:01:34.355384 FixedProfit: 1191880\n",
            "Episode: 119/200 RapTime: 0:01:33.869340 FixedProfit: 1195333\n",
            "Episode: 120/200 RapTime: 0:01:34.049263 FixedProfit: 1193986\n",
            "Episode: 121/200 RapTime: 0:01:35.222720 FixedProfit: 1218990\n",
            "Episode: 122/200 RapTime: 0:01:33.958912 FixedProfit: 1189891\n",
            "Episode: 123/200 RapTime: 0:01:34.260739 FixedProfit: 1197165\n",
            "Episode: 124/200 RapTime: 0:01:34.078175 FixedProfit: 1187051\n",
            "Episode: 125/200 RapTime: 0:01:34.868245 FixedProfit: 1197165\n",
            "Episode: 126/200 RapTime: 0:01:34.590480 FixedProfit: 1209267\n",
            "Episode: 127/200 RapTime: 0:01:34.102225 FixedProfit: 1210613\n",
            "Episode: 128/200 RapTime: 0:01:34.594990 FixedProfit: 1195094\n",
            "Episode: 129/200 RapTime: 0:01:33.680339 FixedProfit: 1213737\n",
            "Episode: 130/200 RapTime: 0:01:34.309845 FixedProfit: 1199573\n",
            "Episode: 131/200 RapTime: 0:01:34.775502 FixedProfit: 1197165\n",
            "Episode: 132/200 RapTime: 0:01:33.719010 FixedProfit: 1164101\n",
            "Episode: 133/200 RapTime: 0:01:33.989421 FixedProfit: 1210959\n",
            "Episode: 134/200 RapTime: 0:01:34.163714 FixedProfit: 1198714\n",
            "Episode: 135/200 RapTime: 0:01:34.000448 FixedProfit: 1211984\n",
            "Episode: 136/200 RapTime: 0:01:33.661328 FixedProfit: 1199565\n",
            "Episode: 137/200 RapTime: 0:01:33.883575 FixedProfit: 1172615\n",
            "Episode: 138/200 RapTime: 0:01:33.988954 FixedProfit: 1196736\n",
            "Episode: 139/200 RapTime: 0:01:33.851517 FixedProfit: 1171566\n",
            "Episode: 140/200 RapTime: 0:01:33.964213 FixedProfit: 1190504\n",
            "Episode: 141/200 RapTime: 0:01:34.265019 FixedProfit: 1188594\n",
            "Episode: 142/200 RapTime: 0:01:34.078899 FixedProfit: 1166943\n",
            "Episode: 143/200 RapTime: 0:01:34.084664 FixedProfit: 1172175\n",
            "Episode: 144/200 RapTime: 0:01:34.299129 FixedProfit: 1179280\n",
            "Episode: 145/200 RapTime: 0:01:33.991498 FixedProfit: 1196820\n",
            "Episode: 146/200 RapTime: 0:01:34.293319 FixedProfit: 1186075\n",
            "Episode: 147/200 RapTime: 0:01:33.746095 FixedProfit: 1210873\n",
            "Episode: 148/200 RapTime: 0:01:33.771498 FixedProfit: 1212355\n",
            "Episode: 149/200 RapTime: 0:01:34.168955 FixedProfit: 1177242\n",
            "Episode: 150/200 RapTime: 0:01:33.631534 FixedProfit: 1202783\n",
            "Episode: 151/200 RapTime: 0:01:33.622175 FixedProfit: 1205844\n",
            "Episode: 152/200 RapTime: 0:01:33.428674 FixedProfit: 1183430\n",
            "Episode: 153/200 RapTime: 0:01:33.304010 FixedProfit: 1187278\n",
            "Episode: 154/200 RapTime: 0:01:33.803728 FixedProfit: 1193080\n",
            "Episode: 155/200 RapTime: 0:01:33.330262 FixedProfit: 1160472\n",
            "Episode: 156/200 RapTime: 0:01:33.286695 FixedProfit: 1212395\n",
            "Episode: 157/200 RapTime: 0:01:33.508966 FixedProfit: 1189922\n",
            "Episode: 158/200 RapTime: 0:01:33.654380 FixedProfit: 1193242\n",
            "Episode: 159/200 RapTime: 0:01:33.877336 FixedProfit: 1197165\n",
            "Episode: 160/200 RapTime: 0:01:33.144454 FixedProfit: 1218755\n",
            "Episode: 161/200 RapTime: 0:01:33.743567 FixedProfit: 1207630\n",
            "Episode: 162/200 RapTime: 0:01:33.833276 FixedProfit: 1187363\n",
            "Episode: 163/200 RapTime: 0:01:33.059194 FixedProfit: 1211395\n",
            "Episode: 164/200 RapTime: 0:01:33.438701 FixedProfit: 1254763\n",
            "Episode: 165/200 RapTime: 0:01:33.617655 FixedProfit: 1197165\n",
            "Episode: 166/200 RapTime: 0:01:33.371281 FixedProfit: 1196524\n",
            "Episode: 167/200 RapTime: 0:01:34.072380 FixedProfit: 1208372\n",
            "Episode: 168/200 RapTime: 0:01:33.963217 FixedProfit: 1200741\n",
            "Episode: 169/200 RapTime: 0:01:33.217302 FixedProfit: 1184247\n",
            "Episode: 170/200 RapTime: 0:01:33.235565 FixedProfit: 1198382\n",
            "Episode: 171/200 RapTime: 0:01:33.471125 FixedProfit: 1184360\n",
            "Episode: 172/200 RapTime: 0:01:33.202113 FixedProfit: 1174293\n",
            "Episode: 173/200 RapTime: 0:01:33.483261 FixedProfit: 1168856\n",
            "Episode: 174/200 RapTime: 0:01:33.689850 FixedProfit: 1197165\n",
            "Episode: 175/200 RapTime: 0:01:33.303729 FixedProfit: 1222810\n",
            "Episode: 176/200 RapTime: 0:01:33.464219 FixedProfit: 1197165\n",
            "Episode: 177/200 RapTime: 0:01:33.267768 FixedProfit: 1207577\n",
            "Episode: 178/200 RapTime: 0:01:33.269764 FixedProfit: 1185747\n",
            "Episode: 179/200 RapTime: 0:01:33.492642 FixedProfit: 1193611\n",
            "Episode: 180/200 RapTime: 0:01:33.127220 FixedProfit: 1208661\n",
            "Episode: 181/200 RapTime: 0:01:33.783357 FixedProfit: 1197165\n",
            "Episode: 182/200 RapTime: 0:01:32.958887 FixedProfit: 1212456\n",
            "Episode: 183/200 RapTime: 0:01:33.195915 FixedProfit: 1198893\n",
            "Episode: 184/200 RapTime: 0:01:33.639155 FixedProfit: 1176493\n",
            "Episode: 185/200 RapTime: 0:01:33.993188 FixedProfit: 1204323\n",
            "Episode: 186/200 RapTime: 0:01:33.341868 FixedProfit: 1188873\n",
            "Episode: 187/200 RapTime: 0:01:33.546764 FixedProfit: 1212339\n",
            "Episode: 188/200 RapTime: 0:01:33.884784 FixedProfit: 1192545\n",
            "Episode: 189/200 RapTime: 0:01:33.424063 FixedProfit: 1193993\n",
            "Episode: 190/200 RapTime: 0:01:33.294862 FixedProfit: 1152704\n",
            "Episode: 191/200 RapTime: 0:01:33.650284 FixedProfit: 1172533\n",
            "Episode: 192/200 RapTime: 0:01:33.154171 FixedProfit: 1178710\n",
            "Episode: 193/200 RapTime: 0:01:33.572153 FixedProfit: 1191061\n",
            "Episode: 194/200 RapTime: 0:01:33.572158 FixedProfit: 1197165\n",
            "Episode: 195/200 RapTime: 0:01:33.349013 FixedProfit: 1196281\n",
            "Episode: 196/200 RapTime: 0:01:33.359152 FixedProfit: 1198734\n",
            "Episode: 197/200 RapTime: 0:01:33.492611 FixedProfit: 1199391\n",
            "Episode: 198/200 RapTime: 0:01:33.299463 FixedProfit: 1207260\n",
            "Episode: 199/200 RapTime: 0:01:33.141984 FixedProfit: 1197165\n",
            "Episode: 200/200 RapTime: 0:01:32.845627 FixedProfit: 1178906\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5b64c49892d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-766790f550b5>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_standard_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-766790f550b5>\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdl_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-5d552f650d97>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'model'"
          ]
        }
      ]
    }
  ]
}