{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ppo_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/ppo_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "e337ca83-70d5-4163-c542-f632ffc55648"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'train'\n",
        "name = 'ppo'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_shape = 3\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain):\n",
        "        self.model = brain.model\n",
        "        self.brain = brain\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            self._reduce_epsilon()\n",
        "            return np.random.choice(3)\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        self._reduce_epsilon()\n",
        "        return np.random.choice(3, p=act_p[0].numpy())\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def _reduce_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model):\n",
        "        self.model = model\n",
        "        self.gamma = 0.9\n",
        "        self.c_gamma = 0.1\n",
        "\n",
        "        self.beta_1 = 0.1\n",
        "        self.beta_2 = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "        advantage = np.asarray([e[\"advantage\"] for e in experiences]).reshape((-1, 1))\n",
        "        prob_batch = np.asarray([e[\"prob\"] for e in experiences])\n",
        "\n",
        "        advantage = (advantage - np.mean(advantage)) / (np.std(advantage) + 1e-8)\n",
        "        onehot_actions = tf.one_hot(action_batch, 3).numpy()\n",
        "\n",
        "        old_pi = tf.reduce_sum(onehot_actions * prob_batch, axis=1, keepdims=True)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "\n",
        "            advantage = advantage - tf.stop_gradient(v)\n",
        "\n",
        "            # π(a|s)とlog(π(a|s))を計算\n",
        "            new_pi = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            new_logpi = tf.math.log(tf.clip_by_value(new_pi, 1e-10, 1.0))\n",
        "\n",
        "            r_t_batch = new_pi / old_pi\n",
        "\n",
        "            losses_clip = self._losses_clip(advantage, r_t_batch)\n",
        "            losses_value = self._losses_value(advantage)\n",
        "            entropy = tf.reduce_sum(new_logpi * new_pi, axis=1, keepdims=True)\n",
        "\n",
        "            total_loss = - losses_clip + self.beta_1 * losses_value - self.beta_2 * entropy\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        # WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\n",
        "        # gradientsがnoneになる時がある。\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        # gradients, _ = tf.clip_by_global_norm(gradients, 0.5)\n",
        "        # self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        self.model.optimizer.apply_gradients((grad, var) for (grad, var) in \n",
        "                                             zip(gradients, self.model.trainable_variables) if grad is not None)\n",
        "\n",
        "    def _losses_value(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _losses_clip(self, advantage, r_t):\n",
        "\n",
        "        r_clip = tf.clip_by_value(r_t, 1 - self.c_gamma, 1 + self.c_gamma)\n",
        "\n",
        "        loss_unclipped = r_t * advantage\n",
        "        loss_clipped = r_clip * advantage\n",
        "\n",
        "        return tf.minimum(loss_clipped, loss_clipped)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1UX215aeoXq"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : List[List[float]] = field(default_factory=list)\n",
        "    action : List[int] = field(default_factory=list)\n",
        "    reward : List[float] = field(default_factory=list)\n",
        "    next_state : List[List[int]] = field(default_factory=list)\n",
        "    done : List[bool] = field(default_factory=list)\n",
        "    prob_bol : List[bool] = field(default_factory=list)\n",
        "    advantage : List[float] = field(default_factory=list)\n",
        "    model : str = None\n",
        "    gae_lambda: float = 0.9\n",
        "    gamma: float = 0.9\n",
        "\n",
        "    def random_experiences(self, batch_size):\n",
        "        max_size = len(self.state) - 1\n",
        "        self._make_advantage()\n",
        "        batch_num = self._random_num(1, max_size, batch_size)\n",
        "        experiences = []\n",
        "        for i in batch_num:\n",
        "            prob = self._probability(self.state[i])\n",
        "            prob = prob.numpy()\n",
        "            prob = prob[0]\n",
        "            experiences.append({\"state\": self.state[i], \"action\": self.action[i], \n",
        "                                \"reward\": self.reward[i], \"next_state\": self.next_state[i], \n",
        "                                \"done\": self.done[i], \"prob\": prob, \"advantage\": self.advantage[i]})\n",
        "        return experiences\n",
        "\n",
        "    def _random_num(self, a, b, k):\n",
        "        ns = []\n",
        "        while len(ns) < k:\n",
        "            n = random.randint(a, b)\n",
        "            if not n in ns:\n",
        "                ns.append(n)\n",
        "        return ns\n",
        "\n",
        "    def _make_advantage(self):\n",
        "\n",
        "        state = np.asarray([e for e in self.state])\n",
        "        next_state = np.asarray([e for e in self.next_state])\n",
        "        v     = self._evaluation_value(state)\n",
        "        v_old = self._evaluation_value(next_state)\n",
        "        v     =v.numpy()\n",
        "        v_old =v_old.numpy()\n",
        "\n",
        "        for i in range(len(self.reward)):\n",
        "            gae = 0\n",
        "            t = 0\n",
        "            for j in range(i, len(self.reward)):\n",
        "                delta = self.reward[j] + self.gamma * v_old[j][0] - v[j][0]\n",
        "                gae += ((self.gae_lambda * self.gamma) ** t) * delta\n",
        "                t += 1\n",
        "            self.advantage.append(gae)\n",
        "\n",
        "    def _probability(self, state):\n",
        "        p, _ = self.model(state.reshape((1,-1)))\n",
        "        return p\n",
        "\n",
        "    def _evaluation_value(self, state):\n",
        "        _, v = self.model(state, training=True)\n",
        "        return v\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, num, mdl_dir, name, batch_size = 128, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory(model = self.actor.model)\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action     = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    memory.state.append(state)\n",
        "                    memory.action.append(action)\n",
        "                    memory.reward.append(reward)\n",
        "                    memory.next_state.append(next_state)\n",
        "                    memory.done.append(done)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                experiences = memory.random_experiences(self.batch_size)\n",
        "                self.critic.valuenetwork(experiences)\n",
        "\n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\"\n",
        "                .format(episode + 1, episodes_times, play_time, info['cur_revenue'], \n",
        "                        info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\"\n",
        "                .format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgv85YlVOaum",
        "outputId": "2cec91a3-45d8-40df-96a8-8be311e70464"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 50\n",
        "batch_size = 128\n",
        "\n",
        "brain = ParameterServer()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    model = brain.model\n",
        "    actor = Actor(brain)\n",
        "    critic = Critic(model)\n",
        "    main = Main(env, actor, critic, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:00:09.230413 FixedProfit: 1175056\n",
            "Episode: 1/50 RapTime: 0:00:09.249819 FixedProfit: 1123671\n",
            "Episode: 1/50 RapTime: 0:00:09.271675 FixedProfit: 1238890\n",
            "Episode: 1/50 RapTime: 0:00:09.308009 FixedProfit: 958876\n",
            "Episode: 2/50 RapTime: 0:00:10.859845 FixedProfit: 1325032\n",
            "Episode: 2/50 RapTime: 0:00:10.946282 FixedProfit: 1259860\n",
            "Episode: 2/50 RapTime: 0:00:10.999897 FixedProfit: 1230019\n",
            "Episode: 2/50 RapTime: 0:00:10.970546 FixedProfit: 1230521\n",
            "Episode: 3/50 RapTime: 0:00:10.872244 FixedProfit: 1116030\n",
            "Episode: 3/50 RapTime: 0:00:10.921321 FixedProfit: 1233463\n",
            "Episode: 3/50 RapTime: 0:00:10.934634 FixedProfit: 1275742\n",
            "Episode: 3/50 RapTime: 0:00:10.980252 FixedProfit: 1215687\n",
            "Episode: 4/50 RapTime: 0:00:10.787300 FixedProfit: 1306206\n",
            "Episode: 4/50 RapTime: 0:00:10.837414 FixedProfit: 1272696\n",
            "Episode: 4/50 RapTime: 0:00:10.870250 FixedProfit: 1226846\n",
            "Episode: 4/50 RapTime: 0:00:10.890408 FixedProfit: 1108518\n",
            "Episode: 5/50 RapTime: 0:00:10.746316 FixedProfit: 1215769\n",
            "Episode: 5/50 RapTime: 0:00:10.861075 FixedProfit: 1147833\n",
            "Episode: 5/50 RapTime: 0:00:10.853132 FixedProfit: 1185848\n",
            "Episode: 5/50 RapTime: 0:00:10.861736 FixedProfit: 1090167\n",
            "Episode: 6/50 RapTime: 0:00:10.879480 FixedProfit: 1262804\n",
            "Episode: 6/50 RapTime: 0:00:10.890448 FixedProfit: 1150996\n",
            "Episode: 6/50 RapTime: 0:00:10.943413 FixedProfit: 1190273\n",
            "Episode: 6/50 RapTime: 0:00:10.906806 FixedProfit: 1318943\n",
            "Episode: 7/50 RapTime: 0:00:10.864026 FixedProfit: 1265973\n",
            "Episode: 7/50 RapTime: 0:00:10.797642 FixedProfit: 1193037\n",
            "Episode: 7/50 RapTime: 0:00:10.858416 FixedProfit: 1204680\n",
            "Episode: 7/50 RapTime: 0:00:10.893145 FixedProfit: 1177998\n",
            "Episode: 8/50 RapTime: 0:00:10.931419 FixedProfit: 1176827\n",
            "Episode: 8/50 RapTime: 0:00:11.054221 FixedProfit: 1189556\n",
            "Episode: 8/50 RapTime: 0:00:11.070611 FixedProfit: 1113379\n",
            "Episode: 8/50 RapTime: 0:00:11.033756 FixedProfit: 1224117\n",
            "Episode: 9/50 RapTime: 0:00:10.826022 FixedProfit: 1230259\n",
            "Episode: 9/50 RapTime: 0:00:10.839840 FixedProfit: 1296028\n",
            "Episode: 9/50 RapTime: 0:00:10.875534 FixedProfit: 1300413\n",
            "Episode: 9/50 RapTime: 0:00:10.890935 FixedProfit: 1257583\n",
            "Episode: 10/50 RapTime: 0:00:10.831504 FixedProfit: 1286010\n",
            "Episode: 10/50 RapTime: 0:00:10.864354 FixedProfit: 1184018\n",
            "Episode: 10/50 RapTime: 0:00:10.851258 FixedProfit: 1167766\n",
            "Episode: 10/50 RapTime: 0:00:10.897601 FixedProfit: 1266780\n",
            "Episode: 11/50 RapTime: 0:00:10.879248 FixedProfit: 1205351\n",
            "Episode: 11/50 RapTime: 0:00:10.812707 FixedProfit: 1160739\n",
            "Episode: 11/50 RapTime: 0:00:10.882411 FixedProfit: 1241555\n",
            "Episode: 11/50 RapTime: 0:00:10.816969 FixedProfit: 1241792\n",
            "Episode: 12/50 RapTime: 0:00:10.836465 FixedProfit: 1253298\n",
            "Episode: 12/50 RapTime: 0:00:10.832024 FixedProfit: 1183418\n",
            "Episode: 12/50 RapTime: 0:00:10.857144 FixedProfit: 1165025\n",
            "Episode: 12/50 RapTime: 0:00:10.818997 FixedProfit: 1224789\n",
            "Episode: 13/50 RapTime: 0:00:10.894169 FixedProfit: 1192856\n",
            "Episode: 13/50 RapTime: 0:00:10.867166 FixedProfit: 1134782\n",
            "Episode: 13/50 RapTime: 0:00:10.840000 FixedProfit: 1245555\n",
            "Episode: 13/50 RapTime: 0:00:10.866996 FixedProfit: 1078458\n",
            "Episode: 14/50 RapTime: 0:00:10.834347 FixedProfit: 1199012\n",
            "Episode: 14/50 RapTime: 0:00:10.912395 FixedProfit: 1181512\n",
            "Episode: 14/50 RapTime: 0:00:10.930502 FixedProfit: 1123010\n",
            "Episode: 14/50 RapTime: 0:00:10.883175 FixedProfit: 1112528\n",
            "Episode: 15/50 RapTime: 0:00:10.830963 FixedProfit: 1155824\n",
            "Episode: 15/50 RapTime: 0:00:10.817382 FixedProfit: 1293213\n",
            "Episode: 15/50 RapTime: 0:00:10.883897 FixedProfit: 1251008\n",
            "Episode: 15/50 RapTime: 0:00:10.921761 FixedProfit: 1211981\n",
            "Episode: 16/50 RapTime: 0:00:10.772023 FixedProfit: 1215745\n",
            "Episode: 16/50 RapTime: 0:00:10.776201 FixedProfit: 1235376\n",
            "Episode: 16/50 RapTime: 0:00:10.782959 FixedProfit: 1206503\n",
            "Episode: 16/50 RapTime: 0:00:10.807753 FixedProfit: 1230790\n",
            "Episode: 17/50 RapTime: 0:00:10.790537 FixedProfit: 1157244\n",
            "Episode: 17/50 RapTime: 0:00:10.864401 FixedProfit: 1166889\n",
            "Episode: 17/50 RapTime: 0:00:10.837867 FixedProfit: 1184858\n",
            "Episode: 17/50 RapTime: 0:00:10.843349 FixedProfit: 1217371\n",
            "Episode: 18/50 RapTime: 0:00:10.794629 FixedProfit: 1148819\n",
            "Episode: 18/50 RapTime: 0:00:10.904465 FixedProfit: 1196816\n",
            "Episode: 18/50 RapTime: 0:00:10.861447 FixedProfit: 1193323\n",
            "Episode: 18/50 RapTime: 0:00:10.838500 FixedProfit: 1292945\n",
            "Episode: 19/50 RapTime: 0:00:10.784456 FixedProfit: 1176967\n",
            "Episode: 19/50 RapTime: 0:00:10.850484 FixedProfit: 1174835\n",
            "Episode: 19/50 RapTime: 0:00:10.909265 FixedProfit: 1194118\n",
            "Episode: 19/50 RapTime: 0:00:10.917616 FixedProfit: 1230666\n",
            "Episode: 20/50 RapTime: 0:00:11.096461 FixedProfit: 1178441\n",
            "Episode: 20/50 RapTime: 0:00:11.036554 FixedProfit: 1146068\n",
            "Episode: 20/50 RapTime: 0:00:11.146243 FixedProfit: 1196400\n",
            "Episode: 20/50 RapTime: 0:00:11.072948 FixedProfit: 1205162\n",
            "Episode: 21/50 RapTime: 0:00:10.621405 FixedProfit: 1176207\n",
            "Episode: 21/50 RapTime: 0:00:10.905763 FixedProfit: 1174380\n",
            "Episode: 21/50 RapTime: 0:00:10.830326 FixedProfit: 1140897\n",
            "Episode: 21/50 RapTime: 0:00:10.942222 FixedProfit: 1225782\n",
            "Episode: 22/50 RapTime: 0:00:11.051189 FixedProfit: 1250113\n",
            "Episode: 22/50 RapTime: 0:00:10.799401 FixedProfit: 1166552\n",
            "Episode: 22/50 RapTime: 0:00:10.830200 FixedProfit: 1219898\n",
            "Episode: 22/50 RapTime: 0:00:10.817798 FixedProfit: 1188941\n",
            "Episode: 23/50 RapTime: 0:00:10.906361 FixedProfit: 1208189\n",
            "Episode: 23/50 RapTime: 0:00:10.769719 FixedProfit: 1157158\n",
            "Episode: 23/50 RapTime: 0:00:10.939219 FixedProfit: 1182789\n",
            "Episode: 23/50 RapTime: 0:00:10.897540 FixedProfit: 1177650\n",
            "Episode: 24/50 RapTime: 0:00:10.794604 FixedProfit: 1229527\n",
            "Episode: 24/50 RapTime: 0:00:10.759629 FixedProfit: 1209589\n",
            "Episode: 24/50 RapTime: 0:00:10.814511 FixedProfit: 1181905\n",
            "Episode: 24/50 RapTime: 0:00:10.761652 FixedProfit: 1196887\n",
            "Episode: 25/50 RapTime: 0:00:10.670319 FixedProfit: 1194830\n",
            "Episode: 25/50 RapTime: 0:00:10.641546 FixedProfit: 1154587\n",
            "Episode: 25/50 RapTime: 0:00:10.652391 FixedProfit: 1226719\n",
            "Episode: 25/50 RapTime: 0:00:10.683067 FixedProfit: 1220651\n",
            "Episode: 26/50 RapTime: 0:00:10.682475 FixedProfit: 1245352\n",
            "Episode: 26/50 RapTime: 0:00:10.672107 FixedProfit: 1215747\n",
            "Episode: 26/50 RapTime: 0:00:10.713705 FixedProfit: 1202022\n",
            "Episode: 26/50 RapTime: 0:00:10.695687 FixedProfit: 1206354\n",
            "Episode: 27/50 RapTime: 0:00:10.649566 FixedProfit: 1199651\n",
            "Episode: 27/50 RapTime: 0:00:10.612428 FixedProfit: 1202209\n",
            "Episode: 27/50 RapTime: 0:00:10.583925 FixedProfit: 1196929\n",
            "Episode: 27/50 RapTime: 0:00:10.683908 FixedProfit: 1220404\n",
            "Episode: 28/50 RapTime: 0:00:10.527148 FixedProfit: 1188953\n",
            "Episode: 28/50 RapTime: 0:00:10.612831 FixedProfit: 1134165\n",
            "Episode: 28/50 RapTime: 0:00:10.686083 FixedProfit: 1156296\n",
            "Episode: 28/50 RapTime: 0:00:10.753512 FixedProfit: 1165102\n",
            "Episode: 29/50 RapTime: 0:00:09.587769 FixedProfit: 1182284\n",
            "Episode: 29/50 RapTime: 0:00:10.725303 FixedProfit: 1175670\n",
            "Episode: 29/50 RapTime: 0:00:10.711707 FixedProfit: 1205707\n",
            "Episode: 29/50 RapTime: 0:00:10.775750 FixedProfit: 1197785\n",
            "Episode: 30/50 RapTime: 0:00:10.489888 FixedProfit: 1200231\n",
            "Episode: 30/50 RapTime: 0:00:10.742260 FixedProfit: 1174644\n",
            "Episode: 30/50 RapTime: 0:00:10.680042 FixedProfit: 1173770\n",
            "Episode: 30/50 RapTime: 0:00:10.717955 FixedProfit: 1196279\n",
            "Episode: 31/50 RapTime: 0:00:11.076931 FixedProfit: 1257348\n",
            "Episode: 31/50 RapTime: 0:00:10.704013 FixedProfit: 1172297\n",
            "Episode: 31/50 RapTime: 0:00:10.546911 FixedProfit: 1208932\n",
            "Episode: 31/50 RapTime: 0:00:10.668134 FixedProfit: 1207780\n",
            "Episode: 32/50 RapTime: 0:00:10.599978 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:10.593209 FixedProfit: 1190336\n",
            "Episode: 32/50 RapTime: 0:00:10.553558 FixedProfit: 1186405\n",
            "Episode: 32/50 RapTime: 0:00:10.639642 FixedProfit: 1191379\n",
            "Episode: 33/50 RapTime: 0:00:10.623141 FixedProfit: 1191259\n",
            "Episode: 33/50 RapTime: 0:00:10.552346 FixedProfit: 1197299\n",
            "Episode: 33/50 RapTime: 0:00:10.591888 FixedProfit: 1187756\n",
            "Episode: 33/50 RapTime: 0:00:10.581930 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:10.650138 FixedProfit: 1166416\n",
            "Episode: 34/50 RapTime: 0:00:10.648494 FixedProfit: 1199191\n",
            "Episode: 34/50 RapTime: 0:00:10.609955 FixedProfit: 1194401\n",
            "Episode: 34/50 RapTime: 0:00:10.597269 FixedProfit: 1194649\n",
            "Episode: 35/50 RapTime: 0:00:10.547525 FixedProfit: 1195459\n",
            "Episode: 35/50 RapTime: 0:00:10.602997 FixedProfit: 1197285\n",
            "Episode: 35/50 RapTime: 0:00:10.584957 FixedProfit: 1196721\n",
            "Episode: 35/50 RapTime: 0:00:10.645679 FixedProfit: 1212764\n",
            "Episode: 36/50 RapTime: 0:00:10.780414 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:10.704044 FixedProfit: 1169752\n",
            "Episode: 36/50 RapTime: 0:00:10.693136 FixedProfit: 1191336\n",
            "Episode: 36/50 RapTime: 0:00:10.694304 FixedProfit: 1193052\n",
            "Episode: 37/50 RapTime: 0:00:10.589754 FixedProfit: 1159780\n",
            "Episode: 37/50 RapTime: 0:00:10.579538 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:10.562999 FixedProfit: 1194104\n",
            "Episode: 37/50 RapTime: 0:00:10.581485 FixedProfit: 1177462\n",
            "Episode: 38/50 RapTime: 0:00:10.645764 FixedProfit: 1193618\n",
            "Episode: 38/50 RapTime: 0:00:10.673203 FixedProfit: 1197516\n",
            "Episode: 38/50 RapTime: 0:00:10.610333 FixedProfit: 1171353\n",
            "Episode: 38/50 RapTime: 0:00:10.620795 FixedProfit: 1191460\n",
            "Episode: 39/50 RapTime: 0:00:10.530774 FixedProfit: 1178941\n",
            "Episode: 39/50 RapTime: 0:00:10.651809 FixedProfit: 1199725\n",
            "Episode: 39/50 RapTime: 0:00:10.622347 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:10.614204 FixedProfit: 1196613\n",
            "Episode: 40/50 RapTime: 0:00:10.687708 FixedProfit: 1184007\n",
            "Episode: 40/50 RapTime: 0:00:10.574345 FixedProfit: 1192395\n",
            "Episode: 40/50 RapTime: 0:00:10.648282 FixedProfit: 1168218\n",
            "Episode: 40/50 RapTime: 0:00:10.673379 FixedProfit: 1160787\n",
            "Episode: 41/50 RapTime: 0:00:10.635826 FixedProfit: 1207790\n",
            "Episode: 41/50 RapTime: 0:00:10.711660 FixedProfit: 1199299\n",
            "Episode: 41/50 RapTime: 0:00:10.660949 FixedProfit: 1207519\n",
            "Episode: 41/50 RapTime: 0:00:10.647111 FixedProfit: 1174448\n",
            "Episode: 42/50 RapTime: 0:00:10.513635 FixedProfit: 1182593\n",
            "Episode: 42/50 RapTime: 0:00:10.517486 FixedProfit: 1200860\n",
            "Episode: 42/50 RapTime: 0:00:10.533379 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:10.567400 FixedProfit: 1198096\n",
            "Episode: 43/50 RapTime: 0:00:10.518278 FixedProfit: 1201158\n",
            "Episode: 43/50 RapTime: 0:00:10.416805 FixedProfit: 1198257\n",
            "Episode: 43/50 RapTime: 0:00:10.484993 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:10.462050 FixedProfit: 1197714\n",
            "Episode: 44/50 RapTime: 0:00:10.464400 FixedProfit: 1192227\n",
            "Episode: 44/50 RapTime: 0:00:10.488314 FixedProfit: 1219947\n",
            "Episode: 44/50 RapTime: 0:00:10.397586 FixedProfit: 1210909\n",
            "Episode: 44/50 RapTime: 0:00:10.547296 FixedProfit: 1208636\n",
            "Episode: 45/50 RapTime: 0:00:10.476645 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:10.400032 FixedProfit: 1218208\n",
            "Episode: 45/50 RapTime: 0:00:10.417103 FixedProfit: 1222859\n",
            "Episode: 45/50 RapTime: 0:00:10.366115 FixedProfit: 1201417\n",
            "Episode: 46/50 RapTime: 0:00:10.662763 FixedProfit: 1197121\n",
            "Episode: 46/50 RapTime: 0:00:10.656417 FixedProfit: 1207513\n",
            "Episode: 46/50 RapTime: 0:00:10.654523 FixedProfit: 1212988\n",
            "Episode: 46/50 RapTime: 0:00:10.691302 FixedProfit: 1196651\n",
            "Episode: 47/50 RapTime: 0:00:10.695390 FixedProfit: 1196516\n",
            "Episode: 47/50 RapTime: 0:00:10.783080 FixedProfit: 1206919\n",
            "Episode: 47/50 RapTime: 0:00:10.747638 FixedProfit: 1182581\n",
            "Episode: 47/50 RapTime: 0:00:10.698793 FixedProfit: 1192250\n",
            "Episode: 48/50 RapTime: 0:00:10.735407 FixedProfit: 1206337\n",
            "Episode: 48/50 RapTime: 0:00:10.697318 FixedProfit: 1203818\n",
            "Episode: 48/50 RapTime: 0:00:10.807389 FixedProfit: 1199002\n",
            "Episode: 48/50 RapTime: 0:00:10.844593 FixedProfit: 1193655\n",
            "Episode: 49/50 RapTime: 0:00:10.940226 FixedProfit: 1189769\n",
            "Episode: 49/50 RapTime: 0:00:10.941565 FixedProfit: 1173574\n",
            "Episode: 49/50 RapTime: 0:00:10.904304 FixedProfit: 1199112\n",
            "Episode: 49/50 RapTime: 0:00:10.867916 FixedProfit: 1203645\n",
            "Episode: 50/50 RapTime: 0:00:10.559983 FixedProfit: 1196736\n",
            "Episode: 50/50 RapTime: 0:00:10.528587 FixedProfit: 1211512\n",
            "Episode: 50/50 RapTime: 0:00:10.303411 FixedProfit: 1191017\n",
            "Episode: 50/50 RapTime: 0:00:09.887063 FixedProfit: 1205155\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}