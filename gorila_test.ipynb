{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gorila_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNGLogmvHlP8Kk7JGRR5gFR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/gorila_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "d7f0c288-d15a-4a0a-b33b-0d12c58b2d19"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_test.csv'\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'gorila_test.csv'\n",
        "\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "rewards_folder = '/content/drive/My Drive/' + exp_dir + 'rl_rewards'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=1000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1RNmtkaZ2W"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        mastermodel = Sequential()\n",
        "        mastermodel.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_mid))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_action))\n",
        "        mastermodel.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((mastermodel.summary()))\n",
        "        self.mastermodel = mastermodel\n",
        "    \n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "        self.masterbrain = masterbrain\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def train_on_batch(self, state, target_full):\n",
        "        self.model.train_on_batch(state, target_full)\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULBV5XpsbOjq"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, memory):\n",
        "\n",
        "        self.brain = brain\n",
        "        self.memory = memory\n",
        "        self.epsilon = 1.0\n",
        "  \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.brain.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w1_BMH7hLQ8"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],self.acts_memory[mb_index],self.rewards_memory[mb_index],self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42NIN-PGBOc8"
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, brain, memory, batch_size=32):\n",
        "\n",
        "        self.brain = brain\n",
        "        self.memory = memory\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay_rate = 0.995\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.brain.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.brain.predict(states)\n",
        "\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.brain.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay_rate\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "def play_game(env, actor, learner, scaler, episodes_times = 25, mode = 'test'):\n",
        "\n",
        "    actor.layering()\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        done = False\n",
        "        start_time = datetime.now()\n",
        "       \n",
        "        while not done:\n",
        "            action = actor.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "\n",
        "            if mode == 'train':\n",
        "                actor.store_transition(state, action, reward, next_state, done)\n",
        "                learner.learn()\n",
        "\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            learner.integration()\n",
        "            actor.layering()\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)\n",
        "        \n",
        "        state = next_state"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "5e545566-0137-40a0-aa52-0acfa072d8cc"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 25\n",
        "batch_size = 32\n",
        "mode = 'test'\n",
        "max_size = 500\n",
        "\n",
        "masterbrain = ParameterServer()\n",
        "\n",
        "if mode == 'test':\n",
        "    masterbrain.load(f'{models_folder}/gorila_model.h5')\n",
        "\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "        print(row, file=f)\n",
        "else:\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit'\n",
        "        print(row, file=f)\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    e = Environment(df, initial_money=initial_money, mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    memory = ReplayMemory(max_size, batch_size)\n",
        "    a = Actor(brain, memory)\n",
        "    l = Learner(brain, memory, batch_size)\n",
        "    if mode == 'test':\n",
        "        a.epsilon = 0.01\n",
        "    s = make_scaler(e)\n",
        "    arr = [e,a,l,s]\n",
        "    envs.append(arr)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: play_game(env[0], env[1], env[2], env[3], episodes_times, mode)\n",
        "        datas.append(executor.submit(job))\n",
        "\n",
        "if mode == 'train':\n",
        "    masterbrain.save(f'{models_folder}/gorila_model.h5')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_5 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_6 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_7 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_8 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_9 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/25 RapTime: 0:02:29.614717 FixedProfit: 982181 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:02:29.635146 FixedProfit: 983826 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 1/25 RapTime: 0:02:29.869828 FixedProfit: 1011553 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 1/25 RapTime: 0:02:33.057959 FixedProfit: 974449 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:02:29.468324 FixedProfit: 984106 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 2/25 RapTime: 0:02:29.963234 FixedProfit: 1024152 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 2/25 RapTime: 0:02:30.986298 FixedProfit: 994971 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:02:28.901842 FixedProfit: 974994 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 3/25 RapTime: 0:02:29.645771 FixedProfit: 919613 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:02:28.441434 FixedProfit: 996438 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 3/25 RapTime: 0:02:31.271674 FixedProfit: 996367 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 3/25 RapTime: 0:02:32.680949 FixedProfit: 1020380 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 4/25 RapTime: 0:02:30.867378 FixedProfit: 1011860 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 4/25 RapTime: 0:02:31.322908 FixedProfit: 1003971 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 4/25 RapTime: 0:02:29.935640 FixedProfit: 1009798 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:02:31.906203 FixedProfit: 975849 TradeTimes: 5 TradeWin: 0\n",
            "Episode: 5/25 RapTime: 0:02:31.047421 FixedProfit: 1008231 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 5/25 RapTime: 0:02:30.825117 FixedProfit: 1014108 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:02:31.782284 FixedProfit: 994389 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:02:31.936483 FixedProfit: 991521 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:02:31.803936 FixedProfit: 985694 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 6/25 RapTime: 0:02:32.314791 FixedProfit: 992861 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:02:34.411184 FixedProfit: 999644 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 6/25 RapTime: 0:02:29.327721 FixedProfit: 979424 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:02:30.399637 FixedProfit: 998463 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 7/25 RapTime: 0:02:34.818008 FixedProfit: 953574 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 7/25 RapTime: 0:02:33.548279 FixedProfit: 1005186 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 7/25 RapTime: 0:02:32.823511 FixedProfit: 1014185 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 8/25 RapTime: 0:02:32.379579 FixedProfit: 966554 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 8/25 RapTime: 0:02:32.742457 FixedProfit: 1012953 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 8/25 RapTime: 0:02:32.252815 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 8/25 RapTime: 0:02:31.918823 FixedProfit: 959141 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 9/25 RapTime: 0:02:34.901210 FixedProfit: 986534 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 9/25 RapTime: 0:02:32.782264 FixedProfit: 1028800 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 9/25 RapTime: 0:02:32.505852 FixedProfit: 979491 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 9/25 RapTime: 0:02:35.207747 FixedProfit: 999626 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 10/25 RapTime: 0:02:31.559018 FixedProfit: 880440 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 10/25 RapTime: 0:02:35.219370 FixedProfit: 990904 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:02:36.091950 FixedProfit: 1034054 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:02:36.120318 FixedProfit: 998738 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 11/25 RapTime: 0:02:33.710354 FixedProfit: 1021017 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 11/25 RapTime: 0:02:32.974926 FixedProfit: 999357 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 11/25 RapTime: 0:02:35.287982 FixedProfit: 991007 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 11/25 RapTime: 0:02:36.663524 FixedProfit: 1003613 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:02:34.476479 FixedProfit: 1002059 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:02:35.094851 FixedProfit: 991704 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:02:32.282177 FixedProfit: 1011723 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 12/25 RapTime: 0:02:35.180525 FixedProfit: 997858 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 13/25 RapTime: 0:02:34.608619 FixedProfit: 982627 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:02:33.834319 FixedProfit: 1065944 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 13/25 RapTime: 0:02:38.013266 FixedProfit: 999461 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 13/25 RapTime: 0:02:35.829637 FixedProfit: 989668 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 14/25 RapTime: 0:02:35.608708 FixedProfit: 1001078 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:02:35.465579 FixedProfit: 964127 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:02:35.964250 FixedProfit: 996364 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 14/25 RapTime: 0:02:37.530015 FixedProfit: 1016229 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 15/25 RapTime: 0:02:36.031798 FixedProfit: 999886 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 15/25 RapTime: 0:02:35.391012 FixedProfit: 993830 TradeTimes: 7 TradeWin: 3\n",
            "Episode: 15/25 RapTime: 0:02:33.493960 FixedProfit: 976452 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:02:42.755936 FixedProfit: 1001372 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:02:37.452404 FixedProfit: 963358 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:02:35.911696 FixedProfit: 1003819 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:02:35.580745 FixedProfit: 976116 TradeTimes: 7 TradeWin: 4\n",
            "Episode: 16/25 RapTime: 0:02:37.377744 FixedProfit: 1021250 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 17/25 RapTime: 0:02:34.973875 FixedProfit: 994318 TradeTimes: 4 TradeWin: 0\n",
            "Episode: 17/25 RapTime: 0:02:39.317856 FixedProfit: 996992 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 17/25 RapTime: 0:02:37.168222 FixedProfit: 1002061 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 17/25 RapTime: 0:02:37.940100 FixedProfit: 1005200 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 18/25 RapTime: 0:02:37.575418 FixedProfit: 999219 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:02:36.712626 FixedProfit: 1008577 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 18/25 RapTime: 0:02:40.938626 FixedProfit: 1025194 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 18/25 RapTime: 0:02:36.772492 FixedProfit: 1024564 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 19/25 RapTime: 0:02:38.703664 FixedProfit: 1007997 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 19/25 RapTime: 0:02:39.156990 FixedProfit: 975834 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 19/25 RapTime: 0:02:36.478117 FixedProfit: 1009838 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:02:42.009943 FixedProfit: 1003972 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 20/25 RapTime: 0:02:37.979665 FixedProfit: 1002438 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 20/25 RapTime: 0:02:37.009632 FixedProfit: 1001015 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 20/25 RapTime: 0:02:40.273292 FixedProfit: 999999 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:02:40.054602 FixedProfit: 994350 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 21/25 RapTime: 0:02:37.734423 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 21/25 RapTime: 0:02:41.958146 FixedProfit: 969156 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 21/25 RapTime: 0:02:40.304533 FixedProfit: 1012265 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 21/25 RapTime: 0:02:41.683232 FixedProfit: 997177 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 22/25 RapTime: 0:02:40.346674 FixedProfit: 1000464 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:02:40.046557 FixedProfit: 1001297 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 22/25 RapTime: 0:02:40.993279 FixedProfit: 964962 TradeTimes: 6 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:02:45.328067 FixedProfit: 1007304 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 23/25 RapTime: 0:02:38.762937 FixedProfit: 973802 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:02:41.981247 FixedProfit: 1013386 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 23/25 RapTime: 0:02:39.961641 FixedProfit: 1028159 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 23/25 RapTime: 0:02:41.195463 FixedProfit: 991722 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 24/25 RapTime: 0:02:40.881886 FixedProfit: 995645 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 24/25 RapTime: 0:02:40.525866 FixedProfit: 1006865 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:02:44.784964 FixedProfit: 998115 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:02:47.367300 FixedProfit: 1054394 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 25/25 RapTime: 0:02:46.741840 FixedProfit: 993374 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 25/25 RapTime: 0:02:40.447660 FixedProfit: 998598 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 25/25 RapTime: 0:02:38.306931 FixedProfit: 1054782 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 25/25 RapTime: 0:02:15.656490 FixedProfit: 1009737 TradeTimes: 3 TradeWin: 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}