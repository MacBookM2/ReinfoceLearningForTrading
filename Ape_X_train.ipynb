{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ape-X_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNpToUaK3RxSeIrHuC3l4Pm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/Ape_X_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NIXg6mTzk0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67584f4e-054f-466f-8fb3-1c5937c01d74"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import Dense, ReLU, Input, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from statistics import mean\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "mode = 'train'\n",
        "name = 'apex'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNPHarYI9zGY"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        learning_rate = 0.00001\n",
        "        neurons_per_layer = 24\n",
        "\n",
        "        input = Input(shape=(3,))\n",
        "        common = Dense(neurons_per_layer*2, activation='relu')(input)\n",
        "        common = Dense(neurons_per_layer*4, activation='relu')(common)\n",
        "\n",
        "        common = Dense(4, activation='linear')(common)\n",
        "        output = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - 0.0*K.mean(a[:, 1:], keepdims=True),output_shape=(3,))(common)\n",
        "\n",
        "        model = keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "        optimizer = Adam(learning_rate=learning_rate, clipvalue=40)\n",
        "        model.compile(loss=Huber(), optimizer=optimizer)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvtDknjO2juu"
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, model1, model2):\n",
        "\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        self.gamma  = 0.99\n",
        "\n",
        "    def learn(self, m_batch, s_flag):\n",
        "\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "\n",
        "        next_act_values = self.model1.predict(next_states,s_flag)\n",
        "        next_action =np.argmax(next_act_values)\n",
        "\n",
        "        if s_flag == 11:\n",
        "            q = self.model1.predict(states)  \n",
        "            next_q = self.model2.predict(next_states)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, actions] = rewards + (1 - done) * self.gamma*np.max(next_q, axis=1)\n",
        "            self.model1.train_on_batch(states, target)\n",
        "        else:\n",
        "            q = self.model2.predict(states)  \n",
        "            next_q = self.model1.predict(next_states)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, actions] = rewards + (1 - done) * self.gamma*np.max(next_q, axis=1)\n",
        "            self.model2.train_on_batch(states, target)\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.model1.load_weights(name)\n",
        "        self.model2.load_weights(name2)\n",
        "\n",
        "    def save(self, name, name2):\n",
        "        self.model1.save_weights(name)\n",
        "        self.model2.save_weights(name2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZLHqi3CpnxI"
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.tderrors_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def findall(self):\n",
        "        return self.states_memory,self.next_states_memory,self.acts_memory,self.rewards_memory,self.done_memory,self.tderrors_memory\n",
        "\n",
        "    def update_memory_tderror(self, val):\n",
        "        self.tderrors_memory = val"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, model1, model2, memory, learner, epsilon, batch_size=32):\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        self.memory = memory\n",
        "        self.learner = learner\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state, s_flag=12):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self._predict(state,s_flag)\n",
        "        return np.argmax(act_values)\n",
        "\n",
        "    def _predict(self, state, s_flag = 12):\n",
        "        values = None\n",
        "        q1 = self.model1.predict(state)\n",
        "        q2 = self.model2.predict(state)\n",
        "        if s_flag == 12:\n",
        "            values = q1 + q2\n",
        "        elif s_flag == 11:\n",
        "            values = q1 + q1\n",
        "        else:\n",
        "            values = q2 + q2\n",
        "        return values\n",
        "\n",
        "    def replay(self):\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        return m_batch\n",
        "\n",
        "    def pioritized_experience_replay(self):\n",
        "\n",
        "        sum_ab_tderror = self._absolute_tderror()\n",
        "        td_list = np.random.uniform(0, sum_ab_tderror, self.batch_size)\n",
        "        td_list = np.sort(td_list)\n",
        "\n",
        "        num_np = np.array([], dtype=np.int16)\n",
        "        i, sum_ab_tderror_tmp = 0, 0\n",
        "        states, next_states, actions, rewards, done, tderror = self.memory.findall()\n",
        "        for item in td_list:\n",
        "            while sum_ab_tderror_tmp < item:\n",
        "                sum_ab_tderror_tmp += abs(tderror[i]) + 0.0001\n",
        "                i += 1\n",
        "            num_np = np.append(num_np, i)\n",
        "\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [states[num_np],next_states[num_np],\n",
        "                 actions[num_np],rewards[num_np],\n",
        "                 done[num_np]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def tderror(self):\n",
        "        states, next_states, acts, rewards, done, tderrors = self.memory.findall()\n",
        "        \n",
        "        n2_rewards = rewards[2:-1]\n",
        "        next_rewards = rewards[1:-2]\n",
        "        n3_action = acts[3:]\n",
        "        n3_next_states = next_states[3:]\n",
        "\n",
        "        states = states[:-3]\n",
        "        next_states = next_states[:-3]\n",
        "        acts = acts[:-3]\n",
        "        rewards = rewards[:-3]\n",
        "        done = done[:-3]\n",
        "\n",
        "        #self.memory.cntr = (self.memory.cntr - 3) if (self.memory.cntr - 3) > 0 else (self.memory.cntr + max_size - 4)\n",
        "        #self.memory.size = (self.memory.size - 3) if (self.memory.size - 3) > 0 else (self.memory.size + max_size - 4)\n",
        "\n",
        "        self.memory.cntr = self.memory.cntr - 3\n",
        "        self.memory.size = self.memory.size - 3\n",
        "\n",
        "        next_action = np.argmax(self._predict(n3_next_states), axis = 1)\n",
        "        next_action = next_action.reshape(-1)\n",
        "\n",
        "        acts = acts.reshape(-1)\n",
        "\n",
        "        acts_one = np.eye(3)[acts]\n",
        "        next_action_one = np.eye(3)[next_action]\n",
        "        n3_q = np.max(self._predict(n3_next_states) * next_action_one, axis = 1)\n",
        "\n",
        "        target = rewards + self.gamma * next_rewards + (self.gamma ** 2) * n2_rewards + (self.gamma ** 3) * n3_q\n",
        "        tderror = target - np.max(self._predict(states) * acts_one, axis = 1)\n",
        "        self.memory.update_memory_tderror(tderror)\n",
        "\n",
        "    def _absolute_tderror(self):\n",
        "        absolute_tderror = 0\n",
        "        tderror = self.memory.tderrors_memory\n",
        "        for i in range(0, (len(tderror)-1)):\n",
        "            absolute_tderror += abs(tderror[i]) + 0.0001\n",
        "        return absolute_tderror\n",
        "\n",
        "    def integration(self):\n",
        "        self.model1 = clone_model(self.learner.model1)\n",
        "        self.model2 = clone_model(self.learner.model2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, learner, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.learner        = learner\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        #total_reward = [0]\n",
        "        total_reward = [1000000]\n",
        "        self.agent.integration()\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                s_flag = 12\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "              \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                self.agent.tderror()\n",
        "                if mean(total_reward) < 1050000:\n",
        "                    m_batch = self.agent.replay()\n",
        "                else:\n",
        "                    m_batch = self.agent.pioritized_experience_replay()\n",
        "                s_flag = 11 if np.random.random() <= 0.5 else 22\n",
        "                learner.learn(m_batch, s_flag)\n",
        " \n",
        "                self.agent.tderror()\n",
        "                total_reward.append(info['cur_revenue'])\n",
        "                agent.integration()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.learner.load('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        self.learner.save('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYFNVDDQz9X9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf73179-316e-4842-d986-5f5cb64cfefa"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 200\n",
        "batch_size = 128\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "\n",
        "brain1 = Brain()\n",
        "model1 = brain1.model\n",
        "brain2 = Brain()\n",
        "model2 = brain2.model\n",
        "\n",
        "learner = Learner(model1, model2)\n",
        "memory = Memory(max_size, batch_size)\n",
        "epsilon_list = [0.4]\n",
        "\n",
        "\n",
        "epsilon = epsilon_list[0]\n",
        "model1_cp = clone_model(model1)\n",
        "model2_cp = clone_model(model2)\n",
        "agent = Agent(model1_cp, model2_cp, memory, learner, epsilon, batch_size)\n",
        "main = Main(env, agent, learner, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 48)                192       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 96)                4704      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 388       \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 5,284\n",
            "Trainable params: 5,284\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 48)                192       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 96)                4704      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 388       \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 5,284\n",
            "Trainable params: 5,284\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/200 RapTime: 0:00:31.068014 FixedProfit: 1139307\n",
            "Episode: 2/200 RapTime: 0:00:31.214809 FixedProfit: 1195480\n",
            "Episode: 3/200 RapTime: 0:00:29.964718 FixedProfit: 1040639\n",
            "Episode: 4/200 RapTime: 0:00:32.250078 FixedProfit: 1053674\n",
            "Episode: 5/200 RapTime: 0:00:29.504582 FixedProfit: 1015898\n",
            "Episode: 6/200 RapTime: 0:00:31.074958 FixedProfit: 954399\n",
            "Episode: 7/200 RapTime: 0:00:33.579435 FixedProfit: 1089121\n",
            "Episode: 8/200 RapTime: 0:00:32.545267 FixedProfit: 1003444\n",
            "Episode: 9/200 RapTime: 0:00:30.343791 FixedProfit: 931047\n",
            "Episode: 10/200 RapTime: 0:00:29.708227 FixedProfit: 1246184\n",
            "Episode: 11/200 RapTime: 0:00:31.284695 FixedProfit: 860154\n",
            "Episode: 12/200 RapTime: 0:00:32.296535 FixedProfit: 1186426\n",
            "Episode: 13/200 RapTime: 0:00:31.747539 FixedProfit: 1095381\n",
            "Episode: 14/200 RapTime: 0:00:29.268812 FixedProfit: 1093226\n",
            "Episode: 15/200 RapTime: 0:00:31.885501 FixedProfit: 918508\n",
            "Episode: 16/200 RapTime: 0:00:30.819740 FixedProfit: 1158593\n",
            "Episode: 17/200 RapTime: 0:00:30.320933 FixedProfit: 1153700\n",
            "Episode: 18/200 RapTime: 0:00:31.756790 FixedProfit: 1159325\n",
            "Episode: 19/200 RapTime: 0:00:32.436415 FixedProfit: 1147530\n",
            "Episode: 20/200 RapTime: 0:00:30.754154 FixedProfit: 1292147\n",
            "Episode: 21/200 RapTime: 0:00:30.197104 FixedProfit: 1096078\n",
            "Episode: 22/200 RapTime: 0:00:32.396796 FixedProfit: 1029153\n",
            "Episode: 23/200 RapTime: 0:00:30.235214 FixedProfit: 1279056\n",
            "Episode: 24/200 RapTime: 0:00:30.508270 FixedProfit: 1303148\n",
            "Episode: 25/200 RapTime: 0:00:29.374929 FixedProfit: 985831\n",
            "Episode: 26/200 RapTime: 0:00:31.782529 FixedProfit: 1136792\n",
            "Episode: 27/200 RapTime: 0:00:31.231674 FixedProfit: 1039718\n",
            "Episode: 28/200 RapTime: 0:00:33.499973 FixedProfit: 1066638\n",
            "Episode: 29/200 RapTime: 0:00:30.094280 FixedProfit: 1197088\n",
            "Episode: 30/200 RapTime: 0:00:30.944533 FixedProfit: 1090564\n",
            "Episode: 31/200 RapTime: 0:00:30.861133 FixedProfit: 1021207\n",
            "Episode: 32/200 RapTime: 0:00:31.172314 FixedProfit: 1314155\n",
            "Episode: 33/200 RapTime: 0:00:30.178569 FixedProfit: 1068677\n",
            "Episode: 34/200 RapTime: 0:00:30.217376 FixedProfit: 917541\n",
            "Episode: 35/200 RapTime: 0:00:32.132668 FixedProfit: 1018215\n",
            "Episode: 36/200 RapTime: 0:00:30.452337 FixedProfit: 1201596\n",
            "Episode: 37/200 RapTime: 0:00:29.972878 FixedProfit: 1049792\n",
            "Episode: 38/200 RapTime: 0:00:32.048704 FixedProfit: 1001502\n",
            "Episode: 39/200 RapTime: 0:00:30.818254 FixedProfit: 1322314\n",
            "Episode: 40/200 RapTime: 0:00:30.033241 FixedProfit: 1187980\n",
            "Episode: 41/200 RapTime: 0:00:31.204028 FixedProfit: 934950\n",
            "Episode: 42/200 RapTime: 0:00:31.203333 FixedProfit: 971899\n",
            "Episode: 43/200 RapTime: 0:00:30.840982 FixedProfit: 954770\n",
            "Episode: 44/200 RapTime: 0:00:32.150732 FixedProfit: 1012375\n",
            "Episode: 45/200 RapTime: 0:00:31.922638 FixedProfit: 980748\n",
            "Episode: 46/200 RapTime: 0:00:29.816185 FixedProfit: 1329763\n",
            "Episode: 47/200 RapTime: 0:00:32.395640 FixedProfit: 1070446\n",
            "Episode: 48/200 RapTime: 0:00:30.645216 FixedProfit: 1010684\n",
            "Episode: 49/200 RapTime: 0:00:30.013516 FixedProfit: 944862\n",
            "Episode: 50/200 RapTime: 0:00:28.995539 FixedProfit: 1046589\n",
            "Episode: 51/200 RapTime: 0:00:30.277912 FixedProfit: 1029276\n",
            "Episode: 52/200 RapTime: 0:00:29.668511 FixedProfit: 1064678\n",
            "Episode: 53/200 RapTime: 0:00:29.041270 FixedProfit: 1155085\n",
            "Episode: 54/200 RapTime: 0:00:32.102111 FixedProfit: 1031342\n",
            "Episode: 55/200 RapTime: 0:00:30.885566 FixedProfit: 1066962\n",
            "Episode: 56/200 RapTime: 0:00:30.867626 FixedProfit: 1069519\n",
            "Episode: 57/200 RapTime: 0:00:30.025243 FixedProfit: 992016\n",
            "Episode: 58/200 RapTime: 0:00:31.019618 FixedProfit: 1172622\n",
            "Episode: 59/200 RapTime: 0:00:32.357769 FixedProfit: 1141023\n",
            "Episode: 60/200 RapTime: 0:00:30.504575 FixedProfit: 1002264\n",
            "Episode: 61/200 RapTime: 0:00:30.147833 FixedProfit: 1183924\n",
            "Episode: 62/200 RapTime: 0:00:30.751231 FixedProfit: 994122\n",
            "Episode: 63/200 RapTime: 0:00:30.500758 FixedProfit: 1212592\n",
            "Episode: 64/200 RapTime: 0:00:32.205365 FixedProfit: 1075743\n",
            "Episode: 65/200 RapTime: 0:00:30.271977 FixedProfit: 946048\n",
            "Episode: 66/200 RapTime: 0:00:30.634557 FixedProfit: 1175711\n",
            "Episode: 67/200 RapTime: 0:00:30.805116 FixedProfit: 1093533\n",
            "Episode: 68/200 RapTime: 0:00:30.395457 FixedProfit: 960838\n",
            "Episode: 69/200 RapTime: 0:00:29.668024 FixedProfit: 976319\n",
            "Episode: 70/200 RapTime: 0:00:30.642423 FixedProfit: 996771\n",
            "Episode: 71/200 RapTime: 0:00:30.923191 FixedProfit: 895463\n",
            "Episode: 72/200 RapTime: 0:00:31.774835 FixedProfit: 1023226\n",
            "Episode: 73/200 RapTime: 0:00:31.002194 FixedProfit: 1047471\n",
            "Episode: 74/200 RapTime: 0:00:33.030621 FixedProfit: 1108098\n",
            "Episode: 75/200 RapTime: 0:00:28.998665 FixedProfit: 1089975\n",
            "Episode: 76/200 RapTime: 0:00:31.430491 FixedProfit: 1132070\n",
            "Episode: 77/200 RapTime: 0:00:30.778766 FixedProfit: 1185346\n",
            "Episode: 78/200 RapTime: 0:00:32.680304 FixedProfit: 1076560\n",
            "Episode: 79/200 RapTime: 0:00:31.058881 FixedProfit: 1149687\n",
            "Episode: 80/200 RapTime: 0:00:30.306782 FixedProfit: 1029220\n",
            "Episode: 81/200 RapTime: 0:00:31.263155 FixedProfit: 1025379\n",
            "Episode: 82/200 RapTime: 0:00:30.871688 FixedProfit: 1003422\n",
            "Episode: 83/200 RapTime: 0:00:31.058684 FixedProfit: 973583\n",
            "Episode: 84/200 RapTime: 0:00:29.399795 FixedProfit: 1097413\n",
            "Episode: 85/200 RapTime: 0:00:31.174472 FixedProfit: 1081644\n",
            "Episode: 86/200 RapTime: 0:00:32.343008 FixedProfit: 1056147\n",
            "Episode: 87/200 RapTime: 0:00:31.928271 FixedProfit: 1128937\n",
            "Episode: 88/200 RapTime: 0:00:31.808746 FixedProfit: 1140728\n",
            "Episode: 89/200 RapTime: 0:00:32.086807 FixedProfit: 1041694\n",
            "Episode: 90/200 RapTime: 0:00:31.939255 FixedProfit: 1157171\n",
            "Episode: 91/200 RapTime: 0:00:32.280489 FixedProfit: 1014520\n",
            "Episode: 92/200 RapTime: 0:00:31.966317 FixedProfit: 1125547\n",
            "Episode: 93/200 RapTime: 0:00:31.126924 FixedProfit: 1123094\n",
            "Episode: 94/200 RapTime: 0:00:34.950332 FixedProfit: 966517\n",
            "Episode: 95/200 RapTime: 0:00:30.201167 FixedProfit: 1076858\n",
            "Episode: 96/200 RapTime: 0:00:30.290190 FixedProfit: 1032270\n",
            "Episode: 97/200 RapTime: 0:00:31.944459 FixedProfit: 1118525\n",
            "Episode: 98/200 RapTime: 0:00:31.329756 FixedProfit: 1162261\n",
            "Episode: 99/200 RapTime: 0:00:32.036786 FixedProfit: 1131393\n",
            "Episode: 100/200 RapTime: 0:00:33.942560 FixedProfit: 1039678\n",
            "Episode: 101/200 RapTime: 0:00:34.500315 FixedProfit: 1013493\n",
            "Episode: 102/200 RapTime: 0:00:31.287933 FixedProfit: 1164775\n",
            "Episode: 103/200 RapTime: 0:00:30.902614 FixedProfit: 1042118\n",
            "Episode: 104/200 RapTime: 0:00:29.609566 FixedProfit: 983893\n",
            "Episode: 105/200 RapTime: 0:00:29.098358 FixedProfit: 944758\n",
            "Episode: 106/200 RapTime: 0:00:30.865936 FixedProfit: 1212265\n",
            "Episode: 107/200 RapTime: 0:00:30.047481 FixedProfit: 1043910\n",
            "Episode: 108/200 RapTime: 0:00:30.770627 FixedProfit: 1049943\n",
            "Episode: 109/200 RapTime: 0:00:31.266009 FixedProfit: 1054634\n",
            "Episode: 110/200 RapTime: 0:00:30.938031 FixedProfit: 1063214\n",
            "Episode: 111/200 RapTime: 0:00:34.314128 FixedProfit: 1129631\n",
            "Episode: 112/200 RapTime: 0:00:31.315856 FixedProfit: 1098864\n",
            "Episode: 113/200 RapTime: 0:00:29.784306 FixedProfit: 926046\n",
            "Episode: 114/200 RapTime: 0:00:29.894084 FixedProfit: 1027910\n",
            "Episode: 115/200 RapTime: 0:00:30.845238 FixedProfit: 1114657\n",
            "Episode: 116/200 RapTime: 0:00:30.099198 FixedProfit: 1273835\n",
            "Episode: 117/200 RapTime: 0:00:31.739277 FixedProfit: 1095399\n",
            "Episode: 118/200 RapTime: 0:00:30.569929 FixedProfit: 1247210\n",
            "Episode: 119/200 RapTime: 0:00:32.380256 FixedProfit: 1046171\n",
            "Episode: 120/200 RapTime: 0:00:31.564759 FixedProfit: 1237468\n",
            "Episode: 121/200 RapTime: 0:00:31.913109 FixedProfit: 921450\n",
            "Episode: 122/200 RapTime: 0:00:30.504404 FixedProfit: 1170060\n",
            "Episode: 123/200 RapTime: 0:00:29.609048 FixedProfit: 1279799\n",
            "Episode: 124/200 RapTime: 0:00:31.864068 FixedProfit: 1064792\n",
            "Episode: 125/200 RapTime: 0:00:30.592825 FixedProfit: 1298267\n",
            "Episode: 126/200 RapTime: 0:00:30.996759 FixedProfit: 1113375\n",
            "Episode: 127/200 RapTime: 0:00:30.805075 FixedProfit: 998932\n",
            "Episode: 128/200 RapTime: 0:00:31.145418 FixedProfit: 1121844\n",
            "Episode: 129/200 RapTime: 0:00:29.020668 FixedProfit: 1289693\n",
            "Episode: 130/200 RapTime: 0:00:30.610685 FixedProfit: 1109185\n",
            "Episode: 131/200 RapTime: 0:00:31.145925 FixedProfit: 1118764\n",
            "Episode: 132/200 RapTime: 0:00:30.271787 FixedProfit: 1245066\n",
            "Episode: 133/200 RapTime: 0:00:33.678122 FixedProfit: 1098876\n",
            "Episode: 134/200 RapTime: 0:00:31.365440 FixedProfit: 996144\n",
            "Episode: 135/200 RapTime: 0:00:29.579407 FixedProfit: 1185544\n",
            "Episode: 136/200 RapTime: 0:00:29.635448 FixedProfit: 968807\n",
            "Episode: 137/200 RapTime: 0:00:30.283314 FixedProfit: 886061\n",
            "Episode: 138/200 RapTime: 0:00:31.112856 FixedProfit: 1060200\n",
            "Episode: 139/200 RapTime: 0:00:30.166273 FixedProfit: 995514\n",
            "Episode: 140/200 RapTime: 0:00:31.242847 FixedProfit: 1107107\n",
            "Episode: 141/200 RapTime: 0:00:30.573538 FixedProfit: 1248732\n",
            "Episode: 142/200 RapTime: 0:00:31.320024 FixedProfit: 1173074\n",
            "Episode: 143/200 RapTime: 0:00:31.172246 FixedProfit: 1340724\n",
            "Episode: 144/200 RapTime: 0:00:31.070335 FixedProfit: 1334050\n",
            "Episode: 145/200 RapTime: 0:00:31.931934 FixedProfit: 1154864\n",
            "Episode: 146/200 RapTime: 0:00:30.140078 FixedProfit: 1159590\n",
            "Episode: 147/200 RapTime: 0:00:29.463096 FixedProfit: 995383\n",
            "Episode: 148/200 RapTime: 0:00:31.688397 FixedProfit: 1053668\n",
            "Episode: 149/200 RapTime: 0:00:31.927163 FixedProfit: 1188111\n",
            "Episode: 150/200 RapTime: 0:00:30.034730 FixedProfit: 1035271\n",
            "Episode: 151/200 RapTime: 0:00:31.515754 FixedProfit: 1103487\n",
            "Episode: 152/200 RapTime: 0:00:31.108901 FixedProfit: 1088116\n",
            "Episode: 153/200 RapTime: 0:00:29.688477 FixedProfit: 1248977\n",
            "Episode: 154/200 RapTime: 0:00:31.029713 FixedProfit: 1157686\n",
            "Episode: 155/200 RapTime: 0:00:30.379831 FixedProfit: 1055687\n",
            "Episode: 156/200 RapTime: 0:00:29.865117 FixedProfit: 1141729\n",
            "Episode: 157/200 RapTime: 0:00:29.414370 FixedProfit: 975688\n",
            "Episode: 158/200 RapTime: 0:00:30.775326 FixedProfit: 952894\n",
            "Episode: 159/200 RapTime: 0:00:30.528577 FixedProfit: 1151582\n",
            "Episode: 160/200 RapTime: 0:00:31.829224 FixedProfit: 1280747\n",
            "Episode: 161/200 RapTime: 0:00:31.239896 FixedProfit: 1064475\n",
            "Episode: 162/200 RapTime: 0:00:28.950935 FixedProfit: 874919\n",
            "Episode: 163/200 RapTime: 0:00:30.056399 FixedProfit: 1044330\n",
            "Episode: 164/200 RapTime: 0:00:30.685168 FixedProfit: 904011\n",
            "Episode: 165/200 RapTime: 0:00:31.399679 FixedProfit: 1040929\n",
            "Episode: 166/200 RapTime: 0:00:30.453275 FixedProfit: 1072053\n",
            "Episode: 167/200 RapTime: 0:00:29.197153 FixedProfit: 918265\n",
            "Episode: 168/200 RapTime: 0:00:28.651781 FixedProfit: 1054812\n",
            "Episode: 169/200 RapTime: 0:00:31.106081 FixedProfit: 1185631\n",
            "Episode: 170/200 RapTime: 0:00:30.219031 FixedProfit: 1261160\n",
            "Episode: 171/200 RapTime: 0:00:31.229615 FixedProfit: 1260997\n",
            "Episode: 172/200 RapTime: 0:00:30.632729 FixedProfit: 1267738\n",
            "Episode: 173/200 RapTime: 0:00:29.390186 FixedProfit: 1122819\n",
            "Episode: 174/200 RapTime: 0:00:31.674564 FixedProfit: 1178172\n",
            "Episode: 175/200 RapTime: 0:00:30.250252 FixedProfit: 1108566\n",
            "Episode: 176/200 RapTime: 0:00:30.469582 FixedProfit: 1212048\n",
            "Episode: 177/200 RapTime: 0:00:31.867047 FixedProfit: 1059354\n",
            "Episode: 178/200 RapTime: 0:00:30.739407 FixedProfit: 1244870\n",
            "Episode: 179/200 RapTime: 0:00:30.314476 FixedProfit: 1228925\n",
            "Episode: 180/200 RapTime: 0:00:29.723226 FixedProfit: 1105549\n",
            "Episode: 181/200 RapTime: 0:00:32.079349 FixedProfit: 1251725\n",
            "Episode: 182/200 RapTime: 0:00:29.987340 FixedProfit: 1004206\n",
            "Episode: 183/200 RapTime: 0:00:31.034343 FixedProfit: 963934\n",
            "Episode: 184/200 RapTime: 0:00:32.442438 FixedProfit: 1067092\n",
            "Episode: 185/200 RapTime: 0:00:29.654808 FixedProfit: 1137522\n",
            "Episode: 186/200 RapTime: 0:00:31.258517 FixedProfit: 1034307\n",
            "Episode: 187/200 RapTime: 0:00:28.967445 FixedProfit: 1079588\n",
            "Episode: 188/200 RapTime: 0:00:32.701813 FixedProfit: 1087702\n",
            "Episode: 189/200 RapTime: 0:00:31.624724 FixedProfit: 1005861\n",
            "Episode: 190/200 RapTime: 0:00:30.704360 FixedProfit: 1244113\n",
            "Episode: 191/200 RapTime: 0:00:31.646923 FixedProfit: 1123248\n",
            "Episode: 192/200 RapTime: 0:00:31.110266 FixedProfit: 982596\n",
            "Episode: 193/200 RapTime: 0:00:30.841216 FixedProfit: 1126056\n",
            "Episode: 194/200 RapTime: 0:00:32.158536 FixedProfit: 1160377\n",
            "Episode: 195/200 RapTime: 0:00:30.519976 FixedProfit: 1094514\n",
            "Episode: 196/200 RapTime: 0:00:30.774642 FixedProfit: 943061\n",
            "Episode: 197/200 RapTime: 0:00:30.526676 FixedProfit: 1067741\n",
            "Episode: 198/200 RapTime: 0:00:30.834029 FixedProfit: 1121904\n",
            "Episode: 199/200 RapTime: 0:00:31.138386 FixedProfit: 932874\n",
            "Episode: 200/200 RapTime: 0:00:30.302910 FixedProfit: 1175161\n"
          ]
        }
      ]
    }
  ]
}