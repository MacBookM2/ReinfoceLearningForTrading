{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a3c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "02ae85fd-6e46-4605-9eca-398404cae70d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'a3c_train.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m51Mu4xy9-Nj"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class MasterBrain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model(input_, [actor, critic])\n",
        "        mastermodel.compile(optimizer=Adam(lr=lr))\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "def play_game(env, actor, critic, scaler, episodes_times = 25, batch_size = 32, mode = 'train'):\n",
        "\n",
        "    experiences = []\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        state = state.flatten()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        while not done:\n",
        "            action = actor.policynetwork(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            next_state = next_state.flatten()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if mode == 'train':\n",
        "                experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                if len(experiences) == batch_size:\n",
        "                    critic.valuenetwork(experiences)\n",
        "                    experiences = []\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        actor.integration()\n",
        "        actor.layering()\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8b7ab2-10ee-4ebe-c385-b3a53fb2f158"
      },
      "source": [
        "initial_money=1000000\n",
        "mode = 'train'\n",
        "episodes_times = 25\n",
        "batch_size = 32\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "if mode == 'test':\n",
        "    masterbrain.load(f'{models_folder}/a3c_model.h5')\n",
        "\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "        print(row, file=f)\n",
        "else:\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit'\n",
        "        print(row, file=f)\n",
        "\n",
        "thred_num = 4\n",
        "envs = []\n",
        "for i in range(thred_num):\n",
        "    e = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    a = Actor(brain)\n",
        "    c = Critic(model)\n",
        "    s = make_scaler(e)\n",
        "    arr = [e,a,c,s]\n",
        "    envs.append(arr)\n",
        "\n",
        "datas = []\n",
        "with PoolExecutor(max_workers=thred_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: play_game(env[0],env[1],env[2],env[3], episodes_times, batch_size, mode)\n",
        "        datas.append(executor.submit(job))\n",
        "\n",
        "if mode == 'train':\n",
        "    masterbrain.save(f'{models_folder}/a3c_model.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/25 RapTime: 0:00:05.944564 FixedProfit: 920507\n",
            "Episode: 1/25 RapTime: 0:00:05.947695 FixedProfit: 1014908\n",
            "Episode: 1/25 RapTime: 0:00:05.949149 FixedProfit: 1119375\n",
            "Episode: 1/25 RapTime: 0:00:05.984549 FixedProfit: 1150987\n",
            "Episode: 2/25 RapTime: 0:00:05.708464 FixedProfit: 913900\n",
            "Episode: 2/25 RapTime: 0:00:05.790449 FixedProfit: 1255326\n",
            "Episode: 2/25 RapTime: 0:00:05.800867 FixedProfit: 1088652\n",
            "Episode: 2/25 RapTime: 0:00:05.796575 FixedProfit: 906786\n",
            "Episode: 3/25 RapTime: 0:00:05.975377 FixedProfit: 1233275\n",
            "Episode: 3/25 RapTime: 0:00:06.028372 FixedProfit: 1197454\n",
            "Episode: 3/25 RapTime: 0:00:05.996131 FixedProfit: 967646\n",
            "Episode: 3/25 RapTime: 0:00:05.998892 FixedProfit: 1291738\n",
            "Episode: 4/25 RapTime: 0:00:06.177542 FixedProfit: 1044813\n",
            "Episode: 4/25 RapTime: 0:00:06.171745 FixedProfit: 1324045\n",
            "Episode: 4/25 RapTime: 0:00:06.212650 FixedProfit: 1036719\n",
            "Episode: 4/25 RapTime: 0:00:06.247149 FixedProfit: 1076366\n",
            "Episode: 5/25 RapTime: 0:00:06.250989 FixedProfit: 1131100Episode: 5/25 RapTime: 0:00:06.249200 FixedProfit: 1117611\n",
            "\n",
            "Episode: 5/25 RapTime: 0:00:06.201590 FixedProfit: 1056414\n",
            "Episode: 5/25 RapTime: 0:00:06.248483 FixedProfit: 1011481\n",
            "Episode: 6/25 RapTime: 0:00:06.013617 FixedProfit: 1228567\n",
            "Episode: 6/25 RapTime: 0:00:06.035900 FixedProfit: 959730\n",
            "Episode: 6/25 RapTime: 0:00:06.074569 FixedProfit: 1097906\n",
            "Episode: 6/25 RapTime: 0:00:06.046113 FixedProfit: 958542\n",
            "Episode: 7/25 RapTime: 0:00:05.829070 FixedProfit: 1194143\n",
            "Episode: 7/25 RapTime: 0:00:05.811315 FixedProfit: 1075430\n",
            "Episode: 7/25 RapTime: 0:00:05.821291 FixedProfit: 1063656\n",
            "Episode: 7/25 RapTime: 0:00:05.888542 FixedProfit: 1134900\n",
            "Episode: 8/25 RapTime: 0:00:05.591267 FixedProfit: 1197165\n",
            "Episode: 8/25 RapTime: 0:00:05.597102 FixedProfit: 1123664\n",
            "Episode: 8/25 RapTime: 0:00:05.590505 FixedProfit: 1019924\n",
            "Episode: 8/25 RapTime: 0:00:05.599907 FixedProfit: 1204616\n",
            "Episode: 9/25 RapTime: 0:00:05.772614 FixedProfit: 1197165\n",
            "Episode: 9/25 RapTime: 0:00:05.736780 FixedProfit: 1009654\n",
            "Episode: 9/25 RapTime: 0:00:05.759417 FixedProfit: 1118991\n",
            "Episode: 9/25 RapTime: 0:00:05.810003 FixedProfit: 1245898\n",
            "Episode: 10/25 RapTime: 0:00:05.645933 FixedProfit: 1185967\n",
            "Episode: 10/25 RapTime: 0:00:05.694855 FixedProfit: 1197165\n",
            "Episode: 10/25 RapTime: 0:00:05.656786 FixedProfit: 1076192\n",
            "Episode: 10/25 RapTime: 0:00:05.657851 FixedProfit: 910852\n",
            "Episode: 11/25 RapTime: 0:00:05.631774 FixedProfit: 1040273\n",
            "Episode: 11/25 RapTime: 0:00:05.679165 FixedProfit: 1197165\n",
            "Episode: 11/25 RapTime: 0:00:05.694901 FixedProfit: 1092884\n",
            "Episode: 11/25 RapTime: 0:00:05.679323 FixedProfit: 1122375\n",
            "Episode: 12/25 RapTime: 0:00:05.631703 FixedProfit: 936013\n",
            "Episode: 12/25 RapTime: 0:00:05.569191 FixedProfit: 992554\n",
            "Episode: 12/25 RapTime: 0:00:05.604439 FixedProfit: 1197165\n",
            "Episode: 12/25 RapTime: 0:00:05.603068 FixedProfit: 1157135\n",
            "Episode: 13/25 RapTime: 0:00:05.494359 FixedProfit: 1232324\n",
            "Episode: 13/25 RapTime: 0:00:05.529203 FixedProfit: 1125308\n",
            "Episode: 13/25 RapTime: 0:00:05.542635 FixedProfit: 1211840\n",
            "Episode: 13/25 RapTime: 0:00:05.542836 FixedProfit: 996525\n",
            "Episode: 14/25 RapTime: 0:00:05.916057 FixedProfit: 1184713\n",
            "Episode: 14/25 RapTime: 0:00:05.963964 FixedProfit: 1113539\n",
            "Episode: 14/25 RapTime: 0:00:05.977876 FixedProfit: 1133212\n",
            "Episode: 14/25 RapTime: 0:00:05.944683 FixedProfit: 1160708\n",
            "Episode: 15/25 RapTime: 0:00:05.920139 FixedProfit: 1203943\n",
            "Episode: 15/25 RapTime: 0:00:05.910532 FixedProfit: 1133966\n",
            "Episode: 15/25 RapTime: 0:00:05.938510 FixedProfit: 1208128\n",
            "Episode: 15/25 RapTime: 0:00:05.925708 FixedProfit: 1035835\n",
            "Episode: 16/25 RapTime: 0:00:06.009449 FixedProfit: 1228520\n",
            "Episode: 16/25 RapTime: 0:00:05.986889 FixedProfit: 1174425\n",
            "Episode: 16/25 RapTime: 0:00:05.980393 FixedProfit: 1197165\n",
            "Episode: 16/25 RapTime: 0:00:05.994229 FixedProfit: 1282455\n",
            "Episode: 17/25 RapTime: 0:00:05.992813 FixedProfit: 1235366\n",
            "Episode: 17/25 RapTime: 0:00:06.043737 FixedProfit: 1183168\n",
            "Episode: 17/25 RapTime: 0:00:05.954845 FixedProfit: 1197165\n",
            "Episode: 17/25 RapTime: 0:00:06.041956 FixedProfit: 953824\n",
            "Episode: 18/25 RapTime: 0:00:05.700677 FixedProfit: 1207089\n",
            "Episode: 18/25 RapTime: 0:00:05.678789 FixedProfit: 1197165\n",
            "Episode: 18/25 RapTime: 0:00:05.741549 FixedProfit: 1172071\n",
            "Episode: 18/25 RapTime: 0:00:05.703806 FixedProfit: 891625\n",
            "Episode: 19/25 RapTime: 0:00:05.659735 FixedProfit: 1185345\n",
            "Episode: 19/25 RapTime: 0:00:05.694791 FixedProfit: 1197165\n",
            "Episode: 19/25 RapTime: 0:00:05.694988 FixedProfit: 1228721\n",
            "Episode: 19/25 RapTime: 0:00:05.714973 FixedProfit: 1093262\n",
            "Episode: 20/25 RapTime: 0:00:05.661824 FixedProfit: 1247148\n",
            "Episode: 20/25 RapTime: 0:00:05.665630 FixedProfit: 1197165\n",
            "Episode: 20/25 RapTime: 0:00:05.660682 FixedProfit: 1087830\n",
            "Episode: 20/25 RapTime: 0:00:05.658376 FixedProfit: 1270794\n",
            "Episode: 21/25 RapTime: 0:00:05.717221 FixedProfit: 1187868\n",
            "Episode: 21/25 RapTime: 0:00:05.698285 FixedProfit: 1197165\n",
            "Episode: 21/25 RapTime: 0:00:05.702760 FixedProfit: 1168063\n",
            "Episode: 21/25 RapTime: 0:00:05.710517 FixedProfit: 994205\n",
            "Episode: 22/25 RapTime: 0:00:05.612020 FixedProfit: 1102424\n",
            "Episode: 22/25 RapTime: 0:00:05.586060 FixedProfit: 1197165\n",
            "Episode: 22/25 RapTime: 0:00:05.641290 FixedProfit: 1201265\n",
            "Episode: 22/25 RapTime: 0:00:05.648395 FixedProfit: 1016468\n",
            "Episode: 23/25 RapTime: 0:00:05.528543 FixedProfit: 1197165\n",
            "Episode: 23/25 RapTime: 0:00:05.556746 FixedProfit: 1304511\n",
            "Episode: 23/25 RapTime: 0:00:05.593504 FixedProfit: 1231954\n",
            "Episode: 23/25 RapTime: 0:00:05.563225 FixedProfit: 1277847\n",
            "Episode: 24/25 RapTime: 0:00:05.593591 FixedProfit: 1197165\n",
            "Episode: 24/25 RapTime: 0:00:05.618995 FixedProfit: 1115323\n",
            "Episode: 24/25 RapTime: 0:00:05.636805 FixedProfit: 1242356\n",
            "Episode: 24/25 RapTime: 0:00:05.630118 FixedProfit: 1064976\n",
            "Episode: 25/25 RapTime: 0:00:05.678425 FixedProfit: 1197165\n",
            "Episode: 25/25 RapTime: 0:00:05.674942 FixedProfit: 1234085\n",
            "Episode: 25/25 RapTime: 0:00:05.587843 FixedProfit: 1152726\n",
            "Episode: 25/25 RapTime: 0:00:05.489018 FixedProfit: 918233\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}