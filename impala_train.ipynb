{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "impala_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/impala_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "50db5215-7888-4782-c6e9-3c30779dd2d2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPool1D, Activation, concatenate\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'train'\n",
        "name = 'impala'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m48th46c8otj"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.n_action = 3\n",
        "        self.gamma = 0.9\n",
        "        self.alfa = 0.5\n",
        "        self.beta = 0.00025\n",
        "\n",
        "    def valuenetwork(self, state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            actions = tf.one_hot(action, self.n_action) # (10, 3)\n",
        "\n",
        "            state = state.reshape(1,10,3)\n",
        "            prev_action = prev_action.reshape(1,10,1)\n",
        "            prev_reward = prev_reward.reshape(1,10,1)\n",
        "\n",
        "            pai, v_theta  = self.model([state, prev_action, prev_reward])\n",
        "\n",
        "            pai = tf.reshape(pai, [10,3]) # (10, 3)\n",
        "\n",
        "            actions = tf.cast(actions, tf.float32)\n",
        "\n",
        "            pais = tf.reduce_sum(actions * pai, axis=1, keepdims=True)\n",
        "\n",
        "            mu = self._reshape_and_cast(mu, 3)\n",
        "            ratio = tf.math.divide_no_nan(pais, mu)\n",
        "            rhoi = ci = tf.minimum(1.0, ratio)\n",
        "\n",
        "            n_num, _ = ratio.shape\n",
        "\n",
        "            rhoi = self._reshape_and_cast(rhoi,3)\n",
        "            ci = self._reshape_and_cast(ci,3)\n",
        "            reward = self._reshape_and_cast(reward,1)\n",
        "            v_next = self._reshape_and_cast(v_next,1)\n",
        "            v = self._reshape_and_cast(v,1)\n",
        "            v_theta = self._reshape_and_cast(v_theta,1)\n",
        "\n",
        "            b4_delta_v = (reward + self.gamma * v_next - v)\n",
        "            b4_delta_v  =  tf.cast(b4_delta_v, tf.float32)\n",
        "            delta_v = tf.multiply(rhoi, b4_delta_v)\n",
        "            delta_v = self._reshape_and_cast(delta_v,3)\n",
        "            v_trace =v + self._sigma(ci, delta_v, n_num)\n",
        "            total_loss = self._compute_baseline_loss(v_trace - v_theta)\n",
        "            total_loss += self._compute_policy_gradient_loss(pai, actions, delta_v)\n",
        "            total_loss += self._compute_entropy_loss(pai)\n",
        "\n",
        "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "    def _reshape_and_cast(self, x, num):\n",
        "        x = tf.reshape(x, [10,num])\n",
        "        x  =  tf.cast(x, tf.float32)\n",
        "        return x\n",
        "\n",
        "    def _infinite_product(self, x, max_num):\n",
        "        num = tf.ones([3, ], tf.float32)\n",
        "        for i in range(max_num):\n",
        "            num *= x[i]   \n",
        "        return num\n",
        "\n",
        "    def _sigma(self, x, delta_v, b):\n",
        "        \n",
        "        num = 0.0\n",
        "        for i in range(b):\n",
        "            num += pow(self.gamma, i) * self._infinite_product(x, i) * delta_v[i]\n",
        "        return tf.cast(num, tf.float32)\n",
        "\n",
        "    def _compute_baseline_loss(self, advantages):\n",
        "        return .5 * tf.reduce_sum(tf.square(advantages))\n",
        "\n",
        "    def _compute_policy_gradient_loss(self, logits, actions, advantages):\n",
        "        cross_entropy = tf.losses.categorical_crossentropy(y_true=actions, y_pred=logits)\n",
        "        cross_entropy = tf.reshape(cross_entropy, [10,1])\n",
        "        advantages = tf.stop_gradient(advantages)\n",
        "        policy_gradient_loss_per_timestep = cross_entropy * advantages\n",
        "        return tf.reduce_sum(policy_gradient_loss_per_timestep)\n",
        "\n",
        "    def _compute_entropy_loss(self, logits):\n",
        "        log_policy = tf.math.log(logits)\n",
        "        entropy_per_timestep = tf.reduce_sum(-logits * log_policy, axis=-1)\n",
        "        return -tf.reduce_sum(entropy_per_timestep)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcPU3_nDFvro"
      },
      "source": [
        "class Learner(Critic):\n",
        "    def __init__(self):\n",
        "\n",
        "        conv_filter = 12\n",
        "        units = 28\n",
        "        look_back = 10\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "\n",
        "        input1_ = Input(shape=(look_back, 3))\n",
        "        input2_ = Input(shape=(look_back, 1))\n",
        "        input3_ = Input(shape=(look_back, 1))\n",
        "\n",
        "        x = Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\")(input1_)\n",
        "        x = MaxPool1D(pool_size=1, padding='same')(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        combined = concatenate([x, input2_, input3_],axis=-1)\n",
        "        common = LSTM(units, return_sequences=True)(combined)\n",
        "        common = Dense(units, kernel_initializer='random_uniform')(common)\n",
        "        common = Activation(\"relu\")(common)\n",
        "\n",
        "        actor  = Dense(3, activation=\"softmax\")(common)\n",
        "        critic = Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model([input1_, input2_, input3_], [actor, critic])\n",
        "        model.compile(loss = \"mean_absolute_error\", optimizer=opt)\n",
        "        model.summary()\n",
        "        #dot_img_file = './f\"{name}_model.png\"'\n",
        "        #tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
        "        self.model = model\n",
        "        super().__init__(model)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        length = memory.max_length_memory()\n",
        "        for i in range(length):\n",
        "            min = i\n",
        "            max = (i + 10)\n",
        "            state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu = memory.get_experiences(min, max)\n",
        "            self.valuenetwork(state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, learner):\n",
        "\n",
        "        self.learner = learner\n",
        "        self.model = clone_model(learner.model)\n",
        "        self.n_action = 3\n",
        "        self.state_arr = np.array([])\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.array([])\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def reset(self):\n",
        "        self.state_arr = np.empty((0,3), int)\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.empty((0,3), int)\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def policynetwork(self, state, prev_action, prev_reward):\n",
        "\n",
        "        if len(self.state_arr) == 10:\n",
        "            self.state_arr[0:-1] = self.state_arr[1:]\n",
        "            self.p_action_arr[0:-1] = self.p_action_arr[1:]\n",
        "            self.p_reward_arr[0:-1] = self.p_reward_arr[1:]\n",
        "            self.state_arr[-1] = state\n",
        "            self.p_action_arr[-1] = prev_action\n",
        "            self.p_reward_arr[-1] = prev_reward\n",
        "            tmp_state = copy.deepcopy(self.state_arr)\n",
        "            tmp_action = copy.deepcopy(self.p_action_arr)\n",
        "            tmp_reward = copy.deepcopy(self.p_reward_arr)\n",
        "            tmp_state = tmp_state.reshape(1,10,3)\n",
        "            tmp_action = tmp_action.reshape(1,10,1)\n",
        "            tmp_reward = tmp_reward.reshape(1,10,1)\n",
        "            act_p, v = self.model([tmp_state, tmp_action, tmp_reward]) # [-1.03245259 -0.55189404  0.87892511] 1 0\n",
        "            v_np = v.numpy()\n",
        "            p_np = act_p.numpy()\n",
        "            one_hot_actions = tf.one_hot([0,1,2], 3)\n",
        "            act_p = p_np[0][9]\n",
        "            mu = tf.reduce_sum(one_hot_actions * act_p, axis=1)\n",
        "            return np.random.choice(3, p=p_np[0][9]), v_np[0][9][0], mu.numpy()\n",
        "\n",
        "        self.state_arr = np.append(self.state_arr, np.array([state]), axis=0)\n",
        "        self.p_action_arr = np.append(self.p_action_arr, np.array([prev_action]))\n",
        "        self.p_reward_arr = np.append(self.p_reward_arr, np.array([prev_reward]))\n",
        "        return 1, 1, [0.0, 1.0, 0.0]\n",
        "\n",
        "    def policynetwork_next(self, next_state, action, reward):\n",
        "\n",
        "        if len(self.next_state_arr) == 10:\n",
        "            self.next_state_arr[0:-1] = self.next_state_arr[1:]\n",
        "            self.action_arr[0:-1] = self.action_arr[1:]\n",
        "            self.reward_arr[0:-1] = self.reward_arr[1:]\n",
        "            self.next_state_arr[-1] = next_state\n",
        "            self.action_arr[-1] = action\n",
        "            self.reward_arr[-1] = reward\n",
        "\n",
        "            tmp_n_state = copy.deepcopy(self.next_state_arr)\n",
        "            tmp_action = copy.deepcopy(self.action_arr)\n",
        "            tmp_reward = copy.deepcopy(self.reward_arr)\n",
        "            tmp_n_state = tmp_n_state.reshape(1,10,3)\n",
        "            tmp_action = tmp_action.reshape(1,10,1)\n",
        "            tmp_reward = tmp_reward.reshape(1,10,1)\n",
        "\n",
        "            _, v_next = self.model([tmp_n_state, tmp_action, tmp_reward])\n",
        "            v_next_np = v_next.numpy()\n",
        "            return v_next_np[0][9][0]\n",
        "\n",
        "        self.next_state_arr = np.append(self.next_state_arr, np.array([next_state]), axis=0)\n",
        "        self.action_arr = np.append(self.action_arr, np.array([action]))\n",
        "        self.reward_arr = np.append(self.reward_arr, np.array([reward]))\n",
        "        return 1.0\n",
        "\n",
        "    def load(self, name):\n",
        "        self.learner.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.learner.save(name)\n",
        "\n",
        "    def integration(self):\n",
        "        self.model = clone_model(self.learner.model)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        self.learner.placement(memory)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4-NrrtJBQWj"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : np.ndarray = np.empty((0,3), int)\n",
        "    next_state : np.ndarray = np.empty((0,3), int)\n",
        "    prev_action : np.ndarray = np.array([])\n",
        "    action : np.ndarray = np.array([])\n",
        "    prev_reward : np.ndarray = np.array([])\n",
        "    reward : np.ndarray = np.array([])\n",
        "    done : np.ndarray = np.array([])\n",
        "    v : np.ndarray = np.array([])\n",
        "    v_next : np.ndarray = np.array([])\n",
        "    mu : np.ndarray = np.empty((0,3), int)\n",
        "    minibatch_size : int = 64\n",
        "\n",
        "    def append_experiences(self, state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu):\n",
        "        self.state = np.append(self.state, np.array([state]), axis=0)\n",
        "        self.next_state = np.append(self.next_state, np.array([next_state]), axis=0)\n",
        "        self.prev_action = np.append(self.prev_action, np.array(prev_action))\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.prev_reward = np.append(self.prev_reward, np.array(prev_reward))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        self.done = np.append(self.done, np.array(done))\n",
        "        self.v = np.append(self.v, np.array(v))\n",
        "        self.v_next = np.append(self.v_next, np.array(v_next))\n",
        "        self.mu = np.append(self.mu, np.array([mu]), axis=0)\n",
        "\n",
        "    def max_length_memory(self):\n",
        "        max_len = len(self.state)\n",
        "        max_len = int(max_len) - 11\n",
        "\n",
        "        return max_len\n",
        "\n",
        "    def get_experiences(self, min, max):\n",
        "        state, next_state, mu = np.empty((0,3), int), np.empty((0,3), int), np.empty((0,3), int)\n",
        "        prev_action, action, prev_reward, reward, done, v, v_next = np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
        "        for i in range(min, max):\n",
        "            state = np.append(state, np.array([self.state[i]]), axis=0)\n",
        "            next_state = np.append(next_state, self.action[i])\n",
        "            prev_action = np.append(prev_action, self.prev_action[i])\n",
        "            action = np.append(action, self.action[i])\n",
        "            prev_reward = np.append(prev_reward, self.prev_reward[i])\n",
        "            reward = np.append(reward, self.reward[i])\n",
        "            done = np.append(done, self.done[i])\n",
        "            v = np.append(v, self.v[i])\n",
        "            v_next = np.append(v_next, self.v_next[i])\n",
        "            mu = np.append(mu, np.array([self.mu[i]]), axis=0)\n",
        "\n",
        "        return state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, num, mdl_dir, name, batch_size = 10, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "        \n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            self.actor.reset()\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory()\n",
        "            prev_action = 1\n",
        "            prev_reward = 0\n",
        "            i = 0\n",
        "    \n",
        "            while not done:\n",
        "                action, v, mu = self.actor.policynetwork(state, prev_action, prev_reward)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "                v_next = self.actor.policynetwork_next(next_state, action, reward)\n",
        "\n",
        "                if (i > self.batch_size) and (self.mode == 'train'):\n",
        "                    memory.append_experiences(state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu)\n",
        "\n",
        "                state = next_state\n",
        "                prev_action = action\n",
        "                prev_reward = reward\n",
        "                i += 1\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                self.actor.placement(memory)\n",
        "                self.actor.integration()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20802ee-db7d-4dac-8ff7-5787381efde8"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 50\n",
        "batch_size = 10\n",
        "learner = Learner()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "    actor = Actor(learner)\n",
        "    main = Main(env, actor, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 10, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 10, 12)       48          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 10, 12)       0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 10, 12)       0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 14)       0           activation[0][0]                 \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 10, 28)       4816        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10, 28)       812         lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 10, 28)       0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10, 3)        87          activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10, 1)        29          activation_1[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,792\n",
            "Trainable params: 5,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:01:15.162026 FixedProfit: 1063760\n",
            "Episode: 1/50 RapTime: 0:01:15.420233 FixedProfit: 957379\n",
            "Episode: 1/50 RapTime: 0:01:15.219243 FixedProfit: 1216137\n",
            "Episode: 1/50 RapTime: 0:01:15.103119 FixedProfit: 962257\n",
            "Episode: 2/50 RapTime: 0:01:14.712841 FixedProfit: 1250135\n",
            "Episode: 2/50 RapTime: 0:01:14.599378 FixedProfit: 929732\n",
            "Episode: 2/50 RapTime: 0:01:14.389025 FixedProfit: 1151962\n",
            "Episode: 2/50 RapTime: 0:01:14.028676 FixedProfit: 1385450\n",
            "Episode: 3/50 RapTime: 0:01:14.695470 FixedProfit: 1235838\n",
            "Episode: 3/50 RapTime: 0:01:14.472325 FixedProfit: 1068483\n",
            "Episode: 3/50 RapTime: 0:01:14.074290 FixedProfit: 1314556\n",
            "Episode: 3/50 RapTime: 0:01:13.308513 FixedProfit: 1234438\n",
            "Episode: 4/50 RapTime: 0:01:14.906170 FixedProfit: 793268\n",
            "Episode: 4/50 RapTime: 0:01:14.752908 FixedProfit: 969441\n",
            "Episode: 4/50 RapTime: 0:01:14.384118 FixedProfit: 1105086\n",
            "Episode: 4/50 RapTime: 0:01:14.056952 FixedProfit: 968965\n",
            "Episode: 5/50 RapTime: 0:01:14.592473 FixedProfit: 957547\n",
            "Episode: 5/50 RapTime: 0:01:14.383502 FixedProfit: 969588\n",
            "Episode: 5/50 RapTime: 0:01:14.118433 FixedProfit: 1081270\n",
            "Episode: 5/50 RapTime: 0:01:13.506826 FixedProfit: 1077715\n",
            "Episode: 6/50 RapTime: 0:01:14.634032 FixedProfit: 978304\n",
            "Episode: 6/50 RapTime: 0:01:14.104343 FixedProfit: 914396\n",
            "Episode: 6/50 RapTime: 0:01:14.328774 FixedProfit: 1106103\n",
            "Episode: 6/50 RapTime: 0:01:13.442314 FixedProfit: 1014785\n",
            "Episode: 7/50 RapTime: 0:01:14.064645 FixedProfit: 1112556\n",
            "Episode: 7/50 RapTime: 0:01:14.254610 FixedProfit: 849194\n",
            "Episode: 7/50 RapTime: 0:01:13.920394 FixedProfit: 1204722\n",
            "Episode: 7/50 RapTime: 0:01:13.522453 FixedProfit: 1232411\n",
            "Episode: 8/50 RapTime: 0:01:13.957123 FixedProfit: 1192432\n",
            "Episode: 8/50 RapTime: 0:01:13.775885 FixedProfit: 1107693\n",
            "Episode: 8/50 RapTime: 0:01:14.391016 FixedProfit: 1341503\n",
            "Episode: 8/50 RapTime: 0:01:13.385439 FixedProfit: 991071\n",
            "Episode: 9/50 RapTime: 0:01:13.676231 FixedProfit: 1188025\n",
            "Episode: 9/50 RapTime: 0:01:13.819405 FixedProfit: 1217013\n",
            "Episode: 9/50 RapTime: 0:01:13.681938 FixedProfit: 1109137\n",
            "Episode: 9/50 RapTime: 0:01:12.821393 FixedProfit: 1158794\n",
            "Episode: 10/50 RapTime: 0:01:12.903306 FixedProfit: 1003646\n",
            "Episode: 10/50 RapTime: 0:01:13.313519 FixedProfit: 1136902\n",
            "Episode: 10/50 RapTime: 0:01:13.542887 FixedProfit: 1104929\n",
            "Episode: 10/50 RapTime: 0:01:12.628238 FixedProfit: 1184326\n",
            "Episode: 11/50 RapTime: 0:01:12.860397 FixedProfit: 1151092\n",
            "Episode: 11/50 RapTime: 0:01:13.216856 FixedProfit: 1078013\n",
            "Episode: 11/50 RapTime: 0:01:12.783888 FixedProfit: 1296098\n",
            "Episode: 11/50 RapTime: 0:01:12.392695 FixedProfit: 1233355\n",
            "Episode: 12/50 RapTime: 0:01:12.875273 FixedProfit: 1127296\n",
            "Episode: 12/50 RapTime: 0:01:13.106527 FixedProfit: 955512\n",
            "Episode: 12/50 RapTime: 0:01:13.516094 FixedProfit: 1146682\n",
            "Episode: 12/50 RapTime: 0:01:12.319332 FixedProfit: 1178606\n",
            "Episode: 13/50 RapTime: 0:01:12.482027 FixedProfit: 1275574\n",
            "Episode: 13/50 RapTime: 0:01:13.191524 FixedProfit: 948575\n",
            "Episode: 13/50 RapTime: 0:01:12.714791 FixedProfit: 1003242\n",
            "Episode: 13/50 RapTime: 0:01:11.886050 FixedProfit: 1244777\n",
            "Episode: 14/50 RapTime: 0:01:11.987635 FixedProfit: 1166269\n",
            "Episode: 14/50 RapTime: 0:01:12.719408 FixedProfit: 1177295\n",
            "Episode: 14/50 RapTime: 0:01:13.023252 FixedProfit: 1135384\n",
            "Episode: 14/50 RapTime: 0:01:11.592927 FixedProfit: 923906\n",
            "Episode: 15/50 RapTime: 0:01:11.888053 FixedProfit: 949009\n",
            "Episode: 15/50 RapTime: 0:01:12.952207 FixedProfit: 1081447\n",
            "Episode: 15/50 RapTime: 0:01:12.824510 FixedProfit: 965972\n",
            "Episode: 15/50 RapTime: 0:01:11.477334 FixedProfit: 1066379\n",
            "Episode: 16/50 RapTime: 0:01:11.153982 FixedProfit: 1178403\n",
            "Episode: 16/50 RapTime: 0:01:12.309734 FixedProfit: 992929\n",
            "Episode: 16/50 RapTime: 0:01:12.215830 FixedProfit: 981844\n",
            "Episode: 16/50 RapTime: 0:01:10.816962 FixedProfit: 1032255\n",
            "Episode: 17/50 RapTime: 0:01:10.890364 FixedProfit: 1176719\n",
            "Episode: 17/50 RapTime: 0:01:12.350106 FixedProfit: 1272479\n",
            "Episode: 17/50 RapTime: 0:01:11.889411 FixedProfit: 1128789\n",
            "Episode: 17/50 RapTime: 0:01:10.255299 FixedProfit: 1043161\n",
            "Episode: 18/50 RapTime: 0:01:11.647308 FixedProfit: 1064973\n",
            "Episode: 18/50 RapTime: 0:01:12.826868 FixedProfit: 934820\n",
            "Episode: 18/50 RapTime: 0:01:12.269250 FixedProfit: 1029537\n",
            "Episode: 18/50 RapTime: 0:01:10.726171 FixedProfit: 1048639\n",
            "Episode: 19/50 RapTime: 0:01:11.454583 FixedProfit: 1285010\n",
            "Episode: 19/50 RapTime: 0:01:12.632607 FixedProfit: 1062227\n",
            "Episode: 19/50 RapTime: 0:01:12.154196 FixedProfit: 1139005\n",
            "Episode: 19/50 RapTime: 0:01:10.702943 FixedProfit: 1224330\n",
            "Episode: 20/50 RapTime: 0:01:11.335338 FixedProfit: 1212627\n",
            "Episode: 20/50 RapTime: 0:01:12.377886 FixedProfit: 1005684\n",
            "Episode: 20/50 RapTime: 0:01:12.119648 FixedProfit: 1292246\n",
            "Episode: 20/50 RapTime: 0:01:10.693001 FixedProfit: 908862\n",
            "Episode: 21/50 RapTime: 0:01:10.917792 FixedProfit: 975168\n",
            "Episode: 21/50 RapTime: 0:01:12.642366 FixedProfit: 939556\n",
            "Episode: 21/50 RapTime: 0:01:11.796013 FixedProfit: 1316911\n",
            "Episode: 21/50 RapTime: 0:01:10.014764 FixedProfit: 1426687\n",
            "Episode: 22/50 RapTime: 0:01:11.289158 FixedProfit: 1380379\n",
            "Episode: 22/50 RapTime: 0:01:12.847724 FixedProfit: 1196334\n",
            "Episode: 22/50 RapTime: 0:01:12.267598 FixedProfit: 1234753\n",
            "Episode: 22/50 RapTime: 0:01:10.279859 FixedProfit: 915836\n",
            "Episode: 23/50 RapTime: 0:01:10.320091 FixedProfit: 1138653\n",
            "Episode: 23/50 RapTime: 0:01:11.575761 FixedProfit: 1135206\n",
            "Episode: 23/50 RapTime: 0:01:12.111797 FixedProfit: 1132044\n",
            "Episode: 23/50 RapTime: 0:01:08.910261 FixedProfit: 1151801\n",
            "Episode: 24/50 RapTime: 0:01:10.097814 FixedProfit: 1069834\n",
            "Episode: 24/50 RapTime: 0:01:12.129879 FixedProfit: 1160937\n",
            "Episode: 24/50 RapTime: 0:01:11.720107 FixedProfit: 1131079\n",
            "Episode: 24/50 RapTime: 0:01:09.594775 FixedProfit: 1065814\n",
            "Episode: 25/50 RapTime: 0:01:10.530870 FixedProfit: 1158601\n",
            "Episode: 25/50 RapTime: 0:01:12.564688 FixedProfit: 1058078\n",
            "Episode: 25/50 RapTime: 0:01:12.101293 FixedProfit: 1139385\n",
            "Episode: 25/50 RapTime: 0:01:10.146231 FixedProfit: 921906\n",
            "Episode: 26/50 RapTime: 0:01:10.997128 FixedProfit: 1197442\n",
            "Episode: 26/50 RapTime: 0:01:12.359306 FixedProfit: 1209563\n",
            "Episode: 26/50 RapTime: 0:01:12.400923 FixedProfit: 1180483\n",
            "Episode: 26/50 RapTime: 0:01:10.601495 FixedProfit: 1125828\n",
            "Episode: 27/50 RapTime: 0:01:10.118647 FixedProfit: 1160784\n",
            "Episode: 27/50 RapTime: 0:01:11.898109 FixedProfit: 975093\n",
            "Episode: 27/50 RapTime: 0:01:12.436635 FixedProfit: 1106822\n",
            "Episode: 27/50 RapTime: 0:01:09.713048 FixedProfit: 1193379\n",
            "Episode: 28/50 RapTime: 0:01:10.181564 FixedProfit: 1294040\n",
            "Episode: 28/50 RapTime: 0:01:12.165602 FixedProfit: 951551\n",
            "Episode: 28/50 RapTime: 0:01:11.612634 FixedProfit: 1135225\n",
            "Episode: 28/50 RapTime: 0:01:09.622064 FixedProfit: 1095103\n",
            "Episode: 29/50 RapTime: 0:01:09.185120 FixedProfit: 1179482\n",
            "Episode: 29/50 RapTime: 0:01:11.811380 FixedProfit: 1101738\n",
            "Episode: 29/50 RapTime: 0:01:11.254066 FixedProfit: 1111117\n",
            "Episode: 29/50 RapTime: 0:01:08.872917 FixedProfit: 1018822\n",
            "Episode: 30/50 RapTime: 0:01:09.370157 FixedProfit: 1105640\n",
            "Episode: 30/50 RapTime: 0:01:11.737587 FixedProfit: 1036352\n",
            "Episode: 30/50 RapTime: 0:01:11.322499 FixedProfit: 865157\n",
            "Episode: 30/50 RapTime: 0:01:09.346449 FixedProfit: 1251872\n",
            "Episode: 31/50 RapTime: 0:01:09.226815 FixedProfit: 1068943\n",
            "Episode: 31/50 RapTime: 0:01:11.431536 FixedProfit: 1330448\n",
            "Episode: 31/50 RapTime: 0:01:11.100689 FixedProfit: 1263174\n",
            "Episode: 31/50 RapTime: 0:01:08.255486 FixedProfit: 933951\n",
            "Episode: 32/50 RapTime: 0:01:08.330726 FixedProfit: 1284368\n",
            "Episode: 32/50 RapTime: 0:01:10.972656 FixedProfit: 1167351\n",
            "Episode: 32/50 RapTime: 0:01:10.667881 FixedProfit: 1267009\n",
            "Episode: 32/50 RapTime: 0:01:08.021339 FixedProfit: 1062326\n",
            "Episode: 33/50 RapTime: 0:01:08.501042 FixedProfit: 1345445\n",
            "Episode: 33/50 RapTime: 0:01:11.419482 FixedProfit: 1056452\n",
            "Episode: 33/50 RapTime: 0:01:10.485887 FixedProfit: 1027193\n",
            "Episode: 33/50 RapTime: 0:01:08.122264 FixedProfit: 1154898\n",
            "Episode: 34/50 RapTime: 0:01:08.233719 FixedProfit: 1001921\n",
            "Episode: 34/50 RapTime: 0:01:10.919278 FixedProfit: 1050067\n",
            "Episode: 34/50 RapTime: 0:01:10.722049 FixedProfit: 1177307\n",
            "Episode: 34/50 RapTime: 0:01:08.416824 FixedProfit: 996397\n",
            "Episode: 35/50 RapTime: 0:01:08.534020 FixedProfit: 1140697\n",
            "Episode: 35/50 RapTime: 0:01:10.983280 FixedProfit: 977029\n",
            "Episode: 35/50 RapTime: 0:01:10.694170 FixedProfit: 1117657\n",
            "Episode: 35/50 RapTime: 0:01:08.251920 FixedProfit: 1020960\n",
            "Episode: 36/50 RapTime: 0:01:07.896576 FixedProfit: 1141008\n",
            "Episode: 36/50 RapTime: 0:01:10.638974 FixedProfit: 1010400\n",
            "Episode: 36/50 RapTime: 0:01:10.304765 FixedProfit: 1009981\n",
            "Episode: 36/50 RapTime: 0:01:08.019316 FixedProfit: 982120\n",
            "Episode: 37/50 RapTime: 0:01:07.444831 FixedProfit: 1274326\n",
            "Episode: 37/50 RapTime: 0:01:10.393681 FixedProfit: 1153579\n",
            "Episode: 37/50 RapTime: 0:01:10.200664 FixedProfit: 1308552\n",
            "Episode: 37/50 RapTime: 0:01:08.085578 FixedProfit: 942360\n",
            "Episode: 38/50 RapTime: 0:01:07.771653 FixedProfit: 965692\n",
            "Episode: 38/50 RapTime: 0:01:10.358113 FixedProfit: 1104020\n",
            "Episode: 38/50 RapTime: 0:01:10.265993 FixedProfit: 1255978\n",
            "Episode: 38/50 RapTime: 0:01:07.594789 FixedProfit: 1113556\n",
            "Episode: 39/50 RapTime: 0:01:08.072806 FixedProfit: 1010551\n",
            "Episode: 39/50 RapTime: 0:01:10.480041 FixedProfit: 950140\n",
            "Episode: 39/50 RapTime: 0:01:10.296617 FixedProfit: 1267157\n",
            "Episode: 39/50 RapTime: 0:01:07.581593 FixedProfit: 1031677\n",
            "Episode: 40/50 RapTime: 0:01:07.714727 FixedProfit: 1014949\n",
            "Episode: 40/50 RapTime: 0:01:10.713569 FixedProfit: 1220687\n",
            "Episode: 40/50 RapTime: 0:01:10.233763 FixedProfit: 1195733\n",
            "Episode: 40/50 RapTime: 0:01:07.791708 FixedProfit: 1147557\n",
            "Episode: 41/50 RapTime: 0:01:07.099607 FixedProfit: 920008\n",
            "Episode: 41/50 RapTime: 0:01:10.881417 FixedProfit: 1074554\n",
            "Episode: 41/50 RapTime: 0:01:10.080958 FixedProfit: 997402\n",
            "Episode: 41/50 RapTime: 0:01:07.233878 FixedProfit: 974896\n",
            "Episode: 42/50 RapTime: 0:01:07.330900 FixedProfit: 905008\n",
            "Episode: 42/50 RapTime: 0:01:10.664191 FixedProfit: 1073067\n",
            "Episode: 42/50 RapTime: 0:01:10.177440 FixedProfit: 1163422\n",
            "Episode: 42/50 RapTime: 0:01:06.706831 FixedProfit: 1381823\n",
            "Episode: 43/50 RapTime: 0:01:06.760233 FixedProfit: 1088101\n",
            "Episode: 43/50 RapTime: 0:01:10.216535 FixedProfit: 1089391\n",
            "Episode: 43/50 RapTime: 0:01:09.881225 FixedProfit: 1324965\n",
            "Episode: 43/50 RapTime: 0:01:06.750766 FixedProfit: 1107524\n",
            "Episode: 44/50 RapTime: 0:01:05.881289 FixedProfit: 1271417\n",
            "Episode: 44/50 RapTime: 0:01:09.704225 FixedProfit: 1042836\n",
            "Episode: 44/50 RapTime: 0:01:09.150011 FixedProfit: 1065442\n",
            "Episode: 44/50 RapTime: 0:01:06.003638 FixedProfit: 1028086\n",
            "Episode: 45/50 RapTime: 0:01:06.394965 FixedProfit: 1051369\n",
            "Episode: 45/50 RapTime: 0:01:09.361857 FixedProfit: 1110621\n",
            "Episode: 45/50 RapTime: 0:01:08.915549 FixedProfit: 1256028\n",
            "Episode: 45/50 RapTime: 0:01:05.718289 FixedProfit: 1059342\n",
            "Episode: 46/50 RapTime: 0:01:06.209813 FixedProfit: 1053909\n",
            "Episode: 46/50 RapTime: 0:01:09.568514 FixedProfit: 997967\n",
            "Episode: 46/50 RapTime: 0:01:08.560185 FixedProfit: 1105833\n",
            "Episode: 46/50 RapTime: 0:01:05.845853 FixedProfit: 1142774\n",
            "Episode: 47/50 RapTime: 0:01:05.775080 FixedProfit: 1135893\n",
            "Episode: 47/50 RapTime: 0:01:09.173716 FixedProfit: 1166306\n",
            "Episode: 47/50 RapTime: 0:01:08.548213 FixedProfit: 1221845\n",
            "Episode: 47/50 RapTime: 0:01:05.456171 FixedProfit: 933641\n",
            "Episode: 48/50 RapTime: 0:01:06.793781 FixedProfit: 1267154\n",
            "Episode: 48/50 RapTime: 0:01:10.006796 FixedProfit: 1101363\n",
            "Episode: 48/50 RapTime: 0:01:09.521563 FixedProfit: 1139549\n",
            "Episode: 48/50 RapTime: 0:01:06.446129 FixedProfit: 1183760\n",
            "Episode: 49/50 RapTime: 0:01:06.268931 FixedProfit: 1098190\n",
            "Episode: 49/50 RapTime: 0:01:09.174231 FixedProfit: 1511364\n",
            "Episode: 49/50 RapTime: 0:01:09.030218 FixedProfit: 1139240\n",
            "Episode: 49/50 RapTime: 0:01:05.571908 FixedProfit: 1231066\n",
            "Episode: 50/50 RapTime: 0:01:06.304125 FixedProfit: 1180365\n",
            "Episode: 50/50 RapTime: 0:01:09.604371 FixedProfit: 1220251\n",
            "Episode: 50/50 RapTime: 0:01:09.181350 FixedProfit: 1491504\n",
            "Episode: 50/50 RapTime: 0:01:05.263911 FixedProfit: 1070487\n"
          ]
        }
      ]
    }
  ]
}