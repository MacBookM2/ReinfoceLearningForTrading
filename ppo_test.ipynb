{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ppo_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/ppo_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "6fbdaaa2-502f-4a2b-bfcd-d5777d8683ae"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'test'\n",
        "name = 'ppo'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_shape = 3\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain):\n",
        "        self.model = brain.model\n",
        "        self.brain = brain\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            self._reduce_epsilon()\n",
        "            return np.random.choice(3)\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        self._reduce_epsilon()\n",
        "        return np.random.choice(3, p=act_p[0].numpy())\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def _reduce_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model):\n",
        "        self.model = model\n",
        "        self.gamma = 0.2\n",
        "        self.beta_1 = 0.1\n",
        "        self.beta_2 = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        state_arr  = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_arr = np.asarray([e[\"action\"] for e in experiences])\n",
        "        advantage  = np.asarray([e[\"advantage\"] for e in experiences]).reshape((-1, 1))\n",
        "        prob_arr   = np.asarray([e[\"prob\"] for e in experiences])\n",
        "\n",
        "        advantage = (advantage - np.mean(advantage)) / (np.std(advantage) + 1e-8)\n",
        "        onehot_actions = tf.one_hot(action_arr, 3).numpy()\n",
        "\n",
        "        old_policy = tf.reduce_sum(onehot_actions * prob_arr, axis=1, keepdims=True)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            act_p, v = self.model(state_arr, training=True)\n",
        "\n",
        "            advantage = advantage - tf.stop_gradient(v)\n",
        "\n",
        "            new_policy = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            log_new_policy = tf.math.log(tf.clip_by_value(new_policy, 1e-10, 1.0))\n",
        "\n",
        "            policy_ration = new_policy / old_policy\n",
        "\n",
        "            losses_clip  = self._losses_clip(advantage, policy_ration)\n",
        "            losses_value = tf.stop_gradient(self._losses_value(advantage))\n",
        "            entropy      = tf.reduce_sum(log_new_policy * new_policy, axis=1, keepdims=True)\n",
        "\n",
        "            total_loss   = losses_clip - self.beta_1 * losses_value - self.beta_2 * entropy\n",
        "            loss         = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.model.optimizer.apply_gradients((grad, var) for (grad, var) in \n",
        "                                             zip(gradients, self.model.trainable_variables) if grad is not None)\n",
        "\n",
        "    def _losses_value(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _losses_clip(self, advantage, policy_ration):\n",
        "\n",
        "        r_clip = tf.clip_by_value(policy_ration, 1 - self.gamma, 1 + self.gamma)\n",
        "\n",
        "        loss_unclipped = policy_ration * advantage\n",
        "        loss_clipped = r_clip * advantage\n",
        "\n",
        "        return tf.minimum(loss_clipped, loss_clipped)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1UX215aeoXq"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : List[List[float]] = field(default_factory=list)\n",
        "    action : List[int] = field(default_factory=list)\n",
        "    reward : List[float] = field(default_factory=list)\n",
        "    next_state : List[List[int]] = field(default_factory=list)\n",
        "    done : List[bool] = field(default_factory=list)\n",
        "    prob_bol : List[bool] = field(default_factory=list)\n",
        "    advantage : List[float] = field(default_factory=list)\n",
        "    model : str = None\n",
        "    gae_lambda: float = 0.9\n",
        "    gamma: float = 0.9\n",
        "\n",
        "    def append_experiences(self, state, action, reward, next_state, done):\n",
        "        self.state.append(state)\n",
        "        self.action.append(action)\n",
        "        self.reward.append(reward)\n",
        "        self.next_state.append(next_state)\n",
        "        self.done.append(done)\n",
        "\n",
        "    def random_experiences(self, batch_size):\n",
        "        max_size = len(self.state) - 1\n",
        "        self._make_advantage()\n",
        "        batch_num = self._random_num(1, max_size, batch_size)\n",
        "        experiences = []\n",
        "        for i in batch_num:\n",
        "            prob = self._probability(self.state[i])\n",
        "            prob = prob.numpy()\n",
        "            prob = prob[0]\n",
        "            experiences.append({\"state\": self.state[i], \"action\": self.action[i], \n",
        "                                \"reward\": self.reward[i], \"next_state\": self.next_state[i], \n",
        "                                \"done\": self.done[i], \"prob\": prob, \"advantage\": self.advantage[i]})\n",
        "        return experiences\n",
        "\n",
        "    def _random_num(self, min_num, max_num, batch_size):\n",
        "        arr = []\n",
        "        while len(arr) < batch_size:\n",
        "            n = random.randint(min_num, max_num)\n",
        "            if not n in arr:\n",
        "                arr.append(n)\n",
        "        return arr\n",
        "\n",
        "    def _make_advantage(self):\n",
        "        state = np.asarray([e for e in self.state])\n",
        "        next_state = np.asarray([e for e in self.next_state])\n",
        "        v     = self._evaluation_value(state)\n",
        "        v_old = self._evaluation_value(next_state)\n",
        "        v     =v.numpy()\n",
        "        v_old =v_old.numpy()\n",
        "\n",
        "        for i in range(len(self.reward)):\n",
        "            gae = 0\n",
        "            t = 0\n",
        "            for j in range(i, len(self.reward)):\n",
        "                delta = self.reward[j] + self.gamma * v_old[j][0] - v[j][0]\n",
        "                gae += ((self.gae_lambda * self.gamma) ** t) * delta\n",
        "                t += 1\n",
        "            self.advantage.append(gae)\n",
        "\n",
        "    def _probability(self, state):\n",
        "        p, _ = self.model(state.reshape((1,-1)))\n",
        "        return p\n",
        "\n",
        "    def _evaluation_value(self, state):\n",
        "        _, v = self.model(state, training=True)\n",
        "        return v"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, num, mdl_dir, name, batch_size = 128, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory(model = self.actor.model)\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action     = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    memory.append_experiences(state, action, reward, next_state, done)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                experiences = memory.random_experiences(self.batch_size)\n",
        "                self.critic.valuenetwork(experiences)\n",
        "\n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\"\n",
        "                .format(episode + 1, episodes_times, play_time, info['cur_revenue'], \n",
        "                        info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\"\n",
        "                .format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgv85YlVOaum",
        "outputId": "0df06a41-2fbd-4f89-ffa2-a7baa0e4bde7"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 25\n",
        "batch_size = 128\n",
        "\n",
        "brain = ParameterServer()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    model = brain.model\n",
        "    actor = Actor(brain)\n",
        "    critic = Critic(model)\n",
        "    main = Main(env, actor, critic, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/25 RapTime: 0:00:10.621618 FixedProfit: 931200 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:00:10.649164 FixedProfit: 1045077 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 1/25 RapTime: 0:00:10.680629 FixedProfit: 957704 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 1/25 RapTime: 0:00:10.680351 FixedProfit: 1352242 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 2/25 RapTime: 0:00:08.973878 FixedProfit: 979412 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 2/25 RapTime: 0:00:08.937299 FixedProfit: 1082075 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 2/25 RapTime: 0:00:08.943104 FixedProfit: 1024624 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 2/25 RapTime: 0:00:09.023656 FixedProfit: 1062399 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 3/25 RapTime: 0:00:08.922783 FixedProfit: 1191493 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:08.898471 FixedProfit: 1204030 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 3/25 RapTime: 0:00:09.020142 FixedProfit: 1076476 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:09.009131 FixedProfit: 1509430 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 4/25 RapTime: 0:00:08.979588 FixedProfit: 1123875 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 4/25 RapTime: 0:00:09.015309 FixedProfit: 1369321 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 4/25 RapTime: 0:00:08.984192 FixedProfit: 966614 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 4/25 RapTime: 0:00:09.022500 FixedProfit: 1489037 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 5/25 RapTime: 0:00:08.957321 FixedProfit: 887496 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 5/25 RapTime: 0:00:09.008679 FixedProfit: 1090788 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 5/25 RapTime: 0:00:08.987660 FixedProfit: 948627 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:09.018792 FixedProfit: 811428 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 6/25 RapTime: 0:00:08.888185 FixedProfit: 1184811 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 6/25 RapTime: 0:00:08.870501 FixedProfit: 1036282 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 6/25 RapTime: 0:00:08.980550 FixedProfit: 1032644 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 6/25 RapTime: 0:00:08.953217 FixedProfit: 1210975 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 7/25 RapTime: 0:00:08.923811 FixedProfit: 1220052 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 7/25 RapTime: 0:00:08.971380 FixedProfit: 1036920 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 7/25 RapTime: 0:00:08.954838 FixedProfit: 789292 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 7/25 RapTime: 0:00:08.992634 FixedProfit: 1005701 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 8/25 RapTime: 0:00:08.948342 FixedProfit: 1285071 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 8/25 RapTime: 0:00:08.942177 FixedProfit: 1331354 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 8/25 RapTime: 0:00:08.952255 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:08.981932 FixedProfit: 1021795 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 9/25 RapTime: 0:00:09.024277 FixedProfit: 967397 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 9/25 RapTime: 0:00:08.956972 FixedProfit: 1376155 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:09.027235 FixedProfit: 1170494 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 9/25 RapTime: 0:00:09.031080 FixedProfit: 923052 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:09.273928 FixedProfit: 980891 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 10/25 RapTime: 0:00:09.348963 FixedProfit: 1268332 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 10/25 RapTime: 0:00:09.178044 FixedProfit: 1015471 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 10/25 RapTime: 0:00:09.284551 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:08.933852 FixedProfit: 1199869 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 11/25 RapTime: 0:00:08.905334 FixedProfit: 1215114 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 11/25 RapTime: 0:00:08.934655 FixedProfit: 1309251 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 11/25 RapTime: 0:00:08.917640 FixedProfit: 1174814 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 12/25 RapTime: 0:00:08.932232 FixedProfit: 1183435 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 12/25 RapTime: 0:00:08.954701 FixedProfit: 1016564 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:08.903560 FixedProfit: 733088 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 12/25 RapTime: 0:00:08.939324 FixedProfit: 1318389 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 13/25 RapTime: 0:00:08.956083 FixedProfit: 932818 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:08.989919 FixedProfit: 1085548 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 13/25 RapTime: 0:00:08.933716 FixedProfit: 1088770 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 13/25 RapTime: 0:00:09.059130 FixedProfit: 1040592 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:08.904140 FixedProfit: 1133229 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 14/25 RapTime: 0:00:08.972888 FixedProfit: 1116028 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 14/25 RapTime: 0:00:08.951757 FixedProfit: 1196881 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 14/25 RapTime: 0:00:08.923308 FixedProfit: 1187775 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 15/25 RapTime: 0:00:08.973875 FixedProfit: 1053619 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 15/25 RapTime: 0:00:08.934252 FixedProfit: 779339 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 15/25 RapTime: 0:00:08.982825 FixedProfit: 979077 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:08.957304 FixedProfit: 1124479 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 16/25 RapTime: 0:00:08.908426 FixedProfit: 1130922 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 16/25 RapTime: 0:00:08.919031 FixedProfit: 946726 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:08.949563 FixedProfit: 1091756 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:08.899037 FixedProfit: 1238912 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 17/25 RapTime: 0:00:08.989097 FixedProfit: 1232774 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 17/25 RapTime: 0:00:08.958993 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:08.889991 FixedProfit: 1196841 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 17/25 RapTime: 0:00:08.996368 FixedProfit: 899537 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:09.016608 FixedProfit: 1073846 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 18/25 RapTime: 0:00:09.034381 FixedProfit: 1077031 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 18/25 RapTime: 0:00:09.023021 FixedProfit: 817431 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 18/25 RapTime: 0:00:08.910352 FixedProfit: 877548 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 19/25 RapTime: 0:00:08.909692 FixedProfit: 1247092 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 19/25 RapTime: 0:00:08.947686 FixedProfit: 1107601 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 19/25 RapTime: 0:00:08.970097 FixedProfit: 1209356 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 19/25 RapTime: 0:00:08.970335 FixedProfit: 1380208 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 20/25 RapTime: 0:00:08.971784 FixedProfit: 959197 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:08.977213 FixedProfit: 1048247 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 20/25 RapTime: 0:00:09.051752 FixedProfit: 1102775 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 20/25 RapTime: 0:00:08.944825 FixedProfit: 1120034 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 21/25 RapTime: 0:00:08.885114 FixedProfit: 1012767 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:08.905770 FixedProfit: 1214319 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 21/25 RapTime: 0:00:08.954979 FixedProfit: 1548009 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 21/25 RapTime: 0:00:08.950124 FixedProfit: 1202043 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:09.005788 FixedProfit: 1159851 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 22/25 RapTime: 0:00:08.999862 FixedProfit: 1386521 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 22/25 RapTime: 0:00:08.970564 FixedProfit: 1316705 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 22/25 RapTime: 0:00:08.924274 FixedProfit: 1411185 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 23/25 RapTime: 0:00:08.942316 FixedProfit: 980382 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 23/25 RapTime: 0:00:08.909296 FixedProfit: 1391969 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 23/25 RapTime: 0:00:08.952412 FixedProfit: 989959 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 23/25 RapTime: 0:00:08.972285 FixedProfit: 901729 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:08.901399 FixedProfit: 1394206 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 24/25 RapTime: 0:00:08.906350 FixedProfit: 1080806 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 24/25 RapTime: 0:00:08.921201 FixedProfit: 921219 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:08.893281 FixedProfit: 1027308 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 25/25 RapTime: 0:00:08.965810 FixedProfit: 1151509 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 25/25 RapTime: 0:00:08.840966 FixedProfit: 1098181 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 25/25 RapTime: 0:00:08.881524 FixedProfit: 1226125 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 25/25 RapTime: 0:00:08.885230 FixedProfit: 1095388 TradeTimes: 4 TradeWin: 4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}