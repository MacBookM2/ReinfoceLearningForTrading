{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a2c_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "7647626d-01a2-4cc7-de19-f730efb81b68"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "mode = 'test'\n",
        "name = 'a2c'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_shape = 3\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        Brain.model = model\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        Brain.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        Brain.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = Brain.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(3, p=act_p[0].numpy())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic(Brain):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "        self.beta  = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, 3)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = Brain.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, Brain.model.trainable_variables)\n",
        "\n",
        "        Brain.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, Brain.model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = Brain.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "            self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "        else:\n",
        "            self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "\n",
        "            total_reward = 0.0\n",
        "            experiences = []\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "                if mode == 'train':\n",
        "                    experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                    if len(experiences) == self.batch_size:\n",
        "                        self.critic.valuenetwork(experiences)\n",
        "                        experiences = []\n",
        "\n",
        "                state = next_state\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=self.df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, self.episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            else:\n",
        "                record = pd.Series(info['cur_revenue'], index=self.df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, self.episodes_times, play_time, info['cur_revenue']))\n",
        "\n",
        "            self.df_rec = self.df_rec.append(record, ignore_index=True)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "        self._save_csv()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "    def _save_csv(self):\n",
        "        self.df_rec.to_csv(csv_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3b2ac9-ed37-4f53-8d60-5757bade1a2e"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "\n",
        "actor = Actor()\n",
        "critic = Critic()\n",
        "env = Environment(df, initial_money, mode)\n",
        "main = Main(env, actor, critic, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:02.663354 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/100 RapTime: 0:00:01.432115 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/100 RapTime: 0:00:01.395333 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/100 RapTime: 0:00:01.391442 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/100 RapTime: 0:00:01.400483 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/100 RapTime: 0:00:01.412450 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/100 RapTime: 0:00:01.388809 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/100 RapTime: 0:00:01.401779 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/100 RapTime: 0:00:01.397120 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/100 RapTime: 0:00:01.401351 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/100 RapTime: 0:00:01.388237 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/100 RapTime: 0:00:01.399910 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/100 RapTime: 0:00:01.400661 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/100 RapTime: 0:00:01.405854 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/100 RapTime: 0:00:01.395626 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/100 RapTime: 0:00:01.408019 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/100 RapTime: 0:00:01.411256 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/100 RapTime: 0:00:01.406244 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/100 RapTime: 0:00:01.382620 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/100 RapTime: 0:00:01.388651 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/100 RapTime: 0:00:01.394461 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/100 RapTime: 0:00:01.383344 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/100 RapTime: 0:00:01.407153 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/100 RapTime: 0:00:01.412994 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/100 RapTime: 0:00:01.403863 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 26/100 RapTime: 0:00:01.405047 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 27/100 RapTime: 0:00:01.400127 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 28/100 RapTime: 0:00:01.402207 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 29/100 RapTime: 0:00:01.460180 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 30/100 RapTime: 0:00:01.512948 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 31/100 RapTime: 0:00:01.480488 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 32/100 RapTime: 0:00:01.478642 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 33/100 RapTime: 0:00:01.511056 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 34/100 RapTime: 0:00:01.475891 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 35/100 RapTime: 0:00:01.508130 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 36/100 RapTime: 0:00:01.514045 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 37/100 RapTime: 0:00:01.508424 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 38/100 RapTime: 0:00:01.493723 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 39/100 RapTime: 0:00:01.479778 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 40/100 RapTime: 0:00:01.506059 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 41/100 RapTime: 0:00:01.492861 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 42/100 RapTime: 0:00:01.482820 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 43/100 RapTime: 0:00:01.511416 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 44/100 RapTime: 0:00:01.461323 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 45/100 RapTime: 0:00:01.488540 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 46/100 RapTime: 0:00:01.404781 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 47/100 RapTime: 0:00:01.378451 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 48/100 RapTime: 0:00:01.403191 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 49/100 RapTime: 0:00:01.365923 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 50/100 RapTime: 0:00:01.386818 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 51/100 RapTime: 0:00:01.377249 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 52/100 RapTime: 0:00:01.396549 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 53/100 RapTime: 0:00:01.400662 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 54/100 RapTime: 0:00:01.393360 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 55/100 RapTime: 0:00:01.397694 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 56/100 RapTime: 0:00:01.447030 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 57/100 RapTime: 0:00:01.378153 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 58/100 RapTime: 0:00:01.422447 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 59/100 RapTime: 0:00:01.397488 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 60/100 RapTime: 0:00:01.393669 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 61/100 RapTime: 0:00:01.369240 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 62/100 RapTime: 0:00:01.370950 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 63/100 RapTime: 0:00:01.406083 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 64/100 RapTime: 0:00:01.385587 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 65/100 RapTime: 0:00:01.392214 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 66/100 RapTime: 0:00:01.402743 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 67/100 RapTime: 0:00:01.432939 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 68/100 RapTime: 0:00:01.403570 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 69/100 RapTime: 0:00:01.381983 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 70/100 RapTime: 0:00:01.398709 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 71/100 RapTime: 0:00:01.419887 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 72/100 RapTime: 0:00:01.390294 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 73/100 RapTime: 0:00:01.382901 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 74/100 RapTime: 0:00:01.390207 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 75/100 RapTime: 0:00:01.448986 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 76/100 RapTime: 0:00:01.485832 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 77/100 RapTime: 0:00:01.491776 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 78/100 RapTime: 0:00:01.492435 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 79/100 RapTime: 0:00:01.494693 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 80/100 RapTime: 0:00:01.487104 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 81/100 RapTime: 0:00:01.511005 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 82/100 RapTime: 0:00:01.534141 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 83/100 RapTime: 0:00:01.529497 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 84/100 RapTime: 0:00:01.536557 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 85/100 RapTime: 0:00:01.541562 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 86/100 RapTime: 0:00:01.534591 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 87/100 RapTime: 0:00:01.523152 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 88/100 RapTime: 0:00:01.508918 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 89/100 RapTime: 0:00:01.483850 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 90/100 RapTime: 0:00:01.505645 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 91/100 RapTime: 0:00:01.491067 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 92/100 RapTime: 0:00:01.506148 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 93/100 RapTime: 0:00:01.472559 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 94/100 RapTime: 0:00:01.408823 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 95/100 RapTime: 0:00:01.385774 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 96/100 RapTime: 0:00:01.393987 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 97/100 RapTime: 0:00:01.380335 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 98/100 RapTime: 0:00:01.391138 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 99/100 RapTime: 0:00:01.414688 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 100/100 RapTime: 0:00:01.403801 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}