{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ppo_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/ppo_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "9aa65b78-355f-44c6-8bc2-8bc142940592"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'train'\n",
        "name = 'ppo'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_shape = 3\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain):\n",
        "        self.model = brain.model\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, v = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(3, p=act_p[0].numpy()), act_p, v\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model):\n",
        "        self.model = model\n",
        "        self.gamma = 0.9\n",
        "        self.c_gamma = 0.1\n",
        "\n",
        "        self.beta_1 = 0.1\n",
        "        self.beta_2 = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "        #r_t_batch = np.asarray([e[\"r_t\"] for e in experiences])\n",
        "        #v_batch = np.asarray([e[\"v\"] for e in experiences])\n",
        "        advantage = np.asarray([e[\"advantage\"] for e in experiences]).reshape((-1, 1))\n",
        "        prob_batch = np.asarray([e[\"prob\"] for e in experiences])\n",
        "\n",
        "        #r_t_batch = tf.constant(r_t_batch.reshape([128, 1]))\n",
        "        #v_batch = tf.constant(v_batch.reshape([128, 1]))\n",
        "\n",
        "        advantage = (advantage - np.mean(advantage)) / (np.std(advantage) + 1e-8)\n",
        "        onehot_actions = tf.one_hot(action_batch, 3).numpy()\n",
        "\n",
        "        old_pi = tf.reduce_sum(onehot_actions * prob_batch, axis=1, keepdims=True)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "\n",
        "            # advantage = advantage - tf.stop_gradient(v_batch)\n",
        "            advantage = advantage - tf.stop_gradient(v)\n",
        "\n",
        "            # π(a|s)とlog(π(a|s))を計算\n",
        "            # new_pi = tf.reduce_sum(onehot_actions * prob_batch, axis=1, keepdims=True)\n",
        "            new_pi = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            new_logpi = tf.math.log(tf.clip_by_value(new_pi, 1e-10, 1.0))\n",
        "\n",
        "            r_t_batch = new_pi / old_pi\n",
        "\n",
        "            losses_clip = self._losses_clip(advantage, r_t_batch)\n",
        "            losses_value = self._losses_value(advantage)\n",
        "            entropy = tf.reduce_sum(new_logpi * new_pi, axis=1, keepdims=True)\n",
        "           \n",
        "            # total_loss = self.beta_1 * losses_value - self.beta_2 * entropy\n",
        "            # total_loss = - losses_clip - self.beta_2 * entropy\n",
        "            # total_loss = - losses_clip + self.beta_1 * losses_value\n",
        "            # total_loss = - self.beta_2 * entropy\n",
        "            #total_loss = - losses_clip + self.beta_1 * losses_value - self.beta_2 * entropy\n",
        "            total_loss = - losses_clip + self.beta_1 * losses_value - self.beta_2 * entropy\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        # WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\n",
        "        # gradientsがnoneになる時がある。\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        # gradients, _ = tf.clip_by_global_norm(gradients, 0.5)\n",
        "        # self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        self.model.optimizer.apply_gradients((grad, var) for (grad, var) in \n",
        "                                             zip(gradients, self.model.trainable_variables) if grad is not None)\n",
        "\n",
        "    def _losses_value(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _losses_clip(self, advantage, r_t):\n",
        "\n",
        "        r_clip = tf.clip_by_value(r_t, 1 - self.c_gamma, 1 + self.c_gamma)\n",
        "\n",
        "        loss_unclipped = r_t * advantage\n",
        "        loss_clipped = r_clip * advantage\n",
        "\n",
        "        return tf.minimum(loss_clipped, loss_clipped)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1UX215aeoXq"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : List[List[float]] = field(default_factory=list)\n",
        "    action : List[int] = field(default_factory=list)\n",
        "    reward : List[float] = field(default_factory=list)\n",
        "    next_state : List[List[int]] = field(default_factory=list)\n",
        "    done : List[bool] = field(default_factory=list)\n",
        "    prob : List[List[float]] = field(default_factory=list)\n",
        "    v : List[float] = field(default_factory=list)\n",
        "    advantage : List[float] = field(default_factory=list)\n",
        "    model : str = None\n",
        "    gae_lambda: float = 0.9\n",
        "    gamma: float = 0.9\n",
        "\n",
        "    def random_experiences(self, batch_size):\n",
        "        max_size = len(self.state) - 1\n",
        "        self._discounted_return(max_size)\n",
        "        batch_num = self._random_num(1, max_size, batch_size)\n",
        "        experiences = []\n",
        "        for i in batch_num:\n",
        "            experiences.append({\"state\": self.state[i], \"action\": self.action[i], \n",
        "                                \"reward\": self.reward[i], \"next_state\": self.next_state[i], \n",
        "                                \"done\": self.done[i], \"prob\": self.prob[i], \"advantage\": self.advantage[i]\n",
        "                                , \"v\": self.v[i], \"r_t\": self.v[i] / self.v[i-1]})\n",
        "        return experiences\n",
        "\n",
        "    def _random_num(self, a, b, k):\n",
        "        ns = []\n",
        "        while len(ns) < k:\n",
        "            n = random.randint(a, b)\n",
        "            if not n in ns:\n",
        "                ns.append(n)\n",
        "        return ns\n",
        "\n",
        "    def _discounted_return(self,max_size):\n",
        "        n_v = copy.deepcopy(self.v)\n",
        "        n_v.pop(0)\n",
        "        _, n_v_last = self.model(self.next_state[max_size].reshape((1,-1)))\n",
        "        n_v_last2 = n_v_last.numpy()\n",
        "        n_v.append(n_v_last2[0][0])\n",
        "\n",
        "        for i in range(len(self.reward)):\n",
        "            gae = 0\n",
        "            t = 0\n",
        "            for j in range(i, len(self.reward)):\n",
        "                delta = self.reward[j] + self.gamma * n_v[j] - self.v[j]\n",
        "                gae += ((self.gae_lambda * self.gamma) ** t) * delta\n",
        "                t += 1\n",
        "            self.advantage.append(gae)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, num, mdl_dir, name, batch_size = 128, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory(model = self.actor.model)\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action, p, v = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    memory.state.append(state)\n",
        "                    memory.action.append(action)\n",
        "                    memory.reward.append(reward)\n",
        "                    memory.next_state.append(next_state)\n",
        "                    memory.done.append(done)\n",
        "                    p2 = p.numpy()\n",
        "                    memory.prob.append(p2[0])\n",
        "                    v2 = v.numpy()\n",
        "                    memory.v.append(v2[0][0])\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                experiences = memory.random_experiences(self.batch_size)\n",
        "                self.critic.valuenetwork(experiences)\n",
        "\n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgv85YlVOaum",
        "outputId": "4aa3d9cc-e6b2-4346-fb15-c3b717ee9e49"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 50\n",
        "batch_size = 128\n",
        "\n",
        "brain = ParameterServer()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    model = brain.model\n",
        "    actor = Actor(brain)\n",
        "    critic = Critic(model)\n",
        "    main = Main(env, actor, critic, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:00:10.054852 FixedProfit: 1060573\n",
            "Episode: 1/50 RapTime: 0:00:10.060535 FixedProfit: 1059218\n",
            "Episode: 1/50 RapTime: 0:00:10.058518 FixedProfit: 1352989\n",
            "Episode: 1/50 RapTime: 0:00:10.063666 FixedProfit: 1040347\n",
            "Episode: 2/50 RapTime: 0:00:09.758302 FixedProfit: 1026867\n",
            "Episode: 2/50 RapTime: 0:00:09.766600 FixedProfit: 1183541\n",
            "Episode: 2/50 RapTime: 0:00:09.802976 FixedProfit: 1146195\n",
            "Episode: 2/50 RapTime: 0:00:09.830061 FixedProfit: 1036926\n",
            "Episode: 3/50 RapTime: 0:00:09.771074 FixedProfit: 1208915\n",
            "Episode: 3/50 RapTime: 0:00:09.885542 FixedProfit: 991884\n",
            "Episode: 3/50 RapTime: 0:00:09.846952 FixedProfit: 1015272\n",
            "Episode: 3/50 RapTime: 0:00:09.913828 FixedProfit: 1051787\n",
            "Episode: 4/50 RapTime: 0:00:09.626450 FixedProfit: 1217915\n",
            "Episode: 4/50 RapTime: 0:00:09.829516 FixedProfit: 1079647\n",
            "Episode: 4/50 RapTime: 0:00:09.872165 FixedProfit: 997750\n",
            "Episode: 4/50 RapTime: 0:00:09.792125 FixedProfit: 1255443\n",
            "Episode: 5/50 RapTime: 0:00:09.869875 FixedProfit: 981849\n",
            "Episode: 5/50 RapTime: 0:00:09.743694 FixedProfit: 1368690\n",
            "Episode: 5/50 RapTime: 0:00:09.771635 FixedProfit: 1022508\n",
            "Episode: 5/50 RapTime: 0:00:09.765771 FixedProfit: 1152018\n",
            "Episode: 6/50 RapTime: 0:00:09.795532 FixedProfit: 1128707\n",
            "Episode: 6/50 RapTime: 0:00:09.861681 FixedProfit: 1243927\n",
            "Episode: 6/50 RapTime: 0:00:09.800024 FixedProfit: 1061621\n",
            "Episode: 6/50 RapTime: 0:00:09.796149 FixedProfit: 1217120\n",
            "Episode: 7/50 RapTime: 0:00:09.771173 FixedProfit: 1334010\n",
            "Episode: 7/50 RapTime: 0:00:09.797100 FixedProfit: 1185503\n",
            "Episode: 7/50 RapTime: 0:00:09.788479 FixedProfit: 1092284\n",
            "Episode: 7/50 RapTime: 0:00:09.798497 FixedProfit: 1280137\n",
            "Episode: 8/50 RapTime: 0:00:09.785029 FixedProfit: 1188539Episode: 8/50 RapTime: 0:00:09.694822 FixedProfit: 1184789\n",
            "\n",
            "Episode: 8/50 RapTime: 0:00:09.780660 FixedProfit: 1074908\n",
            "Episode: 8/50 RapTime: 0:00:09.743508 FixedProfit: 1114547\n",
            "Episode: 9/50 RapTime: 0:00:09.706068 FixedProfit: 1198288Episode: 9/50 RapTime: 0:00:09.702031 FixedProfit: 1231262\n",
            "\n",
            "Episode: 9/50 RapTime: 0:00:09.719922 FixedProfit: 1162513\n",
            "Episode: 9/50 RapTime: 0:00:09.703427 FixedProfit: 1253871\n",
            "Episode: 10/50 RapTime: 0:00:09.653726 FixedProfit: 1193801\n",
            "Episode: 10/50 RapTime: 0:00:09.628177 FixedProfit: 1229355\n",
            "Episode: 10/50 RapTime: 0:00:09.675422 FixedProfit: 1145552\n",
            "Episode: 10/50 RapTime: 0:00:09.691633 FixedProfit: 1216106\n",
            "Episode: 11/50 RapTime: 0:00:09.535601 FixedProfit: 1198505\n",
            "Episode: 11/50 RapTime: 0:00:09.561772 FixedProfit: 1196593\n",
            "Episode: 11/50 RapTime: 0:00:09.589217 FixedProfit: 1188808Episode: 11/50 RapTime: 0:00:09.592836 FixedProfit: 1201829\n",
            "\n",
            "Episode: 12/50 RapTime: 0:00:08.939401 FixedProfit: 1197165\n",
            "Episode: 12/50 RapTime: 0:00:09.584901 FixedProfit: 1198893\n",
            "Episode: 12/50 RapTime: 0:00:09.594850 FixedProfit: 1197165\n",
            "Episode: 12/50 RapTime: 0:00:09.613128 FixedProfit: 1197165\n",
            "Episode: 13/50 RapTime: 0:00:09.973328 FixedProfit: 1197165\n",
            "Episode: 13/50 RapTime: 0:00:09.561718 FixedProfit: 1197165\n",
            "Episode: 13/50 RapTime: 0:00:09.573630 FixedProfit: 1197165\n",
            "Episode: 13/50 RapTime: 0:00:09.594926 FixedProfit: 1208128\n",
            "Episode: 14/50 RapTime: 0:00:09.786051 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:09.759357 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:09.769179 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:09.742135 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:09.344947 FixedProfit: 1184213\n",
            "Episode: 15/50 RapTime: 0:00:09.601230 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:09.831530 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:09.850526 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:09.156018 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:09.538344 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:09.684741 FixedProfit: 1208128\n",
            "Episode: 16/50 RapTime: 0:00:09.640050 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:09.314243 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:09.493786 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:09.629575 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:09.658941 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:09.279871 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:08.828941 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:09.643982 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:09.638933 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:09.628884 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:09.509414 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:09.604121 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:09.678552 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:09.242518 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:09.578201 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:09.645665 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:09.650962 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:09.607154 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:09.648972 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:09.540702 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:09.548958 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:09.461285 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:09.513533 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:09.572194 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:09.525159 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:09.547312 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:09.667063 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:09.664358 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:09.625199 FixedProfit: 1208128\n",
            "Episode: 24/50 RapTime: 0:00:09.647166 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:10.156750 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:09.573236 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:09.592497 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:09.792585 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:09.539317 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:09.523769 FixedProfit: 1197165Episode: 25/50 RapTime: 0:00:09.559087 FixedProfit: 1197165\n",
            "\n",
            "Episode: 26/50 RapTime: 0:00:09.437506 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:09.600715 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:09.612048 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:09.705220 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:09.645608 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:09.853878 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:09.563960 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:09.631368 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:09.258113 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:08.973882 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:09.596836 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:09.559240 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:09.572463 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:09.126963 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:09.536662 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:09.501762 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:09.740648 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:09.785399 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:09.555256 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:09.569203 FixedProfit: 1215077\n",
            "Episode: 31/50 RapTime: 0:00:09.319402 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:09.721831 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:09.578342 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:09.606966 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:09.299003 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:09.513133 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:09.634817 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:09.603066 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:09.605534 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:10.351987 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:09.571561 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:09.568606 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:09.340878 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:08.912711 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:09.604783 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:09.653637 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:09.087573 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:10.134737 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:09.449620 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:09.832615 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:09.494765 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:09.541578 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:09.641834 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:09.551745 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:09.576178 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:09.629818 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:09.149276 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:09.623773 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:09.626170 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:09.432437 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:09.629222 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:09.612230 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:09.794663 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:09.554072 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:09.571049 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:09.562780 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:09.388148 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:09.612196 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:09.562399 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:09.677265 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:09.534596 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:09.592753 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:09.594266 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:09.558210 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:10.128426 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:09.541335 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:09.597768 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:09.502334 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:08.934463 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:09.614881 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:09.616770 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:09.540699 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:09.774523 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:09.596997 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:09.654007 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:09.601404 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:10.214882 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:09.371128 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:09.434941 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:09.539814 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:10.144253 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:09.633235 FixedProfit: 1208128\n",
            "Episode: 46/50 RapTime: 0:00:09.633060 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:09.713811 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:09.865490 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:09.935015 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:09.874211 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:09.858859 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:09.524639 FixedProfit: 1197165Episode: 48/50 RapTime: 0:00:09.540380 FixedProfit: 1197165\n",
            "\n",
            "Episode: 48/50 RapTime: 0:00:09.530925 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:09.531602 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:09.524850 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:09.548913 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:09.551372 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:09.591072 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:09.574038 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:09.727736 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:09.724132 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:09.469147 FixedProfit: 1197165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}