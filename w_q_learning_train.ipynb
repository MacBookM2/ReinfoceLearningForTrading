{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w_q_learning_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMd5ob5idnTFeQoBAkO02B6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/w_q_learning_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "1a083039-776f-4829-a6eb-730604390063"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'w_qlearning_train.csv'\n",
        "\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "rewards_folder = '/content/drive/My Drive/' + exp_dir + 'rl_rewards'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd-UoFozKs9l"
      },
      "source": [
        "def make_scaler(env):\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, n_hidden_layers=1, hidden_dim=32):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "\n",
        "        n_mid = 3\n",
        "        n_state = 3\n",
        "        n_action = 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "\n",
        "        model_2 = Sequential()\n",
        "        model_2.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model_2.add(ReLU()) \n",
        "        model_2.add(Dense(n_mid))\n",
        "        model_2.add(ReLU()) \n",
        "        model_2.add(Dense(n_action))\n",
        "        model_2.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model_2.summary()))\n",
        "        self.model_2 = model_2\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done, s_flag):\n",
        "\n",
        "        next_act_values = self.model.predict(next_state,s_flag)\n",
        "        next_action =np.argmax(next_act_values[0])\n",
        "\n",
        "        if s_flag == 11:\n",
        "            q = self.model.predict(state)  \n",
        "            next_q = self.model_2.predict(next_state)\n",
        "            t = np.copy(q)\n",
        "\n",
        "            if done:\n",
        "                t[:, action] = reward\n",
        "            else:\n",
        "                t[:, action] = reward + self.gamma*np.max(next_q, axis=1)\n",
        "            self.model.train_on_batch(state, t)\n",
        "        else:\n",
        "            q = self.model_2.predict(state)  \n",
        "            next_q = self.model.predict(next_state)\n",
        "            t = np.copy(q)\n",
        "\n",
        "            if done:\n",
        "                t[:, action] = reward\n",
        "            else:\n",
        "                t[:, action] = reward + self.gamma*np.max(next_q, axis=1)\n",
        "            self.model_2.train_on_batch(state, t)\n",
        "\n",
        "    def predict(self, state, s_flag = 12):\n",
        "        values = None\n",
        "        q1 = self.model.predict(state)\n",
        "        q2 = self.model_2.predict(state)\n",
        "        if s_flag == 12:\n",
        "            values = np.array([q1[0,a] + q2[0,a] for a in range(2)])\n",
        "        elif s_flag == 11:\n",
        "            values = np.array([q1[0,a] + q1[0,a] for a in range(2)])\n",
        "        else:\n",
        "            values = np.array([q2[0,a] + q2[0,a] for a in range(2)])\n",
        "        return values\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.model.load_weights(name)\n",
        "        self.model_2.load_weights(name2)\n",
        "\n",
        "    def save(self, name, name2):\n",
        "        self.model.save_weights(name)\n",
        "        self.model_2.save_weights(name2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, brain, state_size=3, action_size=3):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.brain = brain\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "\n",
        "    def act(self, state,s_flag=12):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_size)\n",
        "        act_values = self.brain.predict(state,s_flag)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done, s_flag):\n",
        "        self.brain.train(state, action, reward, next_state, done, s_flag)\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.brain.load(name, name2)\n",
        "\n",
        "    def save(self, name,name2):\n",
        "        self.brain.save(name, name2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "def play_game(env, agent , episodes_times = 1000, mode = 'test', batch_size = 32):\n",
        "    if mode == 'test':\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "    else:\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        done = False\n",
        "        start_time = datetime.now()\n",
        "       \n",
        "        while not done:\n",
        "            s_flag = 12\n",
        "            action = agent.act(state,s_flag)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "\n",
        "            if mode == 'train':\n",
        "                rand = np.random.random()\n",
        "                if rand <= 0.5:\n",
        "                    s_flag = 11\n",
        "                else:\n",
        "                    s_flag = 22\n",
        "                agent.train(state, action, reward, next_state, done, s_flag)\n",
        "            \n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f} TradeTimes: {info['trade_time']} TradeWin: {info['trade_win']}\")\n",
        "        else:\n",
        "            record = pd.Series(info['cur_revenue'], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f}\")\n",
        "    \n",
        "        state = next_state\n",
        "        df_rec = df_rec.append(record, ignore_index=True)\n",
        "    return df_rec"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "03586f12-0a51-4bf5-c0ad-9a9d3a129392"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "mode = 'train'\n",
        "brain = Brain()\n",
        "agent = Agent(brain=brain)\n",
        "\n",
        "if mode == 'test':\n",
        "    with open(f'{models_folder}/scaler_w_ql.pkl', 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    agent.epsilon = 0.01\n",
        "    agent.load(f'{models_folder}/w_ql.h5',f'{models_folder}/w_ql2.h5')\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "scaler = make_scaler(env)\n",
        "df_rec = play_game(env, agent, episodes_times = episodes_times, mode = mode, batch_size = batch_size)\n",
        "\n",
        "if mode == 'train':\n",
        "    agent.save(f'{models_folder}/w_ql.h5',f'{models_folder}/w_ql2.h5')\n",
        "    with open(f'{models_folder}/scaler_w_ql.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "df_rec.to_csv(csv_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/100 RapTime: 0:01:03.686707 FixedProfit: 1148267\n",
            "Episode: 2/100 RapTime: 0:01:00.414553 FixedProfit: 1127613\n",
            "Episode: 3/100 RapTime: 0:01:00.197372 FixedProfit: 1205310\n",
            "Episode: 4/100 RapTime: 0:00:59.358601 FixedProfit: 1091176\n",
            "Episode: 5/100 RapTime: 0:00:59.467240 FixedProfit: 896533\n",
            "Episode: 6/100 RapTime: 0:00:59.717974 FixedProfit: 887070\n",
            "Episode: 7/100 RapTime: 0:01:01.079216 FixedProfit: 1132684\n",
            "Episode: 8/100 RapTime: 0:00:59.309032 FixedProfit: 1036299\n",
            "Episode: 9/100 RapTime: 0:00:59.477942 FixedProfit: 1194570\n",
            "Episode: 10/100 RapTime: 0:00:59.852445 FixedProfit: 1104388\n",
            "Episode: 11/100 RapTime: 0:01:00.907231 FixedProfit: 1019866\n",
            "Episode: 12/100 RapTime: 0:01:02.351330 FixedProfit: 1046532\n",
            "Episode: 13/100 RapTime: 0:00:59.496975 FixedProfit: 1272537\n",
            "Episode: 14/100 RapTime: 0:00:59.739215 FixedProfit: 1138515\n",
            "Episode: 15/100 RapTime: 0:01:01.073842 FixedProfit: 1061231\n",
            "Episode: 16/100 RapTime: 0:00:59.754576 FixedProfit: 1104170\n",
            "Episode: 17/100 RapTime: 0:01:00.039585 FixedProfit: 931724\n",
            "Episode: 18/100 RapTime: 0:01:01.365534 FixedProfit: 1127796\n",
            "Episode: 19/100 RapTime: 0:00:59.629750 FixedProfit: 943405\n",
            "Episode: 20/100 RapTime: 0:00:59.271313 FixedProfit: 1133449\n",
            "Episode: 21/100 RapTime: 0:00:59.453918 FixedProfit: 1025939\n",
            "Episode: 22/100 RapTime: 0:01:00.011168 FixedProfit: 1108639\n",
            "Episode: 23/100 RapTime: 0:01:01.973756 FixedProfit: 1246309\n",
            "Episode: 24/100 RapTime: 0:00:59.752748 FixedProfit: 1099133\n",
            "Episode: 25/100 RapTime: 0:01:00.006332 FixedProfit: 1173623\n",
            "Episode: 26/100 RapTime: 0:01:00.953529 FixedProfit: 1013423\n",
            "Episode: 27/100 RapTime: 0:01:03.377859 FixedProfit: 1116978\n",
            "Episode: 28/100 RapTime: 0:01:04.433990 FixedProfit: 1146180\n",
            "Episode: 29/100 RapTime: 0:01:04.539640 FixedProfit: 1216152\n",
            "Episode: 30/100 RapTime: 0:01:03.322179 FixedProfit: 988903\n",
            "Episode: 31/100 RapTime: 0:01:03.477995 FixedProfit: 985793\n",
            "Episode: 32/100 RapTime: 0:01:05.414537 FixedProfit: 991382\n",
            "Episode: 33/100 RapTime: 0:01:05.588235 FixedProfit: 1114371\n",
            "Episode: 34/100 RapTime: 0:01:03.315791 FixedProfit: 1071179\n",
            "Episode: 35/100 RapTime: 0:01:03.720621 FixedProfit: 1020691\n",
            "Episode: 36/100 RapTime: 0:01:03.409421 FixedProfit: 1078099\n",
            "Episode: 37/100 RapTime: 0:01:02.968432 FixedProfit: 967792\n",
            "Episode: 38/100 RapTime: 0:01:03.920316 FixedProfit: 973409\n",
            "Episode: 39/100 RapTime: 0:01:02.289072 FixedProfit: 873533\n",
            "Episode: 40/100 RapTime: 0:01:02.713498 FixedProfit: 1067187\n",
            "Episode: 41/100 RapTime: 0:01:02.803860 FixedProfit: 1078997\n",
            "Episode: 42/100 RapTime: 0:01:03.229438 FixedProfit: 1440051\n",
            "Episode: 43/100 RapTime: 0:01:02.844249 FixedProfit: 1251445\n",
            "Episode: 44/100 RapTime: 0:01:04.425266 FixedProfit: 1102870\n",
            "Episode: 45/100 RapTime: 0:01:02.988392 FixedProfit: 1059370\n",
            "Episode: 46/100 RapTime: 0:01:02.694482 FixedProfit: 1019294\n",
            "Episode: 47/100 RapTime: 0:01:02.874706 FixedProfit: 1123021\n",
            "Episode: 48/100 RapTime: 0:01:03.114495 FixedProfit: 1153379\n",
            "Episode: 49/100 RapTime: 0:01:05.054054 FixedProfit: 1056349\n",
            "Episode: 50/100 RapTime: 0:01:04.068859 FixedProfit: 1179490\n",
            "Episode: 51/100 RapTime: 0:01:04.892199 FixedProfit: 1124692\n",
            "Episode: 52/100 RapTime: 0:01:04.104815 FixedProfit: 893684\n",
            "Episode: 53/100 RapTime: 0:01:04.413185 FixedProfit: 1280278\n",
            "Episode: 54/100 RapTime: 0:01:04.451773 FixedProfit: 1048836\n",
            "Episode: 55/100 RapTime: 0:01:03.612517 FixedProfit: 1163154\n",
            "Episode: 56/100 RapTime: 0:01:04.334013 FixedProfit: 1333282\n",
            "Episode: 57/100 RapTime: 0:01:03.826355 FixedProfit: 966333\n",
            "Episode: 58/100 RapTime: 0:01:03.910280 FixedProfit: 1060659\n",
            "Episode: 59/100 RapTime: 0:01:06.153380 FixedProfit: 1115738\n",
            "Episode: 60/100 RapTime: 0:01:05.333181 FixedProfit: 1112949\n",
            "Episode: 61/100 RapTime: 0:01:05.024183 FixedProfit: 1283329\n",
            "Episode: 62/100 RapTime: 0:01:05.191286 FixedProfit: 901319\n",
            "Episode: 63/100 RapTime: 0:01:05.055629 FixedProfit: 1311312\n",
            "Episode: 64/100 RapTime: 0:01:04.936627 FixedProfit: 1142711\n",
            "Episode: 65/100 RapTime: 0:01:05.360048 FixedProfit: 1181142\n",
            "Episode: 66/100 RapTime: 0:01:05.531844 FixedProfit: 989910\n",
            "Episode: 67/100 RapTime: 0:01:05.274384 FixedProfit: 1201234\n",
            "Episode: 68/100 RapTime: 0:01:04.930503 FixedProfit: 1148273\n",
            "Episode: 69/100 RapTime: 0:01:06.886993 FixedProfit: 1280720\n",
            "Episode: 70/100 RapTime: 0:01:06.740262 FixedProfit: 1205640\n",
            "Episode: 71/100 RapTime: 0:01:06.704053 FixedProfit: 1258014\n",
            "Episode: 72/100 RapTime: 0:01:06.385638 FixedProfit: 1002244\n",
            "Episode: 73/100 RapTime: 0:01:05.951074 FixedProfit: 892389\n",
            "Episode: 74/100 RapTime: 0:01:06.359727 FixedProfit: 1131991\n",
            "Episode: 75/100 RapTime: 0:01:06.557152 FixedProfit: 1101656\n",
            "Episode: 76/100 RapTime: 0:01:05.995036 FixedProfit: 1080131\n",
            "Episode: 77/100 RapTime: 0:01:06.734893 FixedProfit: 1231633\n",
            "Episode: 78/100 RapTime: 0:01:06.569380 FixedProfit: 1109855\n",
            "Episode: 79/100 RapTime: 0:01:07.818485 FixedProfit: 1084904\n",
            "Episode: 80/100 RapTime: 0:01:06.864129 FixedProfit: 1074618\n",
            "Episode: 81/100 RapTime: 0:01:06.675841 FixedProfit: 1247363\n",
            "Episode: 82/100 RapTime: 0:01:06.361320 FixedProfit: 1094076\n",
            "Episode: 83/100 RapTime: 0:01:06.363271 FixedProfit: 1205647\n",
            "Episode: 84/100 RapTime: 0:01:06.394227 FixedProfit: 1053452\n",
            "Episode: 85/100 RapTime: 0:01:06.304257 FixedProfit: 1459945\n",
            "Episode: 86/100 RapTime: 0:01:04.637875 FixedProfit: 1115715\n",
            "Episode: 87/100 RapTime: 0:01:05.955371 FixedProfit: 1085339\n",
            "Episode: 88/100 RapTime: 0:01:05.494405 FixedProfit: 1100173\n",
            "Episode: 89/100 RapTime: 0:01:07.968615 FixedProfit: 1024197\n",
            "Episode: 90/100 RapTime: 0:01:06.143190 FixedProfit: 1084037\n",
            "Episode: 91/100 RapTime: 0:01:06.167807 FixedProfit: 1255978\n",
            "Episode: 92/100 RapTime: 0:01:06.557753 FixedProfit: 1063619\n",
            "Episode: 93/100 RapTime: 0:01:06.379451 FixedProfit: 909524\n",
            "Episode: 94/100 RapTime: 0:01:06.329657 FixedProfit: 1150179\n",
            "Episode: 95/100 RapTime: 0:01:06.591526 FixedProfit: 1142937\n",
            "Episode: 96/100 RapTime: 0:01:05.551649 FixedProfit: 917352\n",
            "Episode: 97/100 RapTime: 0:01:02.443822 FixedProfit: 1232348\n",
            "Episode: 98/100 RapTime: 0:01:02.812907 FixedProfit: 1074332\n",
            "Episode: 99/100 RapTime: 0:01:03.489249 FixedProfit: 1058078\n",
            "Episode: 100/100 RapTime: 0:01:01.883551 FixedProfit: 1087730\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}