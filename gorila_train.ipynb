{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gorila_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNCNvxaM1JrwXDcpF1rvWv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/gorila_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "4416962e-ee5f-4c6d-a1a3-67571d1b1a66"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'gorila_train.csv'\n",
        "\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "rewards_folder = '/content/drive/My Drive/' + exp_dir + 'rl_rewards'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=1000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "class ReplayMemory:\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "\n",
        "    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.acts_buf = np.zeros(size, dtype=np.uint8)\n",
        "    self.rews_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.done_buf = np.zeros(size, dtype=np.uint8)\n",
        "    self.ptr, self.size, self.max_size = 0, 0, size\n",
        "\n",
        "  def store(self, obs, act, rew, next_obs, done):\n",
        "    self.obs1_buf[self.ptr] = obs\n",
        "    self.obs2_buf[self.ptr] = next_obs\n",
        "    self.acts_buf[self.ptr] = act\n",
        "    self.rews_buf[self.ptr] = rew\n",
        "    self.done_buf[self.ptr] = done\n",
        "    self.ptr = (self.ptr+1) % self.max_size\n",
        "    self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "  def sample_batch(self, batch_size=32):\n",
        "    idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "    return dict(s=self.obs1_buf[idxs],\n",
        "                s2=self.obs2_buf[idxs],\n",
        "                a=self.acts_buf[idxs],\n",
        "                r=self.rews_buf[idxs],\n",
        "                d=self.done_buf[idxs])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1RNmtkaZ2W"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self, n_hidden_layers=1, hidden_dim=32):\n",
        "\n",
        "        n_mid = 3\n",
        "        n_state = 3\n",
        "        n_action = 3\n",
        "\n",
        "        mastermodel = Sequential()\n",
        "        mastermodel.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_mid))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_action))\n",
        "        mastermodel.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((mastermodel.summary()))\n",
        "        self.mastermodel = mastermodel\n",
        "    \n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_hidden_layers=1, hidden_dim=32):\n",
        "\n",
        "        n_mid = 3\n",
        "        n_state = 3\n",
        "        n_action = 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULBV5XpsbOjq"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, state_size, action_size, brain, memory):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = memory\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = brain.model\n",
        "        self.brain = brain\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.memory.store(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size=32):\n",
        "        if self.memory.size < batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = self.memory.sample_batch(batch_size)\n",
        "        states = minibatch['s']\n",
        "        actions = minibatch['a']\n",
        "        rewards = minibatch['r']\n",
        "        next_states = minibatch['s2']\n",
        "        done = minibatch['d']\n",
        "\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.model.predict(states)\n",
        "\n",
        "        target_full[np.arange(batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "def play_game(env, agent, scaler, episodes_times = 25, batch_size = 32, mode = 'test'):\n",
        "\n",
        "    agent.layering()\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        done = False\n",
        "        start_time = datetime.now()\n",
        "       \n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "\n",
        "            if mode == 'train':\n",
        "                agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                agent.replay(batch_size)\n",
        "\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            agent.integration()\n",
        "            agent.layering()\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)\n",
        "        \n",
        "        state = next_state"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "5f0e0e61-96ce-4b05-d9ec-6fbb3cd6adc6"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 25\n",
        "batch_size = 32\n",
        "mode = 'train'\n",
        "state_size = 3\n",
        "action_size = 3\n",
        "\n",
        "masterbrain = ParameterServer()\n",
        "\n",
        "if mode == 'test':\n",
        "    masterbrain.load(f'{models_folder}/gorila_model.h5')\n",
        "\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "        print(row, file=f)\n",
        "else:\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit'\n",
        "        print(row, file=f)\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    e = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    memory = ReplayMemory(state_size, action_size, size=500)\n",
        "    a = Agent(state_size, action_size, brain, memory)\n",
        "    if mode == 'test':\n",
        "        a.epsilon = 0.01\n",
        "    s = make_scaler(e)\n",
        "    arr = [e,a,s]\n",
        "    envs.append(arr)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: play_game(env[0],env[1],env[2], episodes_times, batch_size, mode)\n",
        "        datas.append(executor.submit(job))\n",
        "\n",
        "if mode == 'train':\n",
        "    masterbrain.save(f'{models_folder}/gorila_model.h5')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_5 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_6 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_7 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_8 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_9 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/25 RapTime: 0:03:38.223079 FixedProfit: 1228967\n",
            "Episode: 1/25 RapTime: 0:03:40.041501 FixedProfit: 1148130\n",
            "Episode: 1/25 RapTime: 0:03:42.119701 FixedProfit: 1174502\n",
            "Episode: 1/25 RapTime: 0:03:43.820297 FixedProfit: 938253\n",
            "Episode: 2/25 RapTime: 0:04:21.047158 FixedProfit: 1211357\n",
            "Episode: 2/25 RapTime: 0:04:23.387936 FixedProfit: 1191362\n",
            "Episode: 2/25 RapTime: 0:04:21.704337 FixedProfit: 1182904\n",
            "Episode: 2/25 RapTime: 0:04:25.268934 FixedProfit: 990041\n",
            "Episode: 3/25 RapTime: 0:04:24.242876 FixedProfit: 1202975\n",
            "Episode: 3/25 RapTime: 0:04:20.907090 FixedProfit: 1197165\n",
            "Episode: 3/25 RapTime: 0:04:23.810627 FixedProfit: 1197165\n",
            "Episode: 3/25 RapTime: 0:04:21.604831 FixedProfit: 1165931\n",
            "Episode: 4/25 RapTime: 0:04:27.551278 FixedProfit: 1197165\n",
            "Episode: 4/25 RapTime: 0:04:30.373053 FixedProfit: 1207534\n",
            "Episode: 4/25 RapTime: 0:04:28.427425 FixedProfit: 1194368\n",
            "Episode: 4/25 RapTime: 0:04:27.617609 FixedProfit: 1201871\n",
            "Episode: 5/25 RapTime: 0:04:33.004911 FixedProfit: 1183641\n",
            "Episode: 5/25 RapTime: 0:04:35.299665 FixedProfit: 1199543\n",
            "Episode: 5/25 RapTime: 0:04:34.986695 FixedProfit: 1215549\n",
            "Episode: 5/25 RapTime: 0:04:35.213779 FixedProfit: 1171053\n",
            "Episode: 6/25 RapTime: 0:04:34.291609 FixedProfit: 1197165\n",
            "Episode: 6/25 RapTime: 0:04:39.081472 FixedProfit: 1189025\n",
            "Episode: 6/25 RapTime: 0:04:43.310070 FixedProfit: 1198723\n",
            "Episode: 6/25 RapTime: 0:04:39.621593 FixedProfit: 1199299\n",
            "Episode: 7/25 RapTime: 0:04:35.837756 FixedProfit: 1191043\n",
            "Episode: 7/25 RapTime: 0:04:35.019036 FixedProfit: 1223132\n",
            "Episode: 7/25 RapTime: 0:04:38.354435 FixedProfit: 1197165\n",
            "Episode: 7/25 RapTime: 0:04:39.251754 FixedProfit: 1201770\n",
            "Episode: 8/25 RapTime: 0:04:50.057959 FixedProfit: 1204085\n",
            "Episode: 8/25 RapTime: 0:04:51.651192 FixedProfit: 1217691\n",
            "Episode: 8/25 RapTime: 0:04:54.890310 FixedProfit: 1203236\n",
            "Episode: 8/25 RapTime: 0:04:48.579317 FixedProfit: 1197165\n",
            "Episode: 9/25 RapTime: 0:05:09.443534 FixedProfit: 1197165\n",
            "Episode: 9/25 RapTime: 0:05:04.889418 FixedProfit: 1196981\n",
            "Episode: 9/25 RapTime: 0:05:04.867599 FixedProfit: 1227981\n",
            "Episode: 9/25 RapTime: 0:05:07.643202 FixedProfit: 1197165\n",
            "Episode: 10/25 RapTime: 0:05:13.964855 FixedProfit: 1186846\n",
            "Episode: 10/25 RapTime: 0:05:08.585945 FixedProfit: 1175176\n",
            "Episode: 10/25 RapTime: 0:05:13.004885 FixedProfit: 1165206\n",
            "Episode: 10/25 RapTime: 0:05:15.834027 FixedProfit: 1203866\n",
            "Episode: 11/25 RapTime: 0:05:14.670322 FixedProfit: 1207506\n",
            "Episode: 11/25 RapTime: 0:05:17.758966 FixedProfit: 1193478\n",
            "Episode: 11/25 RapTime: 0:05:16.151027 FixedProfit: 1208128\n",
            "Episode: 11/25 RapTime: 0:05:16.518203 FixedProfit: 1188552\n",
            "Episode: 12/25 RapTime: 0:05:24.395030 FixedProfit: 1205181\n",
            "Episode: 12/25 RapTime: 0:05:28.149799 FixedProfit: 1197165\n",
            "Episode: 12/25 RapTime: 0:05:26.005002 FixedProfit: 1197714\n",
            "Episode: 12/25 RapTime: 0:05:23.365289 FixedProfit: 1189423\n",
            "Episode: 13/25 RapTime: 0:05:29.805796 FixedProfit: 1208804\n",
            "Episode: 13/25 RapTime: 0:05:24.873236 FixedProfit: 1198933\n",
            "Episode: 13/25 RapTime: 0:05:27.537547 FixedProfit: 1196844\n",
            "Episode: 13/25 RapTime: 0:05:29.562419 FixedProfit: 1197165\n",
            "Episode: 14/25 RapTime: 0:05:32.011039 FixedProfit: 1198487\n",
            "Episode: 14/25 RapTime: 0:05:37.446405 FixedProfit: 1194847\n",
            "Episode: 14/25 RapTime: 0:05:33.857086 FixedProfit: 1197165\n",
            "Episode: 14/25 RapTime: 0:05:33.337876 FixedProfit: 1184069\n",
            "Episode: 15/25 RapTime: 0:05:30.140486 FixedProfit: 1195952\n",
            "Episode: 15/25 RapTime: 0:05:30.924882 FixedProfit: 1197812\n",
            "Episode: 15/25 RapTime: 0:05:33.949902 FixedProfit: 1245524\n",
            "Episode: 15/25 RapTime: 0:05:29.232680 FixedProfit: 1205026\n",
            "Episode: 16/25 RapTime: 0:05:47.656889 FixedProfit: 1197165\n",
            "Episode: 16/25 RapTime: 0:05:48.139499 FixedProfit: 1198487\n",
            "Episode: 16/25 RapTime: 0:05:47.880998 FixedProfit: 1196237\n",
            "Episode: 16/25 RapTime: 0:05:47.304059 FixedProfit: 1192956\n",
            "Episode: 17/25 RapTime: 0:05:51.474913 FixedProfit: 1159668\n",
            "Episode: 17/25 RapTime: 0:05:53.423693 FixedProfit: 1199412\n",
            "Episode: 17/25 RapTime: 0:05:49.524410 FixedProfit: 1189929\n",
            "Episode: 17/25 RapTime: 0:05:51.611581 FixedProfit: 1191621\n",
            "Episode: 18/25 RapTime: 0:05:55.517989 FixedProfit: 1153835\n",
            "Episode: 18/25 RapTime: 0:05:55.950882 FixedProfit: 1197937\n",
            "Episode: 18/25 RapTime: 0:06:03.319082 FixedProfit: 1203615\n",
            "Episode: 18/25 RapTime: 0:06:02.607484 FixedProfit: 1197342\n",
            "Episode: 19/25 RapTime: 0:05:54.635205 FixedProfit: 1188390\n",
            "Episode: 19/25 RapTime: 0:05:46.755609 FixedProfit: 1196671\n",
            "Episode: 19/25 RapTime: 0:05:46.811709 FixedProfit: 1197860\n",
            "Episode: 19/25 RapTime: 0:05:50.283823 FixedProfit: 1197165\n",
            "Episode: 20/25 RapTime: 0:06:12.073969 FixedProfit: 1150674\n",
            "Episode: 20/25 RapTime: 0:06:11.822426 FixedProfit: 1220049\n",
            "Episode: 20/25 RapTime: 0:06:13.908801 FixedProfit: 1186881\n",
            "Episode: 20/25 RapTime: 0:06:19.687178 FixedProfit: 1193221\n",
            "Episode: 21/25 RapTime: 0:06:14.683026 FixedProfit: 1188479\n",
            "Episode: 21/25 RapTime: 0:06:23.739202 FixedProfit: 1207506\n",
            "Episode: 21/25 RapTime: 0:06:23.636598 FixedProfit: 1172582\n",
            "Episode: 21/25 RapTime: 0:06:19.289379 FixedProfit: 1182136\n",
            "Episode: 22/25 RapTime: 0:06:19.530558 FixedProfit: 1219619\n",
            "Episode: 22/25 RapTime: 0:06:27.255190 FixedProfit: 1195583\n",
            "Episode: 22/25 RapTime: 0:06:25.829628 FixedProfit: 1208849\n",
            "Episode: 22/25 RapTime: 0:06:28.995462 FixedProfit: 1200809\n",
            "Episode: 23/25 RapTime: 0:06:31.362186 FixedProfit: 1191972\n",
            "Episode: 23/25 RapTime: 0:06:33.470024 FixedProfit: 1206940\n",
            "Episode: 23/25 RapTime: 0:06:36.991294 FixedProfit: 1182706\n",
            "Episode: 23/25 RapTime: 0:06:31.268448 FixedProfit: 1197165\n",
            "Episode: 24/25 RapTime: 0:06:36.408620 FixedProfit: 1197565\n",
            "Episode: 24/25 RapTime: 0:06:38.416773 FixedProfit: 1198816\n",
            "Episode: 24/25 RapTime: 0:06:40.046751 FixedProfit: 1194403\n",
            "Episode: 24/25 RapTime: 0:06:46.266653 FixedProfit: 1184874\n",
            "Episode: 25/25 RapTime: 0:06:30.075751 FixedProfit: 1206001\n",
            "Episode: 25/25 RapTime: 0:06:27.243510 FixedProfit: 1218845\n",
            "Episode: 25/25 RapTime: 0:06:27.012983 FixedProfit: 1197012\n",
            "Episode: 25/25 RapTime: 0:06:20.760985 FixedProfit: 1194970\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}