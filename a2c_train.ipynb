{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a2c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "6f522848-6229-4ee5-cd06-9a05ba37827c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "mode = 'train'\n",
        "name = 'a2c'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_shape = 3\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        Brain.model = model\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        Brain.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        Brain.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = Brain.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(3, p=act_p[0].numpy())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic(Brain):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "        self.beta  = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, 3)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = Brain.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, Brain.model.trainable_variables)\n",
        "\n",
        "        Brain.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, Brain.model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = Brain.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "            self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "        else:\n",
        "            self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "\n",
        "            total_reward = 0.0\n",
        "            experiences = []\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "                if mode == 'train':\n",
        "                    experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                    if len(experiences) == self.batch_size:\n",
        "                        self.critic.valuenetwork(experiences)\n",
        "                        experiences = []\n",
        "\n",
        "                state = next_state\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=self.df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, self.episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            else:\n",
        "                record = pd.Series(info['cur_revenue'], index=self.df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, self.episodes_times, play_time, info['cur_revenue']))\n",
        "\n",
        "            self.df_rec = self.df_rec.append(record, ignore_index=True)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "        self._save_csv()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "    def _save_csv(self):\n",
        "        self.df_rec.to_csv(csv_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ebec28-c139-4abd-99dc-7790c593eeea"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 200\n",
        "batch_size = 32\n",
        "\n",
        "actor = Actor()\n",
        "critic = Critic()\n",
        "env = Environment(df, initial_money, mode)\n",
        "main = Main(env, actor, critic, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/200 RapTime: 0:00:03.069630 FixedProfit: 1192672\n",
            "Episode: 2/200 RapTime: 0:00:01.650693 FixedProfit: 1170335\n",
            "Episode: 3/200 RapTime: 0:00:01.675208 FixedProfit: 1062440\n",
            "Episode: 4/200 RapTime: 0:00:01.656900 FixedProfit: 1270075\n",
            "Episode: 5/200 RapTime: 0:00:01.674611 FixedProfit: 1045727\n",
            "Episode: 6/200 RapTime: 0:00:01.665994 FixedProfit: 1261889\n",
            "Episode: 7/200 RapTime: 0:00:01.685150 FixedProfit: 1014014\n",
            "Episode: 8/200 RapTime: 0:00:01.705967 FixedProfit: 1097085\n",
            "Episode: 9/200 RapTime: 0:00:01.665175 FixedProfit: 1197951\n",
            "Episode: 10/200 RapTime: 0:00:01.704696 FixedProfit: 1162779\n",
            "Episode: 11/200 RapTime: 0:00:01.674014 FixedProfit: 1164892\n",
            "Episode: 12/200 RapTime: 0:00:01.672005 FixedProfit: 1237204\n",
            "Episode: 13/200 RapTime: 0:00:01.686341 FixedProfit: 1185430\n",
            "Episode: 14/200 RapTime: 0:00:01.659173 FixedProfit: 1077395\n",
            "Episode: 15/200 RapTime: 0:00:01.684086 FixedProfit: 956905\n",
            "Episode: 16/200 RapTime: 0:00:01.699694 FixedProfit: 1121851\n",
            "Episode: 17/200 RapTime: 0:00:01.626796 FixedProfit: 1071246\n",
            "Episode: 18/200 RapTime: 0:00:01.665349 FixedProfit: 1187913\n",
            "Episode: 19/200 RapTime: 0:00:01.664978 FixedProfit: 1212529\n",
            "Episode: 20/200 RapTime: 0:00:01.661176 FixedProfit: 1029073\n",
            "Episode: 21/200 RapTime: 0:00:01.671976 FixedProfit: 954211\n",
            "Episode: 22/200 RapTime: 0:00:01.650594 FixedProfit: 959697\n",
            "Episode: 23/200 RapTime: 0:00:01.649990 FixedProfit: 999734\n",
            "Episode: 24/200 RapTime: 0:00:01.667349 FixedProfit: 1155278\n",
            "Episode: 25/200 RapTime: 0:00:01.647915 FixedProfit: 1149385\n",
            "Episode: 26/200 RapTime: 0:00:01.662276 FixedProfit: 1243097\n",
            "Episode: 27/200 RapTime: 0:00:01.676575 FixedProfit: 1134843\n",
            "Episode: 28/200 RapTime: 0:00:01.655076 FixedProfit: 1231798\n",
            "Episode: 29/200 RapTime: 0:00:01.660521 FixedProfit: 1213588\n",
            "Episode: 30/200 RapTime: 0:00:01.657037 FixedProfit: 1197165\n",
            "Episode: 31/200 RapTime: 0:00:01.630380 FixedProfit: 1207622\n",
            "Episode: 32/200 RapTime: 0:00:01.665472 FixedProfit: 1197165\n",
            "Episode: 33/200 RapTime: 0:00:01.661430 FixedProfit: 1197165\n",
            "Episode: 34/200 RapTime: 0:00:01.634574 FixedProfit: 1213872\n",
            "Episode: 35/200 RapTime: 0:00:01.635105 FixedProfit: 1197165\n",
            "Episode: 36/200 RapTime: 0:00:01.633282 FixedProfit: 1208128\n",
            "Episode: 37/200 RapTime: 0:00:01.638031 FixedProfit: 1197165\n",
            "Episode: 38/200 RapTime: 0:00:01.650568 FixedProfit: 1208128\n",
            "Episode: 39/200 RapTime: 0:00:01.645394 FixedProfit: 1197165\n",
            "Episode: 40/200 RapTime: 0:00:01.622620 FixedProfit: 1197165\n",
            "Episode: 41/200 RapTime: 0:00:01.657231 FixedProfit: 1197165\n",
            "Episode: 42/200 RapTime: 0:00:01.637270 FixedProfit: 1208128\n",
            "Episode: 43/200 RapTime: 0:00:01.653862 FixedProfit: 1194143\n",
            "Episode: 44/200 RapTime: 0:00:01.688775 FixedProfit: 1194143\n",
            "Episode: 45/200 RapTime: 0:00:01.647873 FixedProfit: 1208128\n",
            "Episode: 46/200 RapTime: 0:00:01.658060 FixedProfit: 1238217\n",
            "Episode: 47/200 RapTime: 0:00:01.670205 FixedProfit: 1203247\n",
            "Episode: 48/200 RapTime: 0:00:01.692420 FixedProfit: 1173033\n",
            "Episode: 49/200 RapTime: 0:00:01.673207 FixedProfit: 1213438\n",
            "Episode: 50/200 RapTime: 0:00:01.663609 FixedProfit: 1197165\n",
            "Episode: 51/200 RapTime: 0:00:01.664651 FixedProfit: 1208128\n",
            "Episode: 52/200 RapTime: 0:00:01.682703 FixedProfit: 1173059\n",
            "Episode: 53/200 RapTime: 0:00:01.662401 FixedProfit: 1189729\n",
            "Episode: 54/200 RapTime: 0:00:01.694677 FixedProfit: 1171391\n",
            "Episode: 55/200 RapTime: 0:00:01.649723 FixedProfit: 1204009\n",
            "Episode: 56/200 RapTime: 0:00:01.649161 FixedProfit: 1187624\n",
            "Episode: 57/200 RapTime: 0:00:01.649772 FixedProfit: 1206720\n",
            "Episode: 58/200 RapTime: 0:00:01.649172 FixedProfit: 1144525\n",
            "Episode: 59/200 RapTime: 0:00:01.633751 FixedProfit: 1202186\n",
            "Episode: 60/200 RapTime: 0:00:01.659150 FixedProfit: 1216684\n",
            "Episode: 61/200 RapTime: 0:00:01.651503 FixedProfit: 1177373\n",
            "Episode: 62/200 RapTime: 0:00:01.663692 FixedProfit: 1198153\n",
            "Episode: 63/200 RapTime: 0:00:01.642426 FixedProfit: 1196323\n",
            "Episode: 64/200 RapTime: 0:00:01.666765 FixedProfit: 1195698\n",
            "Episode: 65/200 RapTime: 0:00:01.646444 FixedProfit: 1208255\n",
            "Episode: 66/200 RapTime: 0:00:01.630505 FixedProfit: 1197165\n",
            "Episode: 67/200 RapTime: 0:00:01.666149 FixedProfit: 1197165\n",
            "Episode: 68/200 RapTime: 0:00:01.649825 FixedProfit: 1197165\n",
            "Episode: 69/200 RapTime: 0:00:01.628625 FixedProfit: 1197165\n",
            "Episode: 70/200 RapTime: 0:00:01.660133 FixedProfit: 1199605\n",
            "Episode: 71/200 RapTime: 0:00:01.631385 FixedProfit: 1197165\n",
            "Episode: 72/200 RapTime: 0:00:01.639348 FixedProfit: 1197165\n",
            "Episode: 73/200 RapTime: 0:00:01.642423 FixedProfit: 1197165\n",
            "Episode: 74/200 RapTime: 0:00:01.669743 FixedProfit: 1197165\n",
            "Episode: 75/200 RapTime: 0:00:01.650971 FixedProfit: 1197165\n",
            "Episode: 76/200 RapTime: 0:00:01.676926 FixedProfit: 1197165\n",
            "Episode: 77/200 RapTime: 0:00:01.659880 FixedProfit: 1208128\n",
            "Episode: 78/200 RapTime: 0:00:01.641774 FixedProfit: 1197165\n",
            "Episode: 79/200 RapTime: 0:00:01.652258 FixedProfit: 1197165\n",
            "Episode: 80/200 RapTime: 0:00:01.661406 FixedProfit: 1197165\n",
            "Episode: 81/200 RapTime: 0:00:01.629762 FixedProfit: 1197165\n",
            "Episode: 82/200 RapTime: 0:00:01.667821 FixedProfit: 1197165\n",
            "Episode: 83/200 RapTime: 0:00:01.628990 FixedProfit: 1197165\n",
            "Episode: 84/200 RapTime: 0:00:01.619993 FixedProfit: 1181462\n",
            "Episode: 85/200 RapTime: 0:00:01.615627 FixedProfit: 1197165\n",
            "Episode: 86/200 RapTime: 0:00:01.646819 FixedProfit: 1199440\n",
            "Episode: 87/200 RapTime: 0:00:01.649663 FixedProfit: 1187078\n",
            "Episode: 88/200 RapTime: 0:00:01.666171 FixedProfit: 1197165\n",
            "Episode: 89/200 RapTime: 0:00:01.646425 FixedProfit: 1197165\n",
            "Episode: 90/200 RapTime: 0:00:01.626815 FixedProfit: 1197165\n",
            "Episode: 91/200 RapTime: 0:00:01.650611 FixedProfit: 1197165\n",
            "Episode: 92/200 RapTime: 0:00:01.666114 FixedProfit: 1197165\n",
            "Episode: 93/200 RapTime: 0:00:01.696096 FixedProfit: 1197165\n",
            "Episode: 94/200 RapTime: 0:00:01.710064 FixedProfit: 1197165\n",
            "Episode: 95/200 RapTime: 0:00:01.656132 FixedProfit: 1197165\n",
            "Episode: 96/200 RapTime: 0:00:01.643431 FixedProfit: 1197165\n",
            "Episode: 97/200 RapTime: 0:00:01.613844 FixedProfit: 1197165\n",
            "Episode: 98/200 RapTime: 0:00:01.616822 FixedProfit: 1197165\n",
            "Episode: 99/200 RapTime: 0:00:01.643076 FixedProfit: 1197165\n",
            "Episode: 100/200 RapTime: 0:00:01.622612 FixedProfit: 1208128\n",
            "Episode: 101/200 RapTime: 0:00:01.648964 FixedProfit: 1197165\n",
            "Episode: 102/200 RapTime: 0:00:01.621471 FixedProfit: 1208128\n",
            "Episode: 103/200 RapTime: 0:00:01.629477 FixedProfit: 1197165\n",
            "Episode: 104/200 RapTime: 0:00:01.625738 FixedProfit: 1197165\n",
            "Episode: 105/200 RapTime: 0:00:01.640922 FixedProfit: 1208128\n",
            "Episode: 106/200 RapTime: 0:00:01.673466 FixedProfit: 1197165\n",
            "Episode: 107/200 RapTime: 0:00:01.649774 FixedProfit: 1197165\n",
            "Episode: 108/200 RapTime: 0:00:01.625983 FixedProfit: 1197165\n",
            "Episode: 109/200 RapTime: 0:00:01.634245 FixedProfit: 1197165\n",
            "Episode: 110/200 RapTime: 0:00:01.657921 FixedProfit: 1197165\n",
            "Episode: 111/200 RapTime: 0:00:01.700205 FixedProfit: 1197165\n",
            "Episode: 112/200 RapTime: 0:00:01.629335 FixedProfit: 1197165\n",
            "Episode: 113/200 RapTime: 0:00:01.631183 FixedProfit: 1197165\n",
            "Episode: 114/200 RapTime: 0:00:01.636381 FixedProfit: 1197165\n",
            "Episode: 115/200 RapTime: 0:00:01.639271 FixedProfit: 1197165\n",
            "Episode: 116/200 RapTime: 0:00:01.619738 FixedProfit: 1197165\n",
            "Episode: 117/200 RapTime: 0:00:01.645836 FixedProfit: 1194143\n",
            "Episode: 118/200 RapTime: 0:00:01.659356 FixedProfit: 1197165\n",
            "Episode: 119/200 RapTime: 0:00:01.653283 FixedProfit: 1197165\n",
            "Episode: 120/200 RapTime: 0:00:01.639176 FixedProfit: 1208128\n",
            "Episode: 121/200 RapTime: 0:00:01.621557 FixedProfit: 1208128\n",
            "Episode: 122/200 RapTime: 0:00:01.628475 FixedProfit: 1197165\n",
            "Episode: 123/200 RapTime: 0:00:01.671141 FixedProfit: 1197165\n",
            "Episode: 124/200 RapTime: 0:00:01.664212 FixedProfit: 1197165\n",
            "Episode: 125/200 RapTime: 0:00:01.617905 FixedProfit: 1194143\n",
            "Episode: 126/200 RapTime: 0:00:01.649129 FixedProfit: 1197165\n",
            "Episode: 127/200 RapTime: 0:00:01.685731 FixedProfit: 1197165\n",
            "Episode: 128/200 RapTime: 0:00:01.669706 FixedProfit: 1208128\n",
            "Episode: 129/200 RapTime: 0:00:01.656218 FixedProfit: 1197165\n",
            "Episode: 130/200 RapTime: 0:00:01.683536 FixedProfit: 1208128\n",
            "Episode: 131/200 RapTime: 0:00:01.667772 FixedProfit: 1197165\n",
            "Episode: 132/200 RapTime: 0:00:01.659595 FixedProfit: 1197165\n",
            "Episode: 133/200 RapTime: 0:00:01.682642 FixedProfit: 1197165\n",
            "Episode: 134/200 RapTime: 0:00:01.665668 FixedProfit: 1208128\n",
            "Episode: 135/200 RapTime: 0:00:01.711512 FixedProfit: 1208128\n",
            "Episode: 136/200 RapTime: 0:00:01.646823 FixedProfit: 1197165\n",
            "Episode: 137/200 RapTime: 0:00:01.826938 FixedProfit: 1197165\n",
            "Episode: 138/200 RapTime: 0:00:01.628028 FixedProfit: 1173059\n",
            "Episode: 139/200 RapTime: 0:00:01.687288 FixedProfit: 1208128\n",
            "Episode: 140/200 RapTime: 0:00:01.659432 FixedProfit: 1208128\n",
            "Episode: 141/200 RapTime: 0:00:01.677242 FixedProfit: 1208128\n",
            "Episode: 142/200 RapTime: 0:00:01.681951 FixedProfit: 1208128\n",
            "Episode: 143/200 RapTime: 0:00:01.656986 FixedProfit: 1197165\n",
            "Episode: 144/200 RapTime: 0:00:01.662346 FixedProfit: 1208128\n",
            "Episode: 145/200 RapTime: 0:00:01.688138 FixedProfit: 1197165\n",
            "Episode: 146/200 RapTime: 0:00:01.683592 FixedProfit: 1197165\n",
            "Episode: 147/200 RapTime: 0:00:01.654794 FixedProfit: 1197165\n",
            "Episode: 148/200 RapTime: 0:00:01.670771 FixedProfit: 1194143\n",
            "Episode: 149/200 RapTime: 0:00:01.650484 FixedProfit: 1197165\n",
            "Episode: 150/200 RapTime: 0:00:01.665667 FixedProfit: 1197165\n",
            "Episode: 151/200 RapTime: 0:00:01.654689 FixedProfit: 1197165\n",
            "Episode: 152/200 RapTime: 0:00:01.654897 FixedProfit: 1208128\n",
            "Episode: 153/200 RapTime: 0:00:01.677263 FixedProfit: 1197165\n",
            "Episode: 154/200 RapTime: 0:00:01.707226 FixedProfit: 1208128\n",
            "Episode: 155/200 RapTime: 0:00:01.702126 FixedProfit: 1208128\n",
            "Episode: 156/200 RapTime: 0:00:01.689482 FixedProfit: 1194143\n",
            "Episode: 157/200 RapTime: 0:00:01.693372 FixedProfit: 1197165\n",
            "Episode: 158/200 RapTime: 0:00:01.685901 FixedProfit: 1194143\n",
            "Episode: 159/200 RapTime: 0:00:01.661644 FixedProfit: 1197165\n",
            "Episode: 160/200 RapTime: 0:00:01.676872 FixedProfit: 1208128\n",
            "Episode: 161/200 RapTime: 0:00:01.669809 FixedProfit: 1197165\n",
            "Episode: 162/200 RapTime: 0:00:01.690726 FixedProfit: 1197165\n",
            "Episode: 163/200 RapTime: 0:00:01.705780 FixedProfit: 1197165\n",
            "Episode: 164/200 RapTime: 0:00:01.699626 FixedProfit: 1197165\n",
            "Episode: 165/200 RapTime: 0:00:01.670805 FixedProfit: 1197165\n",
            "Episode: 166/200 RapTime: 0:00:01.683923 FixedProfit: 1197165\n",
            "Episode: 167/200 RapTime: 0:00:01.684498 FixedProfit: 1208128\n",
            "Episode: 168/200 RapTime: 0:00:01.667864 FixedProfit: 1197165\n",
            "Episode: 169/200 RapTime: 0:00:01.694120 FixedProfit: 1197165\n",
            "Episode: 170/200 RapTime: 0:00:01.656256 FixedProfit: 1173059\n",
            "Episode: 171/200 RapTime: 0:00:01.674222 FixedProfit: 1194143\n",
            "Episode: 172/200 RapTime: 0:00:01.659644 FixedProfit: 1197165\n",
            "Episode: 173/200 RapTime: 0:00:01.644821 FixedProfit: 1197165\n",
            "Episode: 174/200 RapTime: 0:00:01.672745 FixedProfit: 1197165\n",
            "Episode: 175/200 RapTime: 0:00:01.658828 FixedProfit: 1197165\n",
            "Episode: 176/200 RapTime: 0:00:01.640382 FixedProfit: 1197165\n",
            "Episode: 177/200 RapTime: 0:00:01.625877 FixedProfit: 1197165\n",
            "Episode: 178/200 RapTime: 0:00:01.635499 FixedProfit: 1197165\n",
            "Episode: 179/200 RapTime: 0:00:01.623416 FixedProfit: 1197165\n",
            "Episode: 180/200 RapTime: 0:00:01.631031 FixedProfit: 1197165\n",
            "Episode: 181/200 RapTime: 0:00:01.662234 FixedProfit: 1197165\n",
            "Episode: 182/200 RapTime: 0:00:01.632288 FixedProfit: 1197165\n",
            "Episode: 183/200 RapTime: 0:00:01.634577 FixedProfit: 1197165\n",
            "Episode: 184/200 RapTime: 0:00:01.656349 FixedProfit: 1197165\n",
            "Episode: 185/200 RapTime: 0:00:01.617712 FixedProfit: 1197165\n",
            "Episode: 186/200 RapTime: 0:00:01.674171 FixedProfit: 1197165\n",
            "Episode: 187/200 RapTime: 0:00:01.636608 FixedProfit: 1197165\n",
            "Episode: 188/200 RapTime: 0:00:01.650176 FixedProfit: 1197165\n",
            "Episode: 189/200 RapTime: 0:00:01.636725 FixedProfit: 1197165\n",
            "Episode: 190/200 RapTime: 0:00:01.646692 FixedProfit: 1197165\n",
            "Episode: 191/200 RapTime: 0:00:01.666790 FixedProfit: 1197165\n",
            "Episode: 192/200 RapTime: 0:00:01.651143 FixedProfit: 1197165\n",
            "Episode: 193/200 RapTime: 0:00:01.623838 FixedProfit: 1197165\n",
            "Episode: 194/200 RapTime: 0:00:01.629638 FixedProfit: 1197165\n",
            "Episode: 195/200 RapTime: 0:00:01.611300 FixedProfit: 1197165\n",
            "Episode: 196/200 RapTime: 0:00:01.659602 FixedProfit: 1197165\n",
            "Episode: 197/200 RapTime: 0:00:01.650272 FixedProfit: 1197165\n",
            "Episode: 198/200 RapTime: 0:00:01.654561 FixedProfit: 1197165\n",
            "Episode: 199/200 RapTime: 0:00:01.626941 FixedProfit: 1197165\n",
            "Episode: 200/200 RapTime: 0:00:01.632571 FixedProfit: 1197165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}