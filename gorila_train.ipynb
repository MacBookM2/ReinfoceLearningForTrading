{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gorila_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOFLIvZg4IzyLmLgTKtEIV5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/gorila_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "67dc5f1d-4258-486c-a90c-155438781174"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "mode = 'train'\n",
        "name = 'gorila'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=1000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.now_step        = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1RNmtkaZ2W"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        mastermodel = Sequential()\n",
        "        mastermodel.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_mid))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_action))\n",
        "        mastermodel.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((mastermodel.summary()))\n",
        "        self.mastermodel = mastermodel\n",
        "    \n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "        self.masterbrain = masterbrain\n",
        "\n",
        "    def load(self, name):\n",
        "        self.masterbrain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.masterbrain.save(name)\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def train_on_batch(self, state, target_full):\n",
        "        self.model.train_on_batch(state, target_full)\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w1_BMH7hLQ8"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size3 = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size3, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size3, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size3, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size3, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size3, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size3\n",
        "        self.size = min(self.size+1, self.max_size3)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrEa4wpGG1DF"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, memory, max_size, batch_size = 32):\n",
        "\n",
        "        self.brain   = brain\n",
        "        self.memory  = memory\n",
        "        self.epsilon = 1.0\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.brain.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42NIN-PGBOc8"
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, brain, memory, batch_size = 32):\n",
        "\n",
        "        self.brain  = brain\n",
        "        self.memory = memory\n",
        "\n",
        "        self.gamma       = 0.95\n",
        "        self.epsilon     = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r           = 0.995\n",
        "        self.batch_size  = batch_size\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.brain.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.brain.predict(states)\n",
        "\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.brain.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, learner, num, mdl_dir, name, episodes_times = 1000, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.actor          = actor\n",
        "        self.learner        = learner\n",
        "        self.num            = str(num)\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "        self.actor.layering()\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.actor.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.actor.store_transition(state, action, reward, next_state, done)\n",
        "                    self.learner.learn()\n",
        "                \n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                learner.integration()\n",
        "                actor.layering()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name, self.num))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name, self.num))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "37825618-0d18-44cb-d371-7633a1cf60c0"
      },
      "source": [
        "initial_money  = 1000000\n",
        "episodes_times = 50\n",
        "batch_size     = 32\n",
        "max_size       = 500\n",
        "\n",
        "masterbrain    = ParameterServer()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env     = Environment(df, initial_money=initial_money, mode = mode)\n",
        "    brain   = Brain(masterbrain)\n",
        "    memory  = ReplayMemory(max_size, batch_size)\n",
        "    actor   = Actor(brain, memory, max_size, batch_size)\n",
        "    learner = Learner(brain, memory, batch_size)\n",
        "    main    = Main(env, actor, learner, i, mdl_dir, name, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_5 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_6 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_7 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_8 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_9 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/50 RapTime: 0:03:01.991653 FixedProfit: 1310117\n",
            "Episode: 1/50 RapTime: 0:03:02.134180 FixedProfit: 1269713\n",
            "Episode: 1/50 RapTime: 0:03:02.778482 FixedProfit: 896557\n",
            "Episode: 1/50 RapTime: 0:03:02.779279 FixedProfit: 934745\n",
            "Episode: 2/50 RapTime: 0:03:04.358775 FixedProfit: 1137227\n",
            "Episode: 2/50 RapTime: 0:03:05.091963 FixedProfit: 1142116\n",
            "Episode: 2/50 RapTime: 0:03:05.380180 FixedProfit: 1170064\n",
            "Episode: 2/50 RapTime: 0:03:05.007229 FixedProfit: 968769\n",
            "Episode: 3/50 RapTime: 0:03:00.997736 FixedProfit: 1058568\n",
            "Episode: 3/50 RapTime: 0:03:03.534940 FixedProfit: 994780\n",
            "Episode: 3/50 RapTime: 0:03:06.550209 FixedProfit: 1203291\n",
            "Episode: 3/50 RapTime: 0:03:07.265223 FixedProfit: 1030777\n",
            "Episode: 4/50 RapTime: 0:03:07.552314 FixedProfit: 1115633\n",
            "Episode: 4/50 RapTime: 0:03:03.867262 FixedProfit: 997056\n",
            "Episode: 4/50 RapTime: 0:03:07.608478 FixedProfit: 1007771\n",
            "Episode: 4/50 RapTime: 0:03:10.828628 FixedProfit: 1231531\n",
            "Episode: 5/50 RapTime: 0:03:09.820532 FixedProfit: 1088800\n",
            "Episode: 5/50 RapTime: 0:03:06.159750 FixedProfit: 916447\n",
            "Episode: 5/50 RapTime: 0:03:08.719741 FixedProfit: 1349110\n",
            "Episode: 5/50 RapTime: 0:03:08.187168 FixedProfit: 934287\n",
            "Episode: 6/50 RapTime: 0:03:08.316509 FixedProfit: 1064372\n",
            "Episode: 6/50 RapTime: 0:03:10.925774 FixedProfit: 1017011\n",
            "Episode: 6/50 RapTime: 0:03:11.797785 FixedProfit: 1182657\n",
            "Episode: 6/50 RapTime: 0:03:11.590839 FixedProfit: 1097087\n",
            "Episode: 7/50 RapTime: 0:03:10.528662 FixedProfit: 1085428\n",
            "Episode: 7/50 RapTime: 0:03:10.416610 FixedProfit: 1069734\n",
            "Episode: 7/50 RapTime: 0:03:09.321245 FixedProfit: 1215380\n",
            "Episode: 7/50 RapTime: 0:03:11.283534 FixedProfit: 1013727\n",
            "Episode: 8/50 RapTime: 0:03:10.869980 FixedProfit: 935802\n",
            "Episode: 8/50 RapTime: 0:03:10.899204 FixedProfit: 990191\n",
            "Episode: 8/50 RapTime: 0:03:13.246555 FixedProfit: 1056257\n",
            "Episode: 8/50 RapTime: 0:03:10.801605 FixedProfit: 943985\n",
            "Episode: 9/50 RapTime: 0:03:12.331726 FixedProfit: 896186\n",
            "Episode: 9/50 RapTime: 0:03:11.951408 FixedProfit: 1077669\n",
            "Episode: 9/50 RapTime: 0:03:12.081675 FixedProfit: 1141783\n",
            "Episode: 9/50 RapTime: 0:03:15.700942 FixedProfit: 1135611\n",
            "Episode: 10/50 RapTime: 0:03:14.220192 FixedProfit: 1073698\n",
            "Episode: 10/50 RapTime: 0:03:17.282968 FixedProfit: 859236\n",
            "Episode: 10/50 RapTime: 0:03:16.604462 FixedProfit: 1019789\n",
            "Episode: 10/50 RapTime: 0:03:14.128432 FixedProfit: 1212037\n",
            "Episode: 11/50 RapTime: 0:03:16.487026 FixedProfit: 1071131\n",
            "Episode: 11/50 RapTime: 0:03:15.980212 FixedProfit: 1060313\n",
            "Episode: 11/50 RapTime: 0:03:19.346047 FixedProfit: 1094723\n",
            "Episode: 11/50 RapTime: 0:03:16.495990 FixedProfit: 1264120\n",
            "Episode: 12/50 RapTime: 0:03:15.408874 FixedProfit: 1052109\n",
            "Episode: 12/50 RapTime: 0:03:18.670854 FixedProfit: 1231165\n",
            "Episode: 12/50 RapTime: 0:03:18.616847 FixedProfit: 1011035\n",
            "Episode: 12/50 RapTime: 0:03:15.990663 FixedProfit: 1236351\n",
            "Episode: 13/50 RapTime: 0:03:19.437604 FixedProfit: 1011458\n",
            "Episode: 13/50 RapTime: 0:03:17.015100 FixedProfit: 1061293\n",
            "Episode: 13/50 RapTime: 0:03:19.519957 FixedProfit: 1107486\n",
            "Episode: 13/50 RapTime: 0:03:21.376891 FixedProfit: 1067232\n",
            "Episode: 14/50 RapTime: 0:03:28.053473 FixedProfit: 1185802\n",
            "Episode: 14/50 RapTime: 0:03:29.425539 FixedProfit: 961809\n",
            "Episode: 14/50 RapTime: 0:03:27.902515 FixedProfit: 1088859\n",
            "Episode: 14/50 RapTime: 0:03:26.091731 FixedProfit: 1139167\n",
            "Episode: 15/50 RapTime: 0:03:31.897504 FixedProfit: 1249653\n",
            "Episode: 15/50 RapTime: 0:03:31.997520 FixedProfit: 1096595\n",
            "Episode: 15/50 RapTime: 0:03:31.968838 FixedProfit: 966636\n",
            "Episode: 15/50 RapTime: 0:03:31.453635 FixedProfit: 841299\n",
            "Episode: 16/50 RapTime: 0:03:24.632324 FixedProfit: 1071174\n",
            "Episode: 16/50 RapTime: 0:03:22.246794 FixedProfit: 1260179\n",
            "Episode: 16/50 RapTime: 0:03:27.009660 FixedProfit: 1172693\n",
            "Episode: 16/50 RapTime: 0:03:26.861138 FixedProfit: 1236817\n",
            "Episode: 17/50 RapTime: 0:03:24.786167 FixedProfit: 1026284\n",
            "Episode: 17/50 RapTime: 0:03:26.512346 FixedProfit: 1029381\n",
            "Episode: 17/50 RapTime: 0:03:29.765493 FixedProfit: 1158951\n",
            "Episode: 17/50 RapTime: 0:03:28.308835 FixedProfit: 1013865\n",
            "Episode: 18/50 RapTime: 0:03:28.562903 FixedProfit: 1093597\n",
            "Episode: 18/50 RapTime: 0:03:28.685814 FixedProfit: 946389\n",
            "Episode: 18/50 RapTime: 0:03:28.689931 FixedProfit: 1035018\n",
            "Episode: 18/50 RapTime: 0:03:30.896212 FixedProfit: 920705\n",
            "Episode: 19/50 RapTime: 0:03:26.891729 FixedProfit: 1029075\n",
            "Episode: 19/50 RapTime: 0:03:31.207435 FixedProfit: 960579\n",
            "Episode: 19/50 RapTime: 0:03:31.377369 FixedProfit: 1100934\n",
            "Episode: 19/50 RapTime: 0:03:31.114412 FixedProfit: 1029954\n",
            "Episode: 20/50 RapTime: 0:03:29.848689 FixedProfit: 1120238\n",
            "Episode: 20/50 RapTime: 0:03:34.314610 FixedProfit: 1067114\n",
            "Episode: 20/50 RapTime: 0:03:31.627671 FixedProfit: 1131002\n",
            "Episode: 20/50 RapTime: 0:03:33.716480 FixedProfit: 993883\n",
            "Episode: 21/50 RapTime: 0:03:33.708468 FixedProfit: 1005404\n",
            "Episode: 21/50 RapTime: 0:03:29.167968 FixedProfit: 905794\n",
            "Episode: 21/50 RapTime: 0:03:34.235305 FixedProfit: 870595\n",
            "Episode: 21/50 RapTime: 0:03:35.329162 FixedProfit: 1177537\n",
            "Episode: 22/50 RapTime: 0:03:34.550707 FixedProfit: 1070625\n",
            "Episode: 22/50 RapTime: 0:03:33.735284 FixedProfit: 1031579\n",
            "Episode: 22/50 RapTime: 0:03:33.170629 FixedProfit: 1028613\n",
            "Episode: 22/50 RapTime: 0:03:32.978655 FixedProfit: 974834\n",
            "Episode: 23/50 RapTime: 0:03:36.858786 FixedProfit: 1235055\n",
            "Episode: 23/50 RapTime: 0:03:36.130698 FixedProfit: 1077065\n",
            "Episode: 23/50 RapTime: 0:03:32.746088 FixedProfit: 1077914\n",
            "Episode: 23/50 RapTime: 0:03:34.634227 FixedProfit: 1080788\n",
            "Episode: 24/50 RapTime: 0:03:38.350077 FixedProfit: 990588\n",
            "Episode: 24/50 RapTime: 0:03:36.769226 FixedProfit: 1102546\n",
            "Episode: 24/50 RapTime: 0:03:34.754005 FixedProfit: 991153\n",
            "Episode: 24/50 RapTime: 0:03:36.460671 FixedProfit: 1129810\n",
            "Episode: 25/50 RapTime: 0:03:34.034370 FixedProfit: 995818\n",
            "Episode: 25/50 RapTime: 0:03:35.480960 FixedProfit: 1210747\n",
            "Episode: 25/50 RapTime: 0:03:38.067642 FixedProfit: 931176\n",
            "Episode: 25/50 RapTime: 0:03:41.341817 FixedProfit: 1228528\n",
            "Episode: 26/50 RapTime: 0:03:42.093423 FixedProfit: 1009821\n",
            "Episode: 26/50 RapTime: 0:03:38.861571 FixedProfit: 951346\n",
            "Episode: 26/50 RapTime: 0:03:45.366222 FixedProfit: 1057411\n",
            "Episode: 26/50 RapTime: 0:03:46.720071 FixedProfit: 1171405\n",
            "Episode: 27/50 RapTime: 0:03:42.323799 FixedProfit: 967544\n",
            "Episode: 27/50 RapTime: 0:03:40.510725 FixedProfit: 1122459\n",
            "Episode: 27/50 RapTime: 0:03:40.360044 FixedProfit: 1118887\n",
            "Episode: 27/50 RapTime: 0:03:42.380865 FixedProfit: 1002846\n",
            "Episode: 28/50 RapTime: 0:03:50.316726 FixedProfit: 1166103\n",
            "Episode: 28/50 RapTime: 0:03:53.049845 FixedProfit: 1063642\n",
            "Episode: 28/50 RapTime: 0:03:51.257799 FixedProfit: 1066713\n",
            "Episode: 28/50 RapTime: 0:03:55.544964 FixedProfit: 1278728\n",
            "Episode: 29/50 RapTime: 0:03:57.656857 FixedProfit: 868784\n",
            "Episode: 29/50 RapTime: 0:03:58.415975 FixedProfit: 1060591\n",
            "Episode: 29/50 RapTime: 0:03:56.900957 FixedProfit: 1082139\n",
            "Episode: 29/50 RapTime: 0:03:59.359741 FixedProfit: 1086190\n",
            "Episode: 30/50 RapTime: 0:03:47.099886 FixedProfit: 1095318\n",
            "Episode: 30/50 RapTime: 0:03:51.128089 FixedProfit: 1009843\n",
            "Episode: 30/50 RapTime: 0:03:47.850447 FixedProfit: 1138017\n",
            "Episode: 30/50 RapTime: 0:03:48.511959 FixedProfit: 1051981\n",
            "Episode: 31/50 RapTime: 0:03:51.200456 FixedProfit: 1326479\n",
            "Episode: 31/50 RapTime: 0:03:52.768999 FixedProfit: 1093755\n",
            "Episode: 31/50 RapTime: 0:03:54.018223 FixedProfit: 1176251\n",
            "Episode: 31/50 RapTime: 0:03:54.316695 FixedProfit: 1135929\n",
            "Episode: 32/50 RapTime: 0:04:00.375795 FixedProfit: 1023719\n",
            "Episode: 32/50 RapTime: 0:03:59.330089 FixedProfit: 1212019\n",
            "Episode: 32/50 RapTime: 0:03:57.000142 FixedProfit: 969135\n",
            "Episode: 32/50 RapTime: 0:03:59.837628 FixedProfit: 1340820\n",
            "Episode: 33/50 RapTime: 0:04:07.202989 FixedProfit: 1133224\n",
            "Episode: 33/50 RapTime: 0:04:07.913031 FixedProfit: 970493\n",
            "Episode: 33/50 RapTime: 0:04:08.260849 FixedProfit: 1209743\n",
            "Episode: 33/50 RapTime: 0:04:07.364025 FixedProfit: 928953\n",
            "Episode: 34/50 RapTime: 0:03:51.320763 FixedProfit: 1143155\n",
            "Episode: 34/50 RapTime: 0:03:51.367138 FixedProfit: 1104494\n",
            "Episode: 34/50 RapTime: 0:03:57.017350 FixedProfit: 1028585\n",
            "Episode: 34/50 RapTime: 0:03:51.182721 FixedProfit: 1199976\n",
            "Episode: 35/50 RapTime: 0:03:58.226543 FixedProfit: 1153204\n",
            "Episode: 35/50 RapTime: 0:03:57.107947 FixedProfit: 1061900\n",
            "Episode: 35/50 RapTime: 0:03:59.450718 FixedProfit: 1058553\n",
            "Episode: 35/50 RapTime: 0:03:57.591739 FixedProfit: 1023391\n",
            "Episode: 36/50 RapTime: 0:04:06.846494 FixedProfit: 1155544\n",
            "Episode: 36/50 RapTime: 0:03:57.682789 FixedProfit: 1007294\n",
            "Episode: 36/50 RapTime: 0:03:59.517053 FixedProfit: 927254\n",
            "Episode: 36/50 RapTime: 0:04:01.046357 FixedProfit: 861602\n",
            "Episode: 37/50 RapTime: 0:03:57.423216 FixedProfit: 1210254\n",
            "Episode: 37/50 RapTime: 0:03:58.917208 FixedProfit: 1125942\n",
            "Episode: 37/50 RapTime: 0:03:57.557588 FixedProfit: 1121428\n",
            "Episode: 37/50 RapTime: 0:04:08.063017 FixedProfit: 1027713\n",
            "Episode: 38/50 RapTime: 0:04:06.145254 FixedProfit: 1140419\n",
            "Episode: 38/50 RapTime: 0:04:04.161020 FixedProfit: 993528\n",
            "Episode: 38/50 RapTime: 0:04:04.903319 FixedProfit: 1028145\n",
            "Episode: 38/50 RapTime: 0:04:02.885673 FixedProfit: 1175151\n",
            "Episode: 39/50 RapTime: 0:04:06.919516 FixedProfit: 1012157\n",
            "Episode: 39/50 RapTime: 0:04:05.540035 FixedProfit: 1141185\n",
            "Episode: 39/50 RapTime: 0:04:07.315894 FixedProfit: 1202049\n",
            "Episode: 39/50 RapTime: 0:04:04.619123 FixedProfit: 919926\n",
            "Episode: 40/50 RapTime: 0:04:04.815068 FixedProfit: 1204148\n",
            "Episode: 40/50 RapTime: 0:04:05.400582 FixedProfit: 949795\n",
            "Episode: 40/50 RapTime: 0:04:08.260824 FixedProfit: 1209402\n",
            "Episode: 40/50 RapTime: 0:04:07.697311 FixedProfit: 1203621\n",
            "Episode: 41/50 RapTime: 0:04:10.143974 FixedProfit: 975909\n",
            "Episode: 41/50 RapTime: 0:04:09.629891 FixedProfit: 1165841\n",
            "Episode: 41/50 RapTime: 0:04:13.540449 FixedProfit: 1120623\n",
            "Episode: 41/50 RapTime: 0:04:06.471180 FixedProfit: 1159174\n",
            "Episode: 42/50 RapTime: 0:04:06.959335 FixedProfit: 1138620\n",
            "Episode: 42/50 RapTime: 0:04:09.576713 FixedProfit: 994885\n",
            "Episode: 42/50 RapTime: 0:04:13.181462 FixedProfit: 1036160\n",
            "Episode: 42/50 RapTime: 0:04:08.953388 FixedProfit: 1036477\n",
            "Episode: 43/50 RapTime: 0:04:11.622987 FixedProfit: 1045735\n",
            "Episode: 43/50 RapTime: 0:04:10.220039 FixedProfit: 901411\n",
            "Episode: 43/50 RapTime: 0:04:12.530058 FixedProfit: 1312203\n",
            "Episode: 43/50 RapTime: 0:04:07.017976 FixedProfit: 941568\n",
            "Episode: 44/50 RapTime: 0:04:12.956289 FixedProfit: 1055595\n",
            "Episode: 44/50 RapTime: 0:04:10.305491 FixedProfit: 1056716\n",
            "Episode: 44/50 RapTime: 0:04:10.137681 FixedProfit: 1158630\n",
            "Episode: 44/50 RapTime: 0:04:08.144731 FixedProfit: 861927\n",
            "Episode: 45/50 RapTime: 0:04:12.850815 FixedProfit: 1365475\n",
            "Episode: 45/50 RapTime: 0:04:13.282637 FixedProfit: 1109906\n",
            "Episode: 45/50 RapTime: 0:04:10.230668 FixedProfit: 1040533\n",
            "Episode: 45/50 RapTime: 0:04:11.307832 FixedProfit: 1184797\n",
            "Episode: 46/50 RapTime: 0:04:18.174958 FixedProfit: 1241275\n",
            "Episode: 46/50 RapTime: 0:04:12.359085 FixedProfit: 1059308\n",
            "Episode: 46/50 RapTime: 0:04:16.942293 FixedProfit: 949312\n",
            "Episode: 46/50 RapTime: 0:04:11.221202 FixedProfit: 1106945\n",
            "Episode: 47/50 RapTime: 0:04:16.959704 FixedProfit: 1448732\n",
            "Episode: 47/50 RapTime: 0:04:18.287407 FixedProfit: 1179775\n",
            "Episode: 47/50 RapTime: 0:04:13.984157 FixedProfit: 1076477\n",
            "Episode: 47/50 RapTime: 0:04:17.991926 FixedProfit: 1000580\n",
            "Episode: 48/50 RapTime: 0:04:18.816656 FixedProfit: 1083047\n",
            "Episode: 48/50 RapTime: 0:04:15.170753 FixedProfit: 983026\n",
            "Episode: 48/50 RapTime: 0:04:17.506824 FixedProfit: 1020876\n",
            "Episode: 48/50 RapTime: 0:04:19.901942 FixedProfit: 1101587\n",
            "Episode: 49/50 RapTime: 0:04:18.301730 FixedProfit: 1189179\n",
            "Episode: 49/50 RapTime: 0:04:19.234501 FixedProfit: 1111771\n",
            "Episode: 49/50 RapTime: 0:04:18.739716 FixedProfit: 1172429\n",
            "Episode: 49/50 RapTime: 0:04:25.396905 FixedProfit: 1009619\n",
            "Episode: 50/50 RapTime: 0:04:18.635399 FixedProfit: 1280093\n",
            "Episode: 50/50 RapTime: 0:04:23.825925 FixedProfit: 1109853\n",
            "Episode: 50/50 RapTime: 0:04:14.322707 FixedProfit: 1075355\n",
            "Episode: 50/50 RapTime: 0:04:21.389052 FixedProfit: 1125935\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}