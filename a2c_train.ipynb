{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a2c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "6dee7724-f03b-46a3-e792-379de4ed389b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "mode = 'train'\n",
        "name = 'a2c'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_shape = 3\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        Brain.model = model\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        Brain.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        Brain.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = Brain.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(3, p=act_p[0].numpy())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic(Brain):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "        self.beta  = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, 3)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = Brain.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, Brain.model.trainable_variables)\n",
        "\n",
        "        Brain.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, Brain.model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = Brain.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "            self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "        else:\n",
        "            self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            experiences = []\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if mode == 'train':\n",
        "                    experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                    if len(experiences) == self.batch_size:\n",
        "                        self.critic.valuenetwork(experiences)\n",
        "                        experiences = []\n",
        "\n",
        "                state = next_state\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=self.df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, self.episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            else:\n",
        "                record = pd.Series(info['cur_revenue'], index=self.df_rec.columns)\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, self.episodes_times, play_time, info['cur_revenue']))\n",
        "\n",
        "            self.df_rec = self.df_rec.append(record, ignore_index=True)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "        self._save_csv()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "    def _save_csv(self):\n",
        "        self.df_rec.to_csv(csv_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc26e458-c982-402b-878b-12ef863427cd"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 200\n",
        "batch_size = 32\n",
        "\n",
        "actor = Actor()\n",
        "critic = Critic()\n",
        "env = Environment(df, initial_money, mode)\n",
        "main = Main(env, actor, critic, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/200 RapTime: 0:00:02.839952 FixedProfit: 1202786\n",
            "Episode: 2/200 RapTime: 0:00:01.231877 FixedProfit: 1178007\n",
            "Episode: 3/200 RapTime: 0:00:01.246128 FixedProfit: 1220666\n",
            "Episode: 4/200 RapTime: 0:00:01.243687 FixedProfit: 1170465\n",
            "Episode: 5/200 RapTime: 0:00:01.251950 FixedProfit: 1209330\n",
            "Episode: 6/200 RapTime: 0:00:01.225935 FixedProfit: 1213636\n",
            "Episode: 7/200 RapTime: 0:00:01.242100 FixedProfit: 1152858\n",
            "Episode: 8/200 RapTime: 0:00:01.231906 FixedProfit: 1203963\n",
            "Episode: 9/200 RapTime: 0:00:01.209136 FixedProfit: 1194542\n",
            "Episode: 10/200 RapTime: 0:00:01.233405 FixedProfit: 1194032\n",
            "Episode: 11/200 RapTime: 0:00:01.231358 FixedProfit: 1192980\n",
            "Episode: 12/200 RapTime: 0:00:01.209775 FixedProfit: 1173059\n",
            "Episode: 13/200 RapTime: 0:00:01.215958 FixedProfit: 1173059\n",
            "Episode: 14/200 RapTime: 0:00:01.218000 FixedProfit: 1197165\n",
            "Episode: 15/200 RapTime: 0:00:01.214433 FixedProfit: 1208128\n",
            "Episode: 16/200 RapTime: 0:00:01.210746 FixedProfit: 1208128\n",
            "Episode: 17/200 RapTime: 0:00:01.223876 FixedProfit: 1208128\n",
            "Episode: 18/200 RapTime: 0:00:01.210603 FixedProfit: 1197165\n",
            "Episode: 19/200 RapTime: 0:00:01.223400 FixedProfit: 1197165\n",
            "Episode: 20/200 RapTime: 0:00:01.242431 FixedProfit: 1208128\n",
            "Episode: 21/200 RapTime: 0:00:01.199837 FixedProfit: 1197165\n",
            "Episode: 22/200 RapTime: 0:00:01.203746 FixedProfit: 1194143\n",
            "Episode: 23/200 RapTime: 0:00:01.226994 FixedProfit: 1197165\n",
            "Episode: 24/200 RapTime: 0:00:01.209104 FixedProfit: 1197165\n",
            "Episode: 25/200 RapTime: 0:00:01.237213 FixedProfit: 1197165\n",
            "Episode: 26/200 RapTime: 0:00:01.310334 FixedProfit: 1208128\n",
            "Episode: 27/200 RapTime: 0:00:01.281819 FixedProfit: 1208128\n",
            "Episode: 28/200 RapTime: 0:00:01.291184 FixedProfit: 1194143\n",
            "Episode: 29/200 RapTime: 0:00:01.279791 FixedProfit: 1197165\n",
            "Episode: 30/200 RapTime: 0:00:01.302626 FixedProfit: 1197165\n",
            "Episode: 31/200 RapTime: 0:00:01.296531 FixedProfit: 1197165\n",
            "Episode: 32/200 RapTime: 0:00:01.268346 FixedProfit: 1197165\n",
            "Episode: 33/200 RapTime: 0:00:01.297215 FixedProfit: 1208128\n",
            "Episode: 34/200 RapTime: 0:00:01.276085 FixedProfit: 1197165\n",
            "Episode: 35/200 RapTime: 0:00:01.294135 FixedProfit: 1197165\n",
            "Episode: 36/200 RapTime: 0:00:01.309208 FixedProfit: 1197165\n",
            "Episode: 37/200 RapTime: 0:00:01.323983 FixedProfit: 1195955\n",
            "Episode: 38/200 RapTime: 0:00:01.342738 FixedProfit: 1197165\n",
            "Episode: 39/200 RapTime: 0:00:01.311677 FixedProfit: 1208128\n",
            "Episode: 40/200 RapTime: 0:00:01.323204 FixedProfit: 1208128\n",
            "Episode: 41/200 RapTime: 0:00:01.344856 FixedProfit: 1194143\n",
            "Episode: 42/200 RapTime: 0:00:01.331411 FixedProfit: 1197165\n",
            "Episode: 43/200 RapTime: 0:00:01.368084 FixedProfit: 1197165\n",
            "Episode: 44/200 RapTime: 0:00:01.335829 FixedProfit: 1197165\n",
            "Episode: 45/200 RapTime: 0:00:01.315104 FixedProfit: 1197165\n",
            "Episode: 46/200 RapTime: 0:00:01.285501 FixedProfit: 1197165\n",
            "Episode: 47/200 RapTime: 0:00:01.246758 FixedProfit: 1197165\n",
            "Episode: 48/200 RapTime: 0:00:01.251675 FixedProfit: 1197165\n",
            "Episode: 49/200 RapTime: 0:00:01.241262 FixedProfit: 1197165\n",
            "Episode: 50/200 RapTime: 0:00:01.244151 FixedProfit: 1183321\n",
            "Episode: 51/200 RapTime: 0:00:01.243665 FixedProfit: 1197165\n",
            "Episode: 52/200 RapTime: 0:00:01.253050 FixedProfit: 1197165\n",
            "Episode: 53/200 RapTime: 0:00:01.252223 FixedProfit: 1197165\n",
            "Episode: 54/200 RapTime: 0:00:01.246715 FixedProfit: 1197165\n",
            "Episode: 55/200 RapTime: 0:00:01.261373 FixedProfit: 1197165\n",
            "Episode: 56/200 RapTime: 0:00:01.231962 FixedProfit: 1197165\n",
            "Episode: 57/200 RapTime: 0:00:01.225282 FixedProfit: 1208128\n",
            "Episode: 58/200 RapTime: 0:00:01.197563 FixedProfit: 1188458\n",
            "Episode: 59/200 RapTime: 0:00:01.223879 FixedProfit: 1194143\n",
            "Episode: 60/200 RapTime: 0:00:01.207180 FixedProfit: 1197165\n",
            "Episode: 61/200 RapTime: 0:00:01.230600 FixedProfit: 1197165\n",
            "Episode: 62/200 RapTime: 0:00:01.230458 FixedProfit: 1197165\n",
            "Episode: 63/200 RapTime: 0:00:01.215465 FixedProfit: 1197165\n",
            "Episode: 64/200 RapTime: 0:00:01.193004 FixedProfit: 1197165\n",
            "Episode: 65/200 RapTime: 0:00:01.198888 FixedProfit: 1197165\n",
            "Episode: 66/200 RapTime: 0:00:01.229540 FixedProfit: 1197165\n",
            "Episode: 67/200 RapTime: 0:00:01.221377 FixedProfit: 1197165\n",
            "Episode: 68/200 RapTime: 0:00:01.232350 FixedProfit: 1197165\n",
            "Episode: 69/200 RapTime: 0:00:01.213643 FixedProfit: 1197165\n",
            "Episode: 70/200 RapTime: 0:00:01.213853 FixedProfit: 1197165\n",
            "Episode: 71/200 RapTime: 0:00:01.212077 FixedProfit: 1197165\n",
            "Episode: 72/200 RapTime: 0:00:01.206714 FixedProfit: 1197165\n",
            "Episode: 73/200 RapTime: 0:00:01.217166 FixedProfit: 1197165\n",
            "Episode: 74/200 RapTime: 0:00:01.209882 FixedProfit: 1197165\n",
            "Episode: 75/200 RapTime: 0:00:01.204975 FixedProfit: 1197165\n",
            "Episode: 76/200 RapTime: 0:00:01.201963 FixedProfit: 1197165\n",
            "Episode: 77/200 RapTime: 0:00:01.206536 FixedProfit: 1197165\n",
            "Episode: 78/200 RapTime: 0:00:01.303510 FixedProfit: 1197165\n",
            "Episode: 79/200 RapTime: 0:00:01.275649 FixedProfit: 1197165\n",
            "Episode: 80/200 RapTime: 0:00:01.294714 FixedProfit: 1197165\n",
            "Episode: 81/200 RapTime: 0:00:01.288883 FixedProfit: 1197165\n",
            "Episode: 82/200 RapTime: 0:00:01.284446 FixedProfit: 1197165\n",
            "Episode: 83/200 RapTime: 0:00:01.262587 FixedProfit: 1197165\n",
            "Episode: 84/200 RapTime: 0:00:01.283550 FixedProfit: 1197165\n",
            "Episode: 85/200 RapTime: 0:00:01.271524 FixedProfit: 1197165\n",
            "Episode: 86/200 RapTime: 0:00:01.279791 FixedProfit: 1197165\n",
            "Episode: 87/200 RapTime: 0:00:01.285736 FixedProfit: 1197165\n",
            "Episode: 88/200 RapTime: 0:00:01.290207 FixedProfit: 1197165\n",
            "Episode: 89/200 RapTime: 0:00:01.294934 FixedProfit: 1197165\n",
            "Episode: 90/200 RapTime: 0:00:01.278861 FixedProfit: 1197165\n",
            "Episode: 91/200 RapTime: 0:00:01.253555 FixedProfit: 1197165\n",
            "Episode: 92/200 RapTime: 0:00:01.278266 FixedProfit: 1197165\n",
            "Episode: 93/200 RapTime: 0:00:01.287963 FixedProfit: 1197165\n",
            "Episode: 94/200 RapTime: 0:00:01.278305 FixedProfit: 1197165\n",
            "Episode: 95/200 RapTime: 0:00:01.289324 FixedProfit: 1197165\n",
            "Episode: 96/200 RapTime: 0:00:01.268169 FixedProfit: 1197165\n",
            "Episode: 97/200 RapTime: 0:00:01.211822 FixedProfit: 1197165\n",
            "Episode: 98/200 RapTime: 0:00:01.216275 FixedProfit: 1197165\n",
            "Episode: 99/200 RapTime: 0:00:01.213933 FixedProfit: 1197165\n",
            "Episode: 100/200 RapTime: 0:00:01.216404 FixedProfit: 1197165\n",
            "Episode: 101/200 RapTime: 0:00:01.218477 FixedProfit: 1197165\n",
            "Episode: 102/200 RapTime: 0:00:01.201786 FixedProfit: 1197165\n",
            "Episode: 103/200 RapTime: 0:00:01.216626 FixedProfit: 1197165\n",
            "Episode: 104/200 RapTime: 0:00:01.198026 FixedProfit: 1208128\n",
            "Episode: 105/200 RapTime: 0:00:01.217910 FixedProfit: 1197165\n",
            "Episode: 106/200 RapTime: 0:00:01.204263 FixedProfit: 1208128\n",
            "Episode: 107/200 RapTime: 0:00:01.205144 FixedProfit: 1197165\n",
            "Episode: 108/200 RapTime: 0:00:01.187693 FixedProfit: 1197165\n",
            "Episode: 109/200 RapTime: 0:00:01.194201 FixedProfit: 1208128\n",
            "Episode: 110/200 RapTime: 0:00:01.199466 FixedProfit: 1197165\n",
            "Episode: 111/200 RapTime: 0:00:01.217363 FixedProfit: 1197165\n",
            "Episode: 112/200 RapTime: 0:00:01.178625 FixedProfit: 1208128\n",
            "Episode: 113/200 RapTime: 0:00:01.243840 FixedProfit: 1194143\n",
            "Episode: 114/200 RapTime: 0:00:01.228082 FixedProfit: 1197165\n",
            "Episode: 115/200 RapTime: 0:00:01.190048 FixedProfit: 1208128\n",
            "Episode: 116/200 RapTime: 0:00:01.202607 FixedProfit: 1194143\n",
            "Episode: 117/200 RapTime: 0:00:01.193083 FixedProfit: 1197165\n",
            "Episode: 118/200 RapTime: 0:00:01.193788 FixedProfit: 1197165\n",
            "Episode: 119/200 RapTime: 0:00:01.206707 FixedProfit: 1197165\n",
            "Episode: 120/200 RapTime: 0:00:01.214960 FixedProfit: 1197165\n",
            "Episode: 121/200 RapTime: 0:00:01.183768 FixedProfit: 1197165\n",
            "Episode: 122/200 RapTime: 0:00:01.202630 FixedProfit: 1197165\n",
            "Episode: 123/200 RapTime: 0:00:01.190870 FixedProfit: 1194143\n",
            "Episode: 124/200 RapTime: 0:00:01.224849 FixedProfit: 1197165\n",
            "Episode: 125/200 RapTime: 0:00:01.213228 FixedProfit: 1197165\n",
            "Episode: 126/200 RapTime: 0:00:01.205372 FixedProfit: 1208128\n",
            "Episode: 127/200 RapTime: 0:00:01.220015 FixedProfit: 1197165\n",
            "Episode: 128/200 RapTime: 0:00:01.205803 FixedProfit: 1208128\n",
            "Episode: 129/200 RapTime: 0:00:01.217782 FixedProfit: 1197165\n",
            "Episode: 130/200 RapTime: 0:00:01.211744 FixedProfit: 1173059\n",
            "Episode: 131/200 RapTime: 0:00:01.258460 FixedProfit: 1197165\n",
            "Episode: 132/200 RapTime: 0:00:01.273592 FixedProfit: 1197165\n",
            "Episode: 133/200 RapTime: 0:00:01.279980 FixedProfit: 1197165\n",
            "Episode: 134/200 RapTime: 0:00:01.293261 FixedProfit: 1197165\n",
            "Episode: 135/200 RapTime: 0:00:01.265878 FixedProfit: 1197165\n",
            "Episode: 136/200 RapTime: 0:00:01.278048 FixedProfit: 1197165\n",
            "Episode: 137/200 RapTime: 0:00:01.283689 FixedProfit: 1197165\n",
            "Episode: 138/200 RapTime: 0:00:01.277702 FixedProfit: 1197165\n",
            "Episode: 139/200 RapTime: 0:00:01.293648 FixedProfit: 1197165\n",
            "Episode: 140/200 RapTime: 0:00:01.269953 FixedProfit: 1197165\n",
            "Episode: 141/200 RapTime: 0:00:01.288666 FixedProfit: 1197165\n",
            "Episode: 142/200 RapTime: 0:00:01.282902 FixedProfit: 1197165\n",
            "Episode: 143/200 RapTime: 0:00:01.285861 FixedProfit: 1197165\n",
            "Episode: 144/200 RapTime: 0:00:01.264225 FixedProfit: 1208128\n",
            "Episode: 145/200 RapTime: 0:00:01.266676 FixedProfit: 1197165\n",
            "Episode: 146/200 RapTime: 0:00:01.262922 FixedProfit: 1197165\n",
            "Episode: 147/200 RapTime: 0:00:01.278617 FixedProfit: 1208128\n",
            "Episode: 148/200 RapTime: 0:00:01.272367 FixedProfit: 1197165\n",
            "Episode: 149/200 RapTime: 0:00:01.276221 FixedProfit: 1197165\n",
            "Episode: 150/200 RapTime: 0:00:01.256636 FixedProfit: 1173059\n",
            "Episode: 151/200 RapTime: 0:00:01.211362 FixedProfit: 1197165\n",
            "Episode: 152/200 RapTime: 0:00:01.213736 FixedProfit: 1197165\n",
            "Episode: 153/200 RapTime: 0:00:01.225514 FixedProfit: 1197165\n",
            "Episode: 154/200 RapTime: 0:00:01.218113 FixedProfit: 1208128\n",
            "Episode: 155/200 RapTime: 0:00:01.224007 FixedProfit: 1208128\n",
            "Episode: 156/200 RapTime: 0:00:01.198622 FixedProfit: 1197165\n",
            "Episode: 157/200 RapTime: 0:00:01.195819 FixedProfit: 1208128\n",
            "Episode: 158/200 RapTime: 0:00:01.206786 FixedProfit: 1208128\n",
            "Episode: 159/200 RapTime: 0:00:01.209537 FixedProfit: 1194143\n",
            "Episode: 160/200 RapTime: 0:00:01.209677 FixedProfit: 1197165\n",
            "Episode: 161/200 RapTime: 0:00:01.231430 FixedProfit: 1194143\n",
            "Episode: 162/200 RapTime: 0:00:01.217023 FixedProfit: 1197165\n",
            "Episode: 163/200 RapTime: 0:00:01.203562 FixedProfit: 1197165\n",
            "Episode: 164/200 RapTime: 0:00:01.211259 FixedProfit: 1197165\n",
            "Episode: 165/200 RapTime: 0:00:01.208951 FixedProfit: 1197165\n",
            "Episode: 166/200 RapTime: 0:00:01.226309 FixedProfit: 1197165\n",
            "Episode: 167/200 RapTime: 0:00:01.241197 FixedProfit: 1197165\n",
            "Episode: 168/200 RapTime: 0:00:01.211694 FixedProfit: 1197165\n",
            "Episode: 169/200 RapTime: 0:00:01.210580 FixedProfit: 1197165\n",
            "Episode: 170/200 RapTime: 0:00:01.205890 FixedProfit: 1208128\n",
            "Episode: 171/200 RapTime: 0:00:01.222212 FixedProfit: 1208128\n",
            "Episode: 172/200 RapTime: 0:00:01.208633 FixedProfit: 1197165\n",
            "Episode: 173/200 RapTime: 0:00:01.212309 FixedProfit: 1197165\n",
            "Episode: 174/200 RapTime: 0:00:01.221308 FixedProfit: 1173059\n",
            "Episode: 175/200 RapTime: 0:00:01.238941 FixedProfit: 1173059\n",
            "Episode: 176/200 RapTime: 0:00:01.203576 FixedProfit: 1197165\n",
            "Episode: 177/200 RapTime: 0:00:01.197461 FixedProfit: 1197165\n",
            "Episode: 178/200 RapTime: 0:00:01.222691 FixedProfit: 1197165\n",
            "Episode: 179/200 RapTime: 0:00:01.212911 FixedProfit: 1197165\n",
            "Episode: 180/200 RapTime: 0:00:01.198868 FixedProfit: 1197165\n",
            "Episode: 181/200 RapTime: 0:00:01.221705 FixedProfit: 1197165\n",
            "Episode: 182/200 RapTime: 0:00:01.212109 FixedProfit: 1197165\n",
            "Episode: 183/200 RapTime: 0:00:01.205751 FixedProfit: 1197165\n",
            "Episode: 184/200 RapTime: 0:00:01.199301 FixedProfit: 1197165\n",
            "Episode: 185/200 RapTime: 0:00:01.265977 FixedProfit: 1197165\n",
            "Episode: 186/200 RapTime: 0:00:01.262817 FixedProfit: 1197165\n",
            "Episode: 187/200 RapTime: 0:00:01.278193 FixedProfit: 1197165\n",
            "Episode: 188/200 RapTime: 0:00:01.290254 FixedProfit: 1197165\n",
            "Episode: 189/200 RapTime: 0:00:01.254255 FixedProfit: 1197165\n",
            "Episode: 190/200 RapTime: 0:00:01.282320 FixedProfit: 1197165\n",
            "Episode: 191/200 RapTime: 0:00:01.281325 FixedProfit: 1197165\n",
            "Episode: 192/200 RapTime: 0:00:01.263355 FixedProfit: 1197165\n",
            "Episode: 193/200 RapTime: 0:00:01.283690 FixedProfit: 1197165\n",
            "Episode: 194/200 RapTime: 0:00:01.271780 FixedProfit: 1197165\n",
            "Episode: 195/200 RapTime: 0:00:01.273095 FixedProfit: 1197165\n",
            "Episode: 196/200 RapTime: 0:00:01.278357 FixedProfit: 1197165\n",
            "Episode: 197/200 RapTime: 0:00:01.254494 FixedProfit: 1197165\n",
            "Episode: 198/200 RapTime: 0:00:01.260621 FixedProfit: 1197165\n",
            "Episode: 199/200 RapTime: 0:00:01.269612 FixedProfit: 1197165\n",
            "Episode: 200/200 RapTime: 0:00:01.275848 FixedProfit: 1197165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}