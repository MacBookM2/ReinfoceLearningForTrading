{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gorila_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMPN9CEq/Z9Vwv0rfIustqB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/gorila_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "58d88714-e309-4afd-fc71-ae0e3b3a77ec"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'gorila_train.csv'\n",
        "\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "rewards_folder = '/content/drive/My Drive/' + exp_dir + 'rl_rewards'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=1000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "class ReplayMemory:\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "\n",
        "    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.acts_buf = np.zeros(size, dtype=np.uint8)\n",
        "    self.rews_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.done_buf = np.zeros(size, dtype=np.uint8)\n",
        "    self.ptr, self.size, self.max_size = 0, 0, size\n",
        "\n",
        "  def store(self, obs, act, rew, next_obs, done):\n",
        "    self.obs1_buf[self.ptr] = obs\n",
        "    self.obs2_buf[self.ptr] = next_obs\n",
        "    self.acts_buf[self.ptr] = act\n",
        "    self.rews_buf[self.ptr] = rew\n",
        "    self.done_buf[self.ptr] = done\n",
        "    self.ptr = (self.ptr+1) % self.max_size\n",
        "    self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "  def sample_batch(self, batch_size=32):\n",
        "    idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "    return dict(s=self.obs1_buf[idxs],\n",
        "                s2=self.obs2_buf[idxs],\n",
        "                a=self.acts_buf[idxs],\n",
        "                r=self.rews_buf[idxs],\n",
        "                d=self.done_buf[idxs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1RNmtkaZ2W"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self, n_hidden_layers=1, hidden_dim=32):\n",
        "\n",
        "        n_mid = 3\n",
        "        n_state = 3\n",
        "        n_action = 3\n",
        "\n",
        "        mastermodel = Sequential()\n",
        "        mastermodel.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_mid))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_action))\n",
        "        mastermodel.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.mastermodel = mastermodel\n",
        "    \n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_hidden_layers=1, hidden_dim=32):\n",
        "\n",
        "        n_mid = 3\n",
        "        n_state = 3\n",
        "        n_action = 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULBV5XpsbOjq"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "  states = []\n",
        "  for _ in range(env.df_total_steps):\n",
        "    action = np.random.choice(env.action_space)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    states.append(state)\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(states)\n",
        "  return scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "  def __init__(self, state_size, action_size, brain, memory):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = memory\n",
        "    self.gamma = 0.95\n",
        "    self.epsilon = 1.0\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.model = brain.model\n",
        "\n",
        "  def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "    self.memory.store(state, action, reward, next_state, done)\n",
        "\n",
        "  def act(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return np.random.choice(self.action_size)\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "  def replay(self, batch_size=32):\n",
        "    if self.memory.size < batch_size:\n",
        "      return\n",
        "\n",
        "    minibatch = self.memory.sample_batch(batch_size)\n",
        "    states = minibatch['s']\n",
        "    actions = minibatch['a']\n",
        "    rewards = minibatch['r']\n",
        "    next_states = minibatch['s2']\n",
        "    done = minibatch['d']\n",
        "\n",
        "    target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "\n",
        "    target_full = self.model.predict(states)\n",
        "\n",
        "    target_full[np.arange(batch_size), actions] = target\n",
        "    self.model.train_on_batch(states, target_full)\n",
        "\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "def play_game(env, agent , scaler, episodes_times = 25, mode = 'test', batch_size = 32):\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        done = False\n",
        "        start_time = datetime.now()\n",
        "       \n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "\n",
        "            if mode == 'train':\n",
        "                agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                agent.replay(batch_size)\n",
        "            \n",
        "        if mode == 'test':\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            actor.integration()\n",
        "            actor.layering()\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)\n",
        "        \n",
        "        state = next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "4d20cec8-1caf-43e0-e079-b1e9d21b2c29"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "mode = 'train'\n",
        "state_size = 3\n",
        "action_size = 3\n",
        "\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "if mode == 'test':\n",
        "    masterbrain.load(f'{models_folder}/gorila_model.h5')\n",
        "\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "        print(row, file=f)\n",
        "else:\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit'\n",
        "        print(row, file=f)\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    e = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    memory = ReplayMemory(state_size, action_size, size=500)\n",
        "    a = Agent(state_size, action_size, brain, memory)\n",
        "    if mode == 'test':\n",
        "        a.epsilon = 0.01\n",
        "    s =lambda: make_scaler(e)\n",
        "    arr = [e,a,s]\n",
        "    envs.append(arr)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: play_game(env[0],env[1],env[2], episodes_times, batch_size, mode)\n",
        "        datas.append(executor.submit(job))\n",
        "\n",
        "if mode == 'train':\n",
        "    masterbrain.save(f'{models_folder}/gorila_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/100 RapTime: 0:01:00.991584 FixedProfit: 1059400\n",
            "Episode: 2/100 RapTime: 0:01:11.596594 FixedProfit: 1019774\n",
            "Episode: 3/100 RapTime: 0:01:10.375688 FixedProfit: 1002458\n",
            "Episode: 4/100 RapTime: 0:01:11.003251 FixedProfit: 1002367\n",
            "Episode: 5/100 RapTime: 0:01:11.019084 FixedProfit: 917775\n",
            "Episode: 6/100 RapTime: 0:01:11.249554 FixedProfit: 998420\n",
            "Episode: 7/100 RapTime: 0:01:10.966038 FixedProfit: 994084\n",
            "Episode: 8/100 RapTime: 0:01:09.745594 FixedProfit: 990606\n",
            "Episode: 9/100 RapTime: 0:01:09.597227 FixedProfit: 1073122\n",
            "Episode: 10/100 RapTime: 0:01:10.680226 FixedProfit: 1031132\n",
            "Episode: 11/100 RapTime: 0:01:11.237511 FixedProfit: 993816\n",
            "Episode: 12/100 RapTime: 0:01:09.634095 FixedProfit: 972850\n",
            "Episode: 13/100 RapTime: 0:01:09.742125 FixedProfit: 997094\n",
            "Episode: 14/100 RapTime: 0:01:09.334288 FixedProfit: 1005187\n",
            "Episode: 15/100 RapTime: 0:01:09.876949 FixedProfit: 1158008\n",
            "Episode: 16/100 RapTime: 0:01:09.724434 FixedProfit: 1000000\n",
            "Episode: 17/100 RapTime: 0:01:09.433873 FixedProfit: 994065\n",
            "Episode: 18/100 RapTime: 0:01:09.790861 FixedProfit: 1000000\n",
            "Episode: 19/100 RapTime: 0:01:09.426648 FixedProfit: 1004749\n",
            "Episode: 20/100 RapTime: 0:01:09.222262 FixedProfit: 1030463\n",
            "Episode: 21/100 RapTime: 0:01:09.225207 FixedProfit: 982717\n",
            "Episode: 22/100 RapTime: 0:01:09.244702 FixedProfit: 1252815\n",
            "Episode: 23/100 RapTime: 0:01:09.304808 FixedProfit: 995758\n",
            "Episode: 24/100 RapTime: 0:01:08.981757 FixedProfit: 1013453\n",
            "Episode: 25/100 RapTime: 0:01:09.354350 FixedProfit: 1000000\n",
            "Episode: 26/100 RapTime: 0:01:09.403389 FixedProfit: 1015267\n",
            "Episode: 27/100 RapTime: 0:01:09.470032 FixedProfit: 1222023\n",
            "Episode: 28/100 RapTime: 0:01:09.441803 FixedProfit: 1101053\n",
            "Episode: 29/100 RapTime: 0:01:09.320215 FixedProfit: 1140904\n",
            "Episode: 30/100 RapTime: 0:01:08.957033 FixedProfit: 999058\n",
            "Episode: 31/100 RapTime: 0:01:09.248326 FixedProfit: 986173\n",
            "Episode: 32/100 RapTime: 0:01:08.920261 FixedProfit: 988736\n",
            "Episode: 33/100 RapTime: 0:01:09.495006 FixedProfit: 999358\n",
            "Episode: 34/100 RapTime: 0:01:09.418540 FixedProfit: 999134\n",
            "Episode: 35/100 RapTime: 0:01:09.623673 FixedProfit: 1000000\n",
            "Episode: 36/100 RapTime: 0:01:09.534897 FixedProfit: 1005201\n",
            "Episode: 37/100 RapTime: 0:01:09.459494 FixedProfit: 989312\n",
            "Episode: 38/100 RapTime: 0:01:09.443933 FixedProfit: 1004728\n",
            "Episode: 39/100 RapTime: 0:01:10.104383 FixedProfit: 1030449\n",
            "Episode: 40/100 RapTime: 0:01:09.655901 FixedProfit: 1009790\n",
            "Episode: 41/100 RapTime: 0:01:09.665778 FixedProfit: 1010114\n",
            "Episode: 42/100 RapTime: 0:01:09.861868 FixedProfit: 1008602\n",
            "Episode: 43/100 RapTime: 0:01:09.538465 FixedProfit: 1013721\n",
            "Episode: 44/100 RapTime: 0:01:09.632931 FixedProfit: 1000000\n",
            "Episode: 45/100 RapTime: 0:01:09.738122 FixedProfit: 994207\n",
            "Episode: 46/100 RapTime: 0:01:08.847440 FixedProfit: 994779\n",
            "Episode: 47/100 RapTime: 0:01:08.792094 FixedProfit: 998291\n",
            "Episode: 48/100 RapTime: 0:01:09.480778 FixedProfit: 1004478\n",
            "Episode: 49/100 RapTime: 0:01:09.035423 FixedProfit: 1024283\n",
            "Episode: 50/100 RapTime: 0:01:09.533485 FixedProfit: 996296\n",
            "Episode: 51/100 RapTime: 0:01:09.401612 FixedProfit: 991307\n",
            "Episode: 52/100 RapTime: 0:01:09.120947 FixedProfit: 1055433\n",
            "Episode: 53/100 RapTime: 0:01:09.327231 FixedProfit: 1001807\n",
            "Episode: 54/100 RapTime: 0:01:09.149638 FixedProfit: 1000000\n",
            "Episode: 55/100 RapTime: 0:01:09.150776 FixedProfit: 995733\n",
            "Episode: 56/100 RapTime: 0:01:08.775708 FixedProfit: 997962\n",
            "Episode: 57/100 RapTime: 0:01:09.059007 FixedProfit: 1007912\n",
            "Episode: 58/100 RapTime: 0:01:09.493431 FixedProfit: 1011813\n",
            "Episode: 59/100 RapTime: 0:01:09.206375 FixedProfit: 1011255\n",
            "Episode: 60/100 RapTime: 0:01:09.330290 FixedProfit: 1008488\n",
            "Episode: 61/100 RapTime: 0:01:08.962875 FixedProfit: 1006604\n",
            "Episode: 62/100 RapTime: 0:01:09.152754 FixedProfit: 1000000\n",
            "Episode: 63/100 RapTime: 0:01:08.874640 FixedProfit: 1000000\n",
            "Episode: 64/100 RapTime: 0:01:09.222547 FixedProfit: 1014781\n",
            "Episode: 65/100 RapTime: 0:01:09.384065 FixedProfit: 1104088\n",
            "Episode: 66/100 RapTime: 0:01:09.262596 FixedProfit: 937642\n",
            "Episode: 67/100 RapTime: 0:01:09.055575 FixedProfit: 996546\n",
            "Episode: 68/100 RapTime: 0:01:08.969913 FixedProfit: 999610\n",
            "Episode: 69/100 RapTime: 0:01:09.131753 FixedProfit: 991398\n",
            "Episode: 70/100 RapTime: 0:01:08.598372 FixedProfit: 982230\n",
            "Episode: 71/100 RapTime: 0:01:09.097444 FixedProfit: 1002467\n",
            "Episode: 72/100 RapTime: 0:01:09.242128 FixedProfit: 975484\n",
            "Episode: 73/100 RapTime: 0:01:08.788217 FixedProfit: 929973\n",
            "Episode: 74/100 RapTime: 0:01:08.504015 FixedProfit: 1000000\n",
            "Episode: 75/100 RapTime: 0:01:08.971060 FixedProfit: 1069118\n",
            "Episode: 76/100 RapTime: 0:01:08.883799 FixedProfit: 987688\n",
            "Episode: 77/100 RapTime: 0:01:08.950293 FixedProfit: 999360\n",
            "Episode: 78/100 RapTime: 0:01:08.909819 FixedProfit: 1001754\n",
            "Episode: 79/100 RapTime: 0:01:08.629408 FixedProfit: 1000000\n",
            "Episode: 80/100 RapTime: 0:01:09.358061 FixedProfit: 1000000\n",
            "Episode: 81/100 RapTime: 0:01:08.827731 FixedProfit: 982750\n",
            "Episode: 82/100 RapTime: 0:01:08.933424 FixedProfit: 1001524\n",
            "Episode: 83/100 RapTime: 0:01:08.703562 FixedProfit: 999854\n",
            "Episode: 84/100 RapTime: 0:01:09.179188 FixedProfit: 935619\n",
            "Episode: 85/100 RapTime: 0:01:09.157195 FixedProfit: 976404\n",
            "Episode: 86/100 RapTime: 0:01:08.558065 FixedProfit: 1007651\n",
            "Episode: 87/100 RapTime: 0:01:08.675451 FixedProfit: 1004409\n",
            "Episode: 88/100 RapTime: 0:01:08.543530 FixedProfit: 947922\n",
            "Episode: 89/100 RapTime: 0:01:08.480625 FixedProfit: 1004500\n",
            "Episode: 90/100 RapTime: 0:01:08.608654 FixedProfit: 1000000\n",
            "Episode: 91/100 RapTime: 0:01:08.620397 FixedProfit: 1001760\n",
            "Episode: 92/100 RapTime: 0:01:08.714835 FixedProfit: 1022269\n",
            "Episode: 93/100 RapTime: 0:01:08.343199 FixedProfit: 1000000\n",
            "Episode: 94/100 RapTime: 0:01:08.639614 FixedProfit: 1044681\n",
            "Episode: 95/100 RapTime: 0:01:08.270433 FixedProfit: 1064842\n",
            "Episode: 96/100 RapTime: 0:01:08.278727 FixedProfit: 1095925\n",
            "Episode: 97/100 RapTime: 0:01:08.540267 FixedProfit: 1064482\n",
            "Episode: 98/100 RapTime: 0:01:08.676469 FixedProfit: 1066018\n",
            "Episode: 99/100 RapTime: 0:01:08.302514 FixedProfit: 1049159\n",
            "Episode: 100/100 RapTime: 0:01:08.881668 FixedProfit: 997786\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}