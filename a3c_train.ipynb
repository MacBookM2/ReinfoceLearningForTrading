{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a3c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "47640943-e9be-427b-b7db-c111434a6c35"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "mode = 'train'\n",
        "name = 'a3c'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class MasterBrain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model(input_, [actor, critic])\n",
        "        mastermodel.compile(optimizer=Adam(lr=lr))\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.masterbrain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.masterbrain.save(name)\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP-xlKoLAwG7"
      },
      "source": [
        "class Leaner:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, num, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "        \n",
        "        self.actor.layering()\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            experiences = []\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                    if len(experiences) == self.batch_size:\n",
        "                        self.critic.valuenetwork(experiences)\n",
        "                        experiences = []\n",
        "\n",
        "                state = next_state\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                actor.integration()\n",
        "                actor.layering()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5158b95a-35b2-4507-e743-f6c49936d7b6"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 50\n",
        "batch_size = 32\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    actor = Actor(brain)\n",
        "    critic = Critic(model)\n",
        "    main = Main(env, actor, critic, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:00:08.005553 FixedProfit: 1015161\n",
            "Episode: 1/50 RapTime: 0:00:08.007402 FixedProfit: 1222170\n",
            "Episode: 1/50 RapTime: 0:00:08.029440 FixedProfit: 1002213\n",
            "Episode: 1/50 RapTime: 0:00:08.037916 FixedProfit: 1029784\n",
            "Episode: 2/50 RapTime: 0:00:07.559910 FixedProfit: 1198957\n",
            "Episode: 2/50 RapTime: 0:00:07.588205 FixedProfit: 1355245\n",
            "Episode: 2/50 RapTime: 0:00:07.644385 FixedProfit: 1055519\n",
            "Episode: 2/50 RapTime: 0:00:07.625145 FixedProfit: 1057350\n",
            "Episode: 3/50 RapTime: 0:00:07.612827 FixedProfit: 1192527\n",
            "Episode: 3/50 RapTime: 0:00:07.608479 FixedProfit: 843360\n",
            "Episode: 3/50 RapTime: 0:00:07.653123 FixedProfit: 1091584\n",
            "Episode: 3/50 RapTime: 0:00:07.662312 FixedProfit: 1027769\n",
            "Episode: 4/50 RapTime: 0:00:07.655590 FixedProfit: 1193827\n",
            "Episode: 4/50 RapTime: 0:00:07.606124 FixedProfit: 1139664\n",
            "Episode: 4/50 RapTime: 0:00:07.705336 FixedProfit: 1092869\n",
            "Episode: 4/50 RapTime: 0:00:07.662507 FixedProfit: 1217130\n",
            "Episode: 5/50 RapTime: 0:00:07.577410 FixedProfit: 1101396\n",
            "Episode: 5/50 RapTime: 0:00:07.670972 FixedProfit: 1197165\n",
            "Episode: 5/50 RapTime: 0:00:07.658194 FixedProfit: 1089218\n",
            "Episode: 5/50 RapTime: 0:00:07.653651 FixedProfit: 1161042\n",
            "Episode: 6/50 RapTime: 0:00:07.579613 FixedProfit: 1009483\n",
            "Episode: 6/50 RapTime: 0:00:07.650120 FixedProfit: 1197165\n",
            "Episode: 6/50 RapTime: 0:00:07.681366 FixedProfit: 1196072\n",
            "Episode: 6/50 RapTime: 0:00:07.694779 FixedProfit: 1097725\n",
            "Episode: 7/50 RapTime: 0:00:07.731943 FixedProfit: 913961\n",
            "Episode: 7/50 RapTime: 0:00:07.758029 FixedProfit: 1197165\n",
            "Episode: 7/50 RapTime: 0:00:07.710645 FixedProfit: 1108765\n",
            "Episode: 7/50 RapTime: 0:00:07.759627 FixedProfit: 1017273\n",
            "Episode: 8/50 RapTime: 0:00:07.666283 FixedProfit: 1112511\n",
            "Episode: 8/50 RapTime: 0:00:07.719809 FixedProfit: 1197165\n",
            "Episode: 8/50 RapTime: 0:00:07.699212 FixedProfit: 1027413\n",
            "Episode: 8/50 RapTime: 0:00:07.703759 FixedProfit: 1005887\n",
            "Episode: 9/50 RapTime: 0:00:07.582672 FixedProfit: 1081905\n",
            "Episode: 9/50 RapTime: 0:00:07.678089 FixedProfit: 1197165\n",
            "Episode: 9/50 RapTime: 0:00:07.570029 FixedProfit: 1223885\n",
            "Episode: 9/50 RapTime: 0:00:07.652167 FixedProfit: 1310257\n",
            "Episode: 10/50 RapTime: 0:00:07.657381 FixedProfit: 1202511\n",
            "Episode: 10/50 RapTime: 0:00:07.653402 FixedProfit: 1197165\n",
            "Episode: 10/50 RapTime: 0:00:07.674856 FixedProfit: 1168763\n",
            "Episode: 10/50 RapTime: 0:00:07.669863 FixedProfit: 1106993\n",
            "Episode: 11/50 RapTime: 0:00:07.807176 FixedProfit: 944451\n",
            "Episode: 11/50 RapTime: 0:00:07.721258 FixedProfit: 1197165\n",
            "Episode: 11/50 RapTime: 0:00:07.780441 FixedProfit: 1135674\n",
            "Episode: 11/50 RapTime: 0:00:07.788013 FixedProfit: 1052743\n",
            "Episode: 12/50 RapTime: 0:00:07.929391 FixedProfit: 953678\n",
            "Episode: 12/50 RapTime: 0:00:07.990199 FixedProfit: 1197165\n",
            "Episode: 12/50 RapTime: 0:00:07.941261 FixedProfit: 1153104\n",
            "Episode: 12/50 RapTime: 0:00:07.928393 FixedProfit: 1138487\n",
            "Episode: 13/50 RapTime: 0:00:07.648358 FixedProfit: 955479\n",
            "Episode: 13/50 RapTime: 0:00:07.678195 FixedProfit: 1197165\n",
            "Episode: 13/50 RapTime: 0:00:07.628278 FixedProfit: 982462\n",
            "Episode: 13/50 RapTime: 0:00:07.659748 FixedProfit: 1200181\n",
            "Episode: 14/50 RapTime: 0:00:07.653721 FixedProfit: 1027390\n",
            "Episode: 14/50 RapTime: 0:00:07.664828 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:07.658016 FixedProfit: 1014917\n",
            "Episode: 14/50 RapTime: 0:00:07.657258 FixedProfit: 1200243\n",
            "Episode: 15/50 RapTime: 0:00:07.644494 FixedProfit: 1358270\n",
            "Episode: 15/50 RapTime: 0:00:07.651913 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:07.642151 FixedProfit: 1076621\n",
            "Episode: 15/50 RapTime: 0:00:07.677586 FixedProfit: 974822\n",
            "Episode: 16/50 RapTime: 0:00:07.636790 FixedProfit: 1111481\n",
            "Episode: 16/50 RapTime: 0:00:07.664663 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:07.667003 FixedProfit: 1126393\n",
            "Episode: 16/50 RapTime: 0:00:07.693554 FixedProfit: 1434432\n",
            "Episode: 17/50 RapTime: 0:00:07.724525 FixedProfit: 1153125\n",
            "Episode: 17/50 RapTime: 0:00:07.699137 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:07.681426 FixedProfit: 1087592\n",
            "Episode: 17/50 RapTime: 0:00:07.706567 FixedProfit: 1084304\n",
            "Episode: 18/50 RapTime: 0:00:07.660462 FixedProfit: 1269008\n",
            "Episode: 18/50 RapTime: 0:00:07.719906 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:07.724250 FixedProfit: 1197423\n",
            "Episode: 18/50 RapTime: 0:00:07.733158 FixedProfit: 1279766\n",
            "Episode: 19/50 RapTime: 0:00:07.632288 FixedProfit: 1034165\n",
            "Episode: 19/50 RapTime: 0:00:07.618984 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:07.634178 FixedProfit: 964620\n",
            "Episode: 19/50 RapTime: 0:00:07.652747 FixedProfit: 1174619\n",
            "Episode: 20/50 RapTime: 0:00:07.595354 FixedProfit: 1220499\n",
            "Episode: 20/50 RapTime: 0:00:07.616797 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:07.646777 FixedProfit: 989666\n",
            "Episode: 20/50 RapTime: 0:00:07.596914 FixedProfit: 1107052\n",
            "Episode: 21/50 RapTime: 0:00:07.597902 FixedProfit: 1148832\n",
            "Episode: 21/50 RapTime: 0:00:07.610195 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:07.590139 FixedProfit: 1174052\n",
            "Episode: 21/50 RapTime: 0:00:07.606670 FixedProfit: 1338012\n",
            "Episode: 22/50 RapTime: 0:00:07.558424 FixedProfit: 1172776\n",
            "Episode: 22/50 RapTime: 0:00:07.626362 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:07.662039 FixedProfit: 1015763\n",
            "Episode: 22/50 RapTime: 0:00:07.671397 FixedProfit: 1164638\n",
            "Episode: 23/50 RapTime: 0:00:07.673450 FixedProfit: 1129071\n",
            "Episode: 23/50 RapTime: 0:00:07.686138 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:07.645828 FixedProfit: 1098190\n",
            "Episode: 23/50 RapTime: 0:00:07.622635 FixedProfit: 1162302\n",
            "Episode: 24/50 RapTime: 0:00:07.503653 FixedProfit: 1288225\n",
            "Episode: 24/50 RapTime: 0:00:07.597834 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:07.578399 FixedProfit: 1004218\n",
            "Episode: 24/50 RapTime: 0:00:07.633353 FixedProfit: 898952\n",
            "Episode: 25/50 RapTime: 0:00:07.537810 FixedProfit: 1097918\n",
            "Episode: 25/50 RapTime: 0:00:07.570026 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:07.563947 FixedProfit: 1139812\n",
            "Episode: 25/50 RapTime: 0:00:07.516132 FixedProfit: 966393\n",
            "Episode: 26/50 RapTime: 0:00:07.524655 FixedProfit: 1079037\n",
            "Episode: 26/50 RapTime: 0:00:07.537655 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:07.543769 FixedProfit: 852892\n",
            "Episode: 26/50 RapTime: 0:00:07.538734 FixedProfit: 847081\n",
            "Episode: 27/50 RapTime: 0:00:07.608328 FixedProfit: 1113016\n",
            "Episode: 27/50 RapTime: 0:00:07.581868 FixedProfit: 1187637\n",
            "Episode: 27/50 RapTime: 0:00:07.620964 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:07.671562 FixedProfit: 1063171\n",
            "Episode: 28/50 RapTime: 0:00:07.660582 FixedProfit: 1090735\n",
            "Episode: 28/50 RapTime: 0:00:07.609552 FixedProfit: 950324\n",
            "Episode: 28/50 RapTime: 0:00:07.591915 FixedProfit: 1243963\n",
            "Episode: 28/50 RapTime: 0:00:07.688570 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:07.666720 FixedProfit: 974775\n",
            "Episode: 29/50 RapTime: 0:00:07.604083 FixedProfit: 1174711\n",
            "Episode: 29/50 RapTime: 0:00:07.624486 FixedProfit: 1066484\n",
            "Episode: 29/50 RapTime: 0:00:07.686014 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:07.599221 FixedProfit: 1191623\n",
            "Episode: 30/50 RapTime: 0:00:07.582873 FixedProfit: 1099296\n",
            "Episode: 30/50 RapTime: 0:00:07.605442 FixedProfit: 1090055\n",
            "Episode: 30/50 RapTime: 0:00:07.563745 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:07.627055 FixedProfit: 958895\n",
            "Episode: 31/50 RapTime: 0:00:07.572895 FixedProfit: 1154963\n",
            "Episode: 31/50 RapTime: 0:00:07.485616 FixedProfit: 991408\n",
            "Episode: 31/50 RapTime: 0:00:07.585151 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:07.591398 FixedProfit: 1241293\n",
            "Episode: 32/50 RapTime: 0:00:07.543606 FixedProfit: 1001379\n",
            "Episode: 32/50 RapTime: 0:00:07.551755 FixedProfit: 1081393\n",
            "Episode: 32/50 RapTime: 0:00:07.536889 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:07.521231 FixedProfit: 1002127\n",
            "Episode: 33/50 RapTime: 0:00:07.528614 FixedProfit: 976590\n",
            "Episode: 33/50 RapTime: 0:00:07.526620 FixedProfit: 1104513\n",
            "Episode: 33/50 RapTime: 0:00:07.533423 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:07.452590 FixedProfit: 953888\n",
            "Episode: 34/50 RapTime: 0:00:07.509758 FixedProfit: 988323\n",
            "Episode: 34/50 RapTime: 0:00:07.485031 FixedProfit: 1055414\n",
            "Episode: 34/50 RapTime: 0:00:07.567592 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:07.517081 FixedProfit: 1098768\n",
            "Episode: 35/50 RapTime: 0:00:07.577916 FixedProfit: 1113772\n",
            "Episode: 35/50 RapTime: 0:00:07.590276 FixedProfit: 1098588\n",
            "Episode: 35/50 RapTime: 0:00:07.582915 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:07.532577 FixedProfit: 1075740\n",
            "Episode: 36/50 RapTime: 0:00:07.585798 FixedProfit: 976426\n",
            "Episode: 36/50 RapTime: 0:00:07.562136 FixedProfit: 1030069\n",
            "Episode: 36/50 RapTime: 0:00:07.592897 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:07.603610 FixedProfit: 923563\n",
            "Episode: 37/50 RapTime: 0:00:07.610780 FixedProfit: 1158852\n",
            "Episode: 37/50 RapTime: 0:00:07.671697 FixedProfit: 1069115\n",
            "Episode: 37/50 RapTime: 0:00:07.642670 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:07.596070 FixedProfit: 1087086\n",
            "Episode: 38/50 RapTime: 0:00:07.577497 FixedProfit: 923160\n",
            "Episode: 38/50 RapTime: 0:00:07.636628 FixedProfit: 957279\n",
            "Episode: 38/50 RapTime: 0:00:07.592563 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:07.572329 FixedProfit: 1392994\n",
            "Episode: 39/50 RapTime: 0:00:07.554227 FixedProfit: 905565\n",
            "Episode: 39/50 RapTime: 0:00:07.629534 FixedProfit: 1299855\n",
            "Episode: 39/50 RapTime: 0:00:07.614094 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:07.582585 FixedProfit: 1296555\n",
            "Episode: 40/50 RapTime: 0:00:07.585750 FixedProfit: 921718\n",
            "Episode: 40/50 RapTime: 0:00:07.500005 FixedProfit: 1054484\n",
            "Episode: 40/50 RapTime: 0:00:07.514946 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:07.553819 FixedProfit: 1077712\n",
            "Episode: 41/50 RapTime: 0:00:07.581789 FixedProfit: 862624\n",
            "Episode: 41/50 RapTime: 0:00:07.651619 FixedProfit: 949620\n",
            "Episode: 41/50 RapTime: 0:00:07.598869 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:07.544018 FixedProfit: 936174\n",
            "Episode: 42/50 RapTime: 0:00:07.556336 FixedProfit: 948320\n",
            "Episode: 42/50 RapTime: 0:00:07.570461 FixedProfit: 1081160\n",
            "Episode: 42/50 RapTime: 0:00:07.516886 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:07.513636 FixedProfit: 986415\n",
            "Episode: 43/50 RapTime: 0:00:07.549307 FixedProfit: 1043190\n",
            "Episode: 43/50 RapTime: 0:00:07.564130 FixedProfit: 1120696\n",
            "Episode: 43/50 RapTime: 0:00:07.616452 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:07.560817 FixedProfit: 1091548\n",
            "Episode: 44/50 RapTime: 0:00:07.515227 FixedProfit: 1152387\n",
            "Episode: 44/50 RapTime: 0:00:07.566067 FixedProfit: 966853\n",
            "Episode: 44/50 RapTime: 0:00:07.528758 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:07.647720 FixedProfit: 1218468\n",
            "Episode: 45/50 RapTime: 0:00:07.615994 FixedProfit: 1070433\n",
            "Episode: 45/50 RapTime: 0:00:07.599351 FixedProfit: 1020190\n",
            "Episode: 45/50 RapTime: 0:00:07.650544 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:07.612729 FixedProfit: 1162612\n",
            "Episode: 46/50 RapTime: 0:00:07.661317 FixedProfit: 1084691\n",
            "Episode: 46/50 RapTime: 0:00:07.688295 FixedProfit: 1152508\n",
            "Episode: 46/50 RapTime: 0:00:07.662200 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:07.650906 FixedProfit: 1076051\n",
            "Episode: 47/50 RapTime: 0:00:07.629601 FixedProfit: 1182100\n",
            "Episode: 47/50 RapTime: 0:00:07.605233 FixedProfit: 1135566\n",
            "Episode: 47/50 RapTime: 0:00:07.682239 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:07.549966 FixedProfit: 924149\n",
            "Episode: 48/50 RapTime: 0:00:07.536437 FixedProfit: 1137415\n",
            "Episode: 48/50 RapTime: 0:00:07.560827 FixedProfit: 1238365\n",
            "Episode: 48/50 RapTime: 0:00:07.572711 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:07.545796 FixedProfit: 1062690\n",
            "Episode: 49/50 RapTime: 0:00:07.555696 FixedProfit: 1272252\n",
            "Episode: 49/50 RapTime: 0:00:07.495939 FixedProfit: 956969\n",
            "Episode: 49/50 RapTime: 0:00:07.467988 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:07.472700 FixedProfit: 1107489\n",
            "Episode: 50/50 RapTime: 0:00:07.654284 FixedProfit: 1073077\n",
            "Episode: 50/50 RapTime: 0:00:07.559933 FixedProfit: 1044470\n",
            "Episode: 50/50 RapTime: 0:00:07.425851 FixedProfit: 1197165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}