{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "impala_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/impala_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "a66536e2-f615-4f1b-fee8-f7c0d05f49ba"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPool1D, Activation, concatenate\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'train'\n",
        "name = 'impala'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m48th46c8otj"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model):\n",
        "        self.model = model\n",
        "        self.n_action = 3\n",
        "        self.gamma = 0.9\n",
        "        self.alfa = 0.5\n",
        "        self.beta = 0.00025\n",
        "\n",
        "    def valuenetwork(self, state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            actions = tf.one_hot(action, self.n_action) # (10, 3)\n",
        "\n",
        "            state = state.reshape(1,10,3)\n",
        "            prev_action = prev_action.reshape(1,10,1)\n",
        "            prev_reward = prev_reward.reshape(1,10,1)\n",
        "\n",
        "            pai, v_theta  = self.model([state, prev_action, prev_reward])\n",
        "\n",
        "            pai = tf.reshape(pai, [10,3]) # (10, 3)\n",
        "\n",
        "            actions = tf.cast(actions, tf.float32)\n",
        "\n",
        "            pais = tf.reduce_sum(actions * pai, axis=1, keepdims=True)\n",
        "\n",
        "            mu = self._reshape_and_cast(mu, 3)\n",
        "            ratio = tf.math.divide_no_nan(pais, mu)\n",
        "            rhoi = ci = tf.minimum(1.0, ratio)\n",
        "\n",
        "            n_num, _ = ratio.shape\n",
        "\n",
        "            rhoi = self._reshape_and_cast(rhoi,3)\n",
        "            ci = self._reshape_and_cast(ci,3)\n",
        "            reward = self._reshape_and_cast(reward,1)\n",
        "            v_next = self._reshape_and_cast(v_next,1)\n",
        "            v = self._reshape_and_cast(v,1)\n",
        "            v_theta = self._reshape_and_cast(v_theta,1)\n",
        "\n",
        "            b4_delta_v = (reward + self.gamma * v_next - v)\n",
        "            b4_delta_v  =  tf.cast(b4_delta_v, tf.float32)\n",
        "            delta_v = tf.multiply(rhoi, b4_delta_v)\n",
        "            delta_v = self._reshape_and_cast(delta_v,3)\n",
        "            v_trace =v + self._sigma(ci, delta_v, n_num)\n",
        "            total_loss = self._compute_baseline_loss(v_trace - v_theta)\n",
        "            total_loss += self._compute_policy_gradient_loss(pai, actions, delta_v)\n",
        "            total_loss += self._compute_entropy_loss(pai)\n",
        "\n",
        "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "    def _reshape_and_cast(self, x, num):\n",
        "        x = tf.reshape(x, [10,num])\n",
        "        x  =  tf.cast(x, tf.float32)\n",
        "        return x\n",
        "\n",
        "    def _infinite_product(self, x, max_num):\n",
        "        num = tf.ones([3, ], tf.float32)\n",
        "        for i in range(max_num):\n",
        "            num *= x[i]   \n",
        "        return num\n",
        "\n",
        "    def _sigma(self, x, delta_v, b):\n",
        "        \n",
        "        num = 0.0\n",
        "        for i in range(b):\n",
        "            num += pow(self.gamma, i) * self._infinite_product(x, i) * delta_v[i]\n",
        "        return tf.cast(num, tf.float32)\n",
        "\n",
        "    def _compute_baseline_loss(self, advantages):\n",
        "        return .5 * tf.reduce_sum(tf.square(advantages))\n",
        "\n",
        "    def _compute_policy_gradient_loss(self, logits, actions, advantages):\n",
        "        cross_entropy = tf.losses.categorical_crossentropy(y_true=actions, y_pred=logits)\n",
        "        cross_entropy = tf.reshape(cross_entropy, [10,1])\n",
        "        advantages = tf.stop_gradient(advantages)\n",
        "        policy_gradient_loss_per_timestep = cross_entropy * advantages\n",
        "        return tf.reduce_sum(policy_gradient_loss_per_timestep)\n",
        "\n",
        "    def _compute_entropy_loss(self, logits):\n",
        "        log_policy = tf.math.log(logits)\n",
        "        entropy_per_timestep = tf.reduce_sum(-logits * log_policy, axis=-1)\n",
        "        return -tf.reduce_sum(entropy_per_timestep)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcPU3_nDFvro"
      },
      "source": [
        "class Learner(Critic):\n",
        "    def __init__(self):\n",
        "\n",
        "        conv_filter = 12\n",
        "        units = 28\n",
        "        look_back = 10\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "\n",
        "        input1_ = Input(shape=(look_back, 3))\n",
        "        input2_ = Input(shape=(look_back, 1))\n",
        "        input3_ = Input(shape=(look_back, 1))\n",
        "\n",
        "        x = Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\")(input1_)\n",
        "        x = MaxPool1D(pool_size=1, padding='same')(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        combined = concatenate([x, input2_, input3_],axis=-1)\n",
        "        common = LSTM(units, return_sequences=True)(combined)\n",
        "        common = Dense(units, kernel_initializer='random_uniform')(common)\n",
        "        common = Activation(\"relu\")(common)\n",
        "\n",
        "        actor  = Dense(3, activation=\"softmax\")(common)\n",
        "        critic = Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model([input1_, input2_, input3_], [actor, critic])\n",
        "        mastermodel.compile(loss = \"mean_absolute_error\", optimizer=opt)\n",
        "        mastermodel.summary()\n",
        "        #dot_img_file = './f\"{name}_model.png\"'\n",
        "        #tf.keras.utils.plot_model(mastermodel, to_file=dot_img_file, show_shapes=True)\n",
        "        self.mastermodel = mastermodel\n",
        "        super().__init__(mastermodel)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        length = memory.max_length_memory()\n",
        "        for i in range(length):\n",
        "            min = i * 10\n",
        "            max = (i + 1) * 10\n",
        "            state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu = memory.get_experiences(min, max)\n",
        "            self.valuenetwork(state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu)\n",
        "\n",
        "    def layering(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        conv_filter = 12\n",
        "        units = 28\n",
        "        look_back = 10\n",
        "        opt = Adam(learning_rate=0.001)\n",
        "\n",
        "        input1_ = Input(shape=(look_back, 3))\n",
        "        input2_ = Input(shape=(look_back, 1))\n",
        "        input3_ = Input(shape=(look_back, 1))\n",
        "\n",
        "        x = Conv1D(filters=conv_filter, kernel_size=1, padding=\"same\", activation=\"tanh\")(input1_)\n",
        "        x = MaxPool1D(pool_size=1, padding='same')(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        combined = concatenate([x, input2_, input3_],axis=-1)\n",
        "        common = LSTM(units, return_sequences=True)(combined)\n",
        "        common = Dense(units, kernel_initializer='random_uniform')(common)\n",
        "        common = Activation(\"relu\")(common)\n",
        "\n",
        "        actor  = Dense(3, activation=\"softmax\")(common)\n",
        "        critic = Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model([input1_, input2_, input3_], [actor, critic])\n",
        "        model.compile(loss = \"mean_absolute_error\", optimizer=opt)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self, learner):\n",
        "\n",
        "        self.learner = learner\n",
        "        self.model = learner.mastermodel\n",
        "        self.n_action = 3\n",
        "        self.state_arr = np.array([])\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.array([])\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "        super().__init__()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state_arr = np.empty((0,3), int)\n",
        "        self.p_action_arr = np.array([])\n",
        "        self.p_reward_arr = np.array([])\n",
        "\n",
        "        self.next_state_arr = np.empty((0,3), int)\n",
        "        self.action_arr = np.array([])\n",
        "        self.reward_arr = np.array([])\n",
        "\n",
        "    def policynetwork(self, state, prev_action, prev_reward):\n",
        "\n",
        "        if len(self.state_arr) == 10:\n",
        "            self.state_arr[0:-1] = self.state_arr[1:]\n",
        "            self.p_action_arr[0:-1] = self.p_action_arr[1:]\n",
        "            self.p_reward_arr[0:-1] = self.p_reward_arr[1:]\n",
        "            self.state_arr[-1] = state\n",
        "            self.p_action_arr[-1] = prev_action\n",
        "            self.p_reward_arr[-1] = prev_reward\n",
        "            tmp_state = copy.deepcopy(self.state_arr)\n",
        "            tmp_action = copy.deepcopy(self.p_action_arr)\n",
        "            tmp_reward = copy.deepcopy(self.p_reward_arr)\n",
        "            tmp_state = tmp_state.reshape(1,10,3)\n",
        "            tmp_action = tmp_action.reshape(1,10,1)\n",
        "            tmp_reward = tmp_reward.reshape(1,10,1)\n",
        "            act_p, v = self.model([tmp_state, tmp_action, tmp_reward]) # [-1.03245259 -0.55189404  0.87892511] 1 0\n",
        "            v_np = v.numpy()\n",
        "            p_np = act_p.numpy()\n",
        "            one_hot_actions = tf.one_hot([0,1,2], 3)\n",
        "            act_p = p_np[0][9]\n",
        "            mu = tf.reduce_sum(one_hot_actions * act_p, axis=1)\n",
        "            return np.random.choice(3, p=p_np[0][9]), v_np[0][9][0], mu.numpy()\n",
        "\n",
        "        self.state_arr = np.append(self.state_arr, np.array([state]), axis=0)\n",
        "        self.p_action_arr = np.append(self.p_action_arr, np.array([prev_action]))\n",
        "        self.p_reward_arr = np.append(self.p_reward_arr, np.array([prev_reward]))\n",
        "        return 1, 1, [0.0, 1.0, 0.0]\n",
        "\n",
        "    def policynetwork_next(self, next_state, action, reward):\n",
        "\n",
        "        if len(self.next_state_arr) == 10:\n",
        "            self.next_state_arr[0:-1] = self.next_state_arr[1:]\n",
        "            self.action_arr[0:-1] = self.action_arr[1:]\n",
        "            self.reward_arr[0:-1] = self.reward_arr[1:]\n",
        "            self.next_state_arr[-1] = next_state\n",
        "            self.action_arr[-1] = action\n",
        "            self.reward_arr[-1] = reward\n",
        "\n",
        "            tmp_n_state = copy.deepcopy(self.next_state_arr)\n",
        "            tmp_action = copy.deepcopy(self.action_arr)\n",
        "            tmp_reward = copy.deepcopy(self.reward_arr)\n",
        "            tmp_n_state = tmp_n_state.reshape(1,10,3)\n",
        "            tmp_action = tmp_action.reshape(1,10,1)\n",
        "            tmp_reward = tmp_reward.reshape(1,10,1)\n",
        "\n",
        "            _, v_next = self.model([tmp_n_state, tmp_action, tmp_reward])\n",
        "            v_next_np = v_next.numpy()\n",
        "            return v_next_np[0][9][0]\n",
        "\n",
        "        self.next_state_arr = np.append(self.next_state_arr, np.array([next_state]), axis=0)\n",
        "        self.action_arr = np.append(self.action_arr, np.array([action]))\n",
        "        self.reward_arr = np.append(self.reward_arr, np.array([reward]))\n",
        "        return 1.0\n",
        "\n",
        "    def load(self, name):\n",
        "        self.learner.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.learner.save(name)\n",
        "\n",
        "    def integration(self):\n",
        "        self.learner.layering(self.model)\n",
        "\n",
        "    def placement(self, memory):\n",
        "        self.learner.placement(memory)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4-NrrtJBQWj"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : np.ndarray = np.empty((0,3), int)\n",
        "    next_state : np.ndarray = np.empty((0,3), int)\n",
        "    prev_action : np.ndarray = np.array([])\n",
        "    action : np.ndarray = np.array([])\n",
        "    prev_reward : np.ndarray = np.array([])\n",
        "    reward : np.ndarray = np.array([])\n",
        "    done : np.ndarray = np.array([])\n",
        "    v : np.ndarray = np.array([])\n",
        "    v_next : np.ndarray = np.array([])\n",
        "    mu : np.ndarray = np.empty((0,3), int)\n",
        "    minibatch_size : int = 64\n",
        "\n",
        "    def append_experiences(self, state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu):\n",
        "        self.state = np.append(self.state, np.array([state]), axis=0)\n",
        "        self.next_state = np.append(self.next_state, np.array([next_state]), axis=0)\n",
        "        self.prev_action = np.append(self.prev_action, np.array(prev_action))\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.prev_reward = np.append(self.prev_reward, np.array(prev_reward))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        self.done = np.append(self.done, np.array(done))\n",
        "        self.v = np.append(self.v, np.array(v))\n",
        "        self.v_next = np.append(self.v_next, np.array(v_next))\n",
        "        self.mu = np.append(self.mu, np.array([mu]), axis=0)\n",
        "\n",
        "    def max_length_memory(self):\n",
        "        max_len = len(self.state)\n",
        "        max_len = int(max_len/10)\n",
        "\n",
        "        return max_len\n",
        "\n",
        "    def get_experiences(self, min, max):\n",
        "        state, next_state, mu = np.empty((0,3), int), np.empty((0,3), int), np.empty((0,3), int)\n",
        "        prev_action, action, prev_reward, reward, done, v, v_next = np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
        "        for i in range(min, max):\n",
        "            state = np.append(state, np.array([self.state[i]]), axis=0)\n",
        "            next_state = np.append(next_state, self.action[i])\n",
        "            prev_action = np.append(prev_action, self.prev_action[i])\n",
        "            action = np.append(action, self.action[i])\n",
        "            prev_reward = np.append(prev_reward, self.prev_reward[i])\n",
        "            reward = np.append(reward, self.reward[i])\n",
        "            done = np.append(done, self.done[i])\n",
        "            v = np.append(v, self.v[i])\n",
        "            v_next = np.append(v_next, self.v_next[i])\n",
        "            mu = np.append(mu, np.array([self.mu[i]]), axis=0)\n",
        "\n",
        "        return state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, num, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "        \n",
        "        self.actor.integration()\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            self.actor.reset()\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory()\n",
        "            prev_action = 1\n",
        "            prev_reward = 0\n",
        "            i = 0\n",
        "    \n",
        "            while not done:\n",
        "                action, v, mu = self.actor.policynetwork(state, prev_action, prev_reward)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "                v_next = self.actor.policynetwork_next(next_state, action, reward)\n",
        "\n",
        "                if (i > 10) and (self.mode == 'train'):\n",
        "                    memory.append_experiences(state, next_state, prev_action, action, prev_reward, reward, done, v, v_next, mu)\n",
        "\n",
        "                state = next_state\n",
        "                prev_action = action\n",
        "                prev_reward = reward\n",
        "                i += 1\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                self.actor.placement(memory)\n",
        "                self.actor.integration()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd645348-4c3f-41b9-fb44-18f71378e9c8"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "Learner = Learner()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    actor = Actor(Learner)\n",
        "    main = Main(env, actor, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 10, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 10, 12)       48          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 10, 12)       0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 10, 12)       0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 14)       0           activation[0][0]                 \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 10, 28)       4816        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10, 28)       812         lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 10, 28)       0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10, 3)        87          activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10, 1)        29          activation_1[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,792\n",
            "Trainable params: 5,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 10, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 10, 12)       48          input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 10, 12)       0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 10, 12)       0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 10, 14)       0           activation_2[0][0]               \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 10, 28)       4816        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10, 28)       812         lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 10, 28)       0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10, 3)        87          activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10, 1)        29          activation_3[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,792\n",
            "Trainable params: 5,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 10, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 10, 12)       48          input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 10, 12)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 10, 12)       0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 10, 14)       0           activation_4[0][0]               \n",
            "                                                                 input_8[0][0]                    \n",
            "                                                                 input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 10, 28)       4816        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10, 28)       812         lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 10, 28)       0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 10, 3)        87          activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 10, 1)        29          activation_5[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,792\n",
            "Trainable params: 5,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 10, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 10, 12)       48          input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 10, 12)       0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 10, 12)       0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 10, 14)       0           activation_6[0][0]               \n",
            "                                                                 input_11[0][0]                   \n",
            "                                                                 input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 10, 28)       4816        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 10, 28)       812         lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 10, 28)       0           dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 10, 3)        87          activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 10, 1)        29          activation_7[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,792\n",
            "Trainable params: 5,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 10, 3)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 10, 12)       48          input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 10, 12)       0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 10, 12)       0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_15 (InputLayer)           [(None, 10, 1)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 10, 14)       0           activation_8[0][0]               \n",
            "                                                                 input_14[0][0]                   \n",
            "                                                                 input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 10, 28)       4816        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 10, 28)       812         lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 10, 28)       0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 10, 3)        87          activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 10, 1)        29          activation_9[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,792\n",
            "Trainable params: 5,792\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:55.473380 FixedProfit: 1064209\n",
            "Episode: 1/100 RapTime: 0:00:55.237617 FixedProfit: 1195191\n",
            "Episode: 1/100 RapTime: 0:00:55.287154 FixedProfit: 1063032\n",
            "Episode: 1/100 RapTime: 0:00:55.580594 FixedProfit: 1123402\n",
            "Episode: 2/100 RapTime: 0:00:52.241467 FixedProfit: 1048540\n",
            "Episode: 2/100 RapTime: 0:00:52.470987 FixedProfit: 1066808\n",
            "Episode: 2/100 RapTime: 0:00:52.371259 FixedProfit: 1119763\n",
            "Episode: 2/100 RapTime: 0:00:52.492739 FixedProfit: 1156028\n",
            "Episode: 3/100 RapTime: 0:00:51.772562 FixedProfit: 1111927\n",
            "Episode: 3/100 RapTime: 0:00:51.891794 FixedProfit: 1197755\n",
            "Episode: 3/100 RapTime: 0:00:52.096282 FixedProfit: 950343\n",
            "Episode: 3/100 RapTime: 0:00:52.100243 FixedProfit: 999116\n",
            "Episode: 4/100 RapTime: 0:00:51.730773 FixedProfit: 905021\n",
            "Episode: 4/100 RapTime: 0:00:51.709310 FixedProfit: 1034132\n",
            "Episode: 4/100 RapTime: 0:00:51.570506 FixedProfit: 1191091\n",
            "Episode: 4/100 RapTime: 0:00:51.936259 FixedProfit: 1120647\n",
            "Episode: 5/100 RapTime: 0:00:51.784606 FixedProfit: 1057862\n",
            "Episode: 5/100 RapTime: 0:00:52.028256 FixedProfit: 1089138\n",
            "Episode: 5/100 RapTime: 0:00:52.009452 FixedProfit: 898002\n",
            "Episode: 5/100 RapTime: 0:00:51.920841 FixedProfit: 1061047\n",
            "Episode: 6/100 RapTime: 0:00:52.134932 FixedProfit: 1120710\n",
            "Episode: 6/100 RapTime: 0:00:52.495238 FixedProfit: 1105142\n",
            "Episode: 6/100 RapTime: 0:00:52.339970 FixedProfit: 1029762\n",
            "Episode: 6/100 RapTime: 0:00:52.179726 FixedProfit: 1048464\n",
            "Episode: 7/100 RapTime: 0:00:51.706511 FixedProfit: 1102482\n",
            "Episode: 7/100 RapTime: 0:00:51.908748 FixedProfit: 1157750\n",
            "Episode: 7/100 RapTime: 0:00:51.620806 FixedProfit: 1026909\n",
            "Episode: 7/100 RapTime: 0:00:51.697794 FixedProfit: 1015488\n",
            "Episode: 8/100 RapTime: 0:00:51.689529 FixedProfit: 1045311\n",
            "Episode: 8/100 RapTime: 0:00:51.708624 FixedProfit: 862775\n",
            "Episode: 8/100 RapTime: 0:00:51.829204 FixedProfit: 1152785\n",
            "Episode: 8/100 RapTime: 0:00:51.760096 FixedProfit: 1464520\n",
            "Episode: 9/100 RapTime: 0:00:52.215964 FixedProfit: 1239910\n",
            "Episode: 9/100 RapTime: 0:00:52.132695 FixedProfit: 1080408\n",
            "Episode: 9/100 RapTime: 0:00:51.920644 FixedProfit: 1041624\n",
            "Episode: 9/100 RapTime: 0:00:52.116973 FixedProfit: 1251168\n",
            "Episode: 10/100 RapTime: 0:00:51.934468 FixedProfit: 975985\n",
            "Episode: 10/100 RapTime: 0:00:52.131759 FixedProfit: 915579\n",
            "Episode: 10/100 RapTime: 0:00:52.251728 FixedProfit: 1169696\n",
            "Episode: 10/100 RapTime: 0:00:52.209241 FixedProfit: 956302\n",
            "Episode: 11/100 RapTime: 0:00:51.729911 FixedProfit: 1170299\n",
            "Episode: 11/100 RapTime: 0:00:51.685562 FixedProfit: 1018800\n",
            "Episode: 11/100 RapTime: 0:00:51.560838 FixedProfit: 881325\n",
            "Episode: 11/100 RapTime: 0:00:51.254892 FixedProfit: 993694\n",
            "Episode: 12/100 RapTime: 0:00:51.693412 FixedProfit: 1059783\n",
            "Episode: 12/100 RapTime: 0:00:51.656335 FixedProfit: 901110\n",
            "Episode: 12/100 RapTime: 0:00:52.053092 FixedProfit: 1094027\n",
            "Episode: 12/100 RapTime: 0:00:51.597943 FixedProfit: 1416705\n",
            "Episode: 13/100 RapTime: 0:00:51.772588 FixedProfit: 1113024\n",
            "Episode: 13/100 RapTime: 0:00:51.853570 FixedProfit: 1052253\n",
            "Episode: 13/100 RapTime: 0:00:51.838680 FixedProfit: 746806\n",
            "Episode: 13/100 RapTime: 0:00:51.846392 FixedProfit: 1070021\n",
            "Episode: 14/100 RapTime: 0:00:51.761459 FixedProfit: 1253191\n",
            "Episode: 14/100 RapTime: 0:00:51.942776 FixedProfit: 1250955\n",
            "Episode: 14/100 RapTime: 0:00:51.866853 FixedProfit: 829521\n",
            "Episode: 14/100 RapTime: 0:00:51.817771 FixedProfit: 1156104\n",
            "Episode: 15/100 RapTime: 0:00:51.579235 FixedProfit: 1189726\n",
            "Episode: 15/100 RapTime: 0:00:51.696196 FixedProfit: 920054\n",
            "Episode: 15/100 RapTime: 0:00:51.674849 FixedProfit: 1020130\n",
            "Episode: 15/100 RapTime: 0:00:51.647368 FixedProfit: 1333167\n",
            "Episode: 16/100 RapTime: 0:00:51.448750 FixedProfit: 991203\n",
            "Episode: 16/100 RapTime: 0:00:51.723457 FixedProfit: 1175943\n",
            "Episode: 16/100 RapTime: 0:00:51.984059 FixedProfit: 1074925\n",
            "Episode: 16/100 RapTime: 0:00:51.607862 FixedProfit: 933846\n",
            "Episode: 17/100 RapTime: 0:00:51.559374 FixedProfit: 1090112\n",
            "Episode: 17/100 RapTime: 0:00:51.473714 FixedProfit: 1033768\n",
            "Episode: 17/100 RapTime: 0:00:51.679354 FixedProfit: 1114793\n",
            "Episode: 17/100 RapTime: 0:00:51.563707 FixedProfit: 964497\n",
            "Episode: 18/100 RapTime: 0:00:51.207658 FixedProfit: 1137467\n",
            "Episode: 18/100 RapTime: 0:00:51.433774 FixedProfit: 999599\n",
            "Episode: 18/100 RapTime: 0:00:51.389871 FixedProfit: 975853\n",
            "Episode: 18/100 RapTime: 0:00:51.311325 FixedProfit: 1042176\n",
            "Episode: 19/100 RapTime: 0:00:51.701245 FixedProfit: 1176681\n",
            "Episode: 19/100 RapTime: 0:00:51.789746 FixedProfit: 1011475\n",
            "Episode: 19/100 RapTime: 0:00:51.905747 FixedProfit: 1228470\n",
            "Episode: 19/100 RapTime: 0:00:51.749345 FixedProfit: 990471\n",
            "Episode: 20/100 RapTime: 0:00:51.583059 FixedProfit: 1138762\n",
            "Episode: 20/100 RapTime: 0:00:51.435693 FixedProfit: 949898\n",
            "Episode: 20/100 RapTime: 0:00:51.630562 FixedProfit: 842217\n",
            "Episode: 20/100 RapTime: 0:00:51.326024 FixedProfit: 1140608\n",
            "Episode: 21/100 RapTime: 0:00:51.543707 FixedProfit: 1230540\n",
            "Episode: 21/100 RapTime: 0:00:51.817269 FixedProfit: 1043025\n",
            "Episode: 21/100 RapTime: 0:00:51.796781 FixedProfit: 1029279\n",
            "Episode: 21/100 RapTime: 0:00:51.412908 FixedProfit: 1108992\n",
            "Episode: 22/100 RapTime: 0:00:51.365641 FixedProfit: 982460\n",
            "Episode: 22/100 RapTime: 0:00:51.620589 FixedProfit: 1109278\n",
            "Episode: 22/100 RapTime: 0:00:52.034477 FixedProfit: 1325362\n",
            "Episode: 22/100 RapTime: 0:00:51.621848 FixedProfit: 1082402\n",
            "Episode: 23/100 RapTime: 0:00:51.304128 FixedProfit: 1045834\n",
            "Episode: 23/100 RapTime: 0:00:51.739014 FixedProfit: 1045698\n",
            "Episode: 23/100 RapTime: 0:00:51.979937 FixedProfit: 1053819\n",
            "Episode: 23/100 RapTime: 0:00:51.730578 FixedProfit: 923777\n",
            "Episode: 24/100 RapTime: 0:00:50.923322 FixedProfit: 1178206\n",
            "Episode: 24/100 RapTime: 0:00:51.403224 FixedProfit: 1188478\n",
            "Episode: 24/100 RapTime: 0:00:51.590583 FixedProfit: 1213730\n",
            "Episode: 24/100 RapTime: 0:00:51.391982 FixedProfit: 1071712\n",
            "Episode: 25/100 RapTime: 0:00:51.093185 FixedProfit: 1108456\n",
            "Episode: 25/100 RapTime: 0:00:51.537042 FixedProfit: 1084561\n",
            "Episode: 25/100 RapTime: 0:00:51.684954 FixedProfit: 1183931\n",
            "Episode: 25/100 RapTime: 0:00:51.475533 FixedProfit: 1093037\n",
            "Episode: 26/100 RapTime: 0:00:51.196414 FixedProfit: 1096528\n",
            "Episode: 26/100 RapTime: 0:00:51.538396 FixedProfit: 1021143\n",
            "Episode: 26/100 RapTime: 0:00:51.554903 FixedProfit: 1182405\n",
            "Episode: 26/100 RapTime: 0:00:51.439649 FixedProfit: 1005600\n",
            "Episode: 27/100 RapTime: 0:00:51.186242 FixedProfit: 1225308\n",
            "Episode: 27/100 RapTime: 0:00:51.740133 FixedProfit: 1115964\n",
            "Episode: 27/100 RapTime: 0:00:51.933131 FixedProfit: 1097962\n",
            "Episode: 27/100 RapTime: 0:00:51.965447 FixedProfit: 1228404\n",
            "Episode: 28/100 RapTime: 0:00:50.990323 FixedProfit: 1031823\n",
            "Episode: 28/100 RapTime: 0:00:51.205424 FixedProfit: 1019628\n",
            "Episode: 28/100 RapTime: 0:00:51.456609 FixedProfit: 969961\n",
            "Episode: 28/100 RapTime: 0:00:51.120309 FixedProfit: 923719\n",
            "Episode: 29/100 RapTime: 0:00:51.104779 FixedProfit: 1095605\n",
            "Episode: 29/100 RapTime: 0:00:51.332227 FixedProfit: 907566\n",
            "Episode: 29/100 RapTime: 0:00:51.369672 FixedProfit: 1071257\n",
            "Episode: 29/100 RapTime: 0:00:50.969647 FixedProfit: 858260\n",
            "Episode: 30/100 RapTime: 0:00:51.150667 FixedProfit: 981813\n",
            "Episode: 30/100 RapTime: 0:00:51.479961 FixedProfit: 1066157\n",
            "Episode: 30/100 RapTime: 0:00:51.293227 FixedProfit: 1132457\n",
            "Episode: 30/100 RapTime: 0:00:51.304422 FixedProfit: 1132603\n",
            "Episode: 31/100 RapTime: 0:00:51.277579 FixedProfit: 1262743\n",
            "Episode: 31/100 RapTime: 0:00:52.069047 FixedProfit: 881834\n",
            "Episode: 31/100 RapTime: 0:00:51.479105 FixedProfit: 1220400\n",
            "Episode: 31/100 RapTime: 0:00:51.656949 FixedProfit: 1038239\n",
            "Episode: 32/100 RapTime: 0:00:50.957560 FixedProfit: 1027159\n",
            "Episode: 32/100 RapTime: 0:00:51.378718 FixedProfit: 1026012\n",
            "Episode: 32/100 RapTime: 0:00:51.359089 FixedProfit: 1283258\n",
            "Episode: 32/100 RapTime: 0:00:51.041037 FixedProfit: 1225918\n",
            "Episode: 33/100 RapTime: 0:00:51.210591 FixedProfit: 1146828\n",
            "Episode: 33/100 RapTime: 0:00:51.304138 FixedProfit: 959061\n",
            "Episode: 33/100 RapTime: 0:00:51.520232 FixedProfit: 1040374\n",
            "Episode: 33/100 RapTime: 0:00:51.184210 FixedProfit: 901226\n",
            "Episode: 34/100 RapTime: 0:00:51.447112 FixedProfit: 1259608\n",
            "Episode: 34/100 RapTime: 0:00:51.492692 FixedProfit: 1041539\n",
            "Episode: 34/100 RapTime: 0:00:51.210189 FixedProfit: 972177\n",
            "Episode: 34/100 RapTime: 0:00:51.662700 FixedProfit: 1071841\n",
            "Episode: 35/100 RapTime: 0:00:51.045156 FixedProfit: 1132418\n",
            "Episode: 35/100 RapTime: 0:00:51.533081 FixedProfit: 921773\n",
            "Episode: 35/100 RapTime: 0:00:51.351699 FixedProfit: 1276056\n",
            "Episode: 35/100 RapTime: 0:00:51.065395 FixedProfit: 1054272\n",
            "Episode: 36/100 RapTime: 0:00:51.304961 FixedProfit: 982822\n",
            "Episode: 36/100 RapTime: 0:00:51.502109 FixedProfit: 1321031\n",
            "Episode: 36/100 RapTime: 0:00:51.465600 FixedProfit: 939640\n",
            "Episode: 36/100 RapTime: 0:00:51.099765 FixedProfit: 1200013\n",
            "Episode: 37/100 RapTime: 0:00:51.548646 FixedProfit: 1235032\n",
            "Episode: 37/100 RapTime: 0:00:51.339159 FixedProfit: 1178615\n",
            "Episode: 37/100 RapTime: 0:00:51.567676 FixedProfit: 1336191\n",
            "Episode: 37/100 RapTime: 0:00:51.343731 FixedProfit: 1233162\n",
            "Episode: 38/100 RapTime: 0:00:51.811615 FixedProfit: 1241975\n",
            "Episode: 38/100 RapTime: 0:00:51.681554 FixedProfit: 950852\n",
            "Episode: 38/100 RapTime: 0:00:51.751524 FixedProfit: 1187481\n",
            "Episode: 38/100 RapTime: 0:00:51.619145 FixedProfit: 1151461\n",
            "Episode: 39/100 RapTime: 0:00:51.381072 FixedProfit: 1299712\n",
            "Episode: 39/100 RapTime: 0:00:51.529872 FixedProfit: 1141180\n",
            "Episode: 39/100 RapTime: 0:00:51.609129 FixedProfit: 1176582\n",
            "Episode: 39/100 RapTime: 0:00:51.342426 FixedProfit: 1079588\n",
            "Episode: 40/100 RapTime: 0:00:51.849673 FixedProfit: 1314275\n",
            "Episode: 40/100 RapTime: 0:00:51.536062 FixedProfit: 1006398\n",
            "Episode: 40/100 RapTime: 0:00:52.110610 FixedProfit: 949136\n",
            "Episode: 40/100 RapTime: 0:00:51.726595 FixedProfit: 955339\n",
            "Episode: 41/100 RapTime: 0:00:51.393887 FixedProfit: 1194174\n",
            "Episode: 41/100 RapTime: 0:00:51.380778 FixedProfit: 1174482\n",
            "Episode: 41/100 RapTime: 0:00:51.570282 FixedProfit: 960688\n",
            "Episode: 41/100 RapTime: 0:00:51.090911 FixedProfit: 1173732\n",
            "Episode: 42/100 RapTime: 0:00:51.526035 FixedProfit: 1114454\n",
            "Episode: 42/100 RapTime: 0:00:51.538329 FixedProfit: 1053803\n",
            "Episode: 42/100 RapTime: 0:00:51.574576 FixedProfit: 1111971\n",
            "Episode: 42/100 RapTime: 0:00:51.122377 FixedProfit: 1141492\n",
            "Episode: 43/100 RapTime: 0:00:51.292529 FixedProfit: 1068822\n",
            "Episode: 43/100 RapTime: 0:00:51.453709 FixedProfit: 1029972\n",
            "Episode: 43/100 RapTime: 0:00:51.199543 FixedProfit: 1175859\n",
            "Episode: 43/100 RapTime: 0:00:51.391906 FixedProfit: 1052100\n",
            "Episode: 44/100 RapTime: 0:00:51.608656 FixedProfit: 1228094\n",
            "Episode: 44/100 RapTime: 0:00:51.683731 FixedProfit: 1098682\n",
            "Episode: 44/100 RapTime: 0:00:51.702102 FixedProfit: 1113129\n",
            "Episode: 44/100 RapTime: 0:00:51.326637 FixedProfit: 1009792\n",
            "Episode: 45/100 RapTime: 0:00:51.361268 FixedProfit: 1107414\n",
            "Episode: 45/100 RapTime: 0:00:51.375076 FixedProfit: 955442\n",
            "Episode: 45/100 RapTime: 0:00:51.255868 FixedProfit: 1201003\n",
            "Episode: 45/100 RapTime: 0:00:50.764008 FixedProfit: 1078921\n",
            "Episode: 46/100 RapTime: 0:00:51.028402 FixedProfit: 1154271\n",
            "Episode: 46/100 RapTime: 0:00:51.200848 FixedProfit: 1171191\n",
            "Episode: 46/100 RapTime: 0:00:51.287949 FixedProfit: 1070581\n",
            "Episode: 46/100 RapTime: 0:00:50.846712 FixedProfit: 1136576\n",
            "Episode: 47/100 RapTime: 0:00:51.159496 FixedProfit: 993070\n",
            "Episode: 47/100 RapTime: 0:00:51.242813 FixedProfit: 1019569\n",
            "Episode: 47/100 RapTime: 0:00:51.405190 FixedProfit: 1251266\n",
            "Episode: 47/100 RapTime: 0:00:50.960629 FixedProfit: 927715\n",
            "Episode: 48/100 RapTime: 0:00:51.275437 FixedProfit: 962361\n",
            "Episode: 48/100 RapTime: 0:00:51.425049 FixedProfit: 1031479\n",
            "Episode: 48/100 RapTime: 0:00:51.539999 FixedProfit: 1119270\n",
            "Episode: 48/100 RapTime: 0:00:51.031774 FixedProfit: 1238377\n",
            "Episode: 49/100 RapTime: 0:00:50.902555 FixedProfit: 1135686\n",
            "Episode: 49/100 RapTime: 0:00:51.284127 FixedProfit: 1085907\n",
            "Episode: 49/100 RapTime: 0:00:51.087995 FixedProfit: 981396\n",
            "Episode: 49/100 RapTime: 0:00:50.545097 FixedProfit: 1141579\n",
            "Episode: 50/100 RapTime: 0:00:50.851826 FixedProfit: 1097215\n",
            "Episode: 50/100 RapTime: 0:00:50.897794 FixedProfit: 1140824\n",
            "Episode: 50/100 RapTime: 0:00:51.074935 FixedProfit: 1226684\n",
            "Episode: 50/100 RapTime: 0:00:50.574045 FixedProfit: 910504\n",
            "Episode: 51/100 RapTime: 0:00:50.942972 FixedProfit: 1158110\n",
            "Episode: 51/100 RapTime: 0:00:51.124945 FixedProfit: 1011775\n",
            "Episode: 51/100 RapTime: 0:00:50.930578 FixedProfit: 1012471\n",
            "Episode: 51/100 RapTime: 0:00:50.622659 FixedProfit: 1202212\n",
            "Episode: 52/100 RapTime: 0:00:51.013664 FixedProfit: 1258518\n",
            "Episode: 52/100 RapTime: 0:00:51.308184 FixedProfit: 1132823\n",
            "Episode: 52/100 RapTime: 0:00:51.331001 FixedProfit: 938567\n",
            "Episode: 52/100 RapTime: 0:00:50.823234 FixedProfit: 1060904\n",
            "Episode: 53/100 RapTime: 0:00:50.958387 FixedProfit: 1060791\n",
            "Episode: 53/100 RapTime: 0:00:50.884396 FixedProfit: 1138821\n",
            "Episode: 53/100 RapTime: 0:00:50.836708 FixedProfit: 1095318\n",
            "Episode: 53/100 RapTime: 0:00:50.239976 FixedProfit: 1185724\n",
            "Episode: 54/100 RapTime: 0:00:50.996064 FixedProfit: 1100768\n",
            "Episode: 54/100 RapTime: 0:00:50.864572 FixedProfit: 969629\n",
            "Episode: 54/100 RapTime: 0:00:50.990707 FixedProfit: 966948\n",
            "Episode: 54/100 RapTime: 0:00:50.268422 FixedProfit: 1149927\n",
            "Episode: 55/100 RapTime: 0:00:51.032078 FixedProfit: 1085247\n",
            "Episode: 55/100 RapTime: 0:00:51.047844 FixedProfit: 1077626\n",
            "Episode: 55/100 RapTime: 0:00:51.092960 FixedProfit: 1141409\n",
            "Episode: 55/100 RapTime: 0:00:50.494634 FixedProfit: 988005\n",
            "Episode: 56/100 RapTime: 0:00:50.905999 FixedProfit: 1134026\n",
            "Episode: 56/100 RapTime: 0:00:51.200604 FixedProfit: 1275248\n",
            "Episode: 56/100 RapTime: 0:00:51.192792 FixedProfit: 1437651\n",
            "Episode: 56/100 RapTime: 0:00:50.544041 FixedProfit: 1202500\n",
            "Episode: 57/100 RapTime: 0:00:51.135650 FixedProfit: 1136309\n",
            "Episode: 57/100 RapTime: 0:00:51.181560 FixedProfit: 1152217\n",
            "Episode: 57/100 RapTime: 0:00:51.388301 FixedProfit: 1194957\n",
            "Episode: 57/100 RapTime: 0:00:50.713642 FixedProfit: 1050983\n",
            "Episode: 58/100 RapTime: 0:00:50.672852 FixedProfit: 970134\n",
            "Episode: 58/100 RapTime: 0:00:50.851029 FixedProfit: 1078269\n",
            "Episode: 58/100 RapTime: 0:00:50.524241 FixedProfit: 938703\n",
            "Episode: 58/100 RapTime: 0:00:50.006345 FixedProfit: 1119842\n",
            "Episode: 59/100 RapTime: 0:00:50.611126 FixedProfit: 782282\n",
            "Episode: 59/100 RapTime: 0:00:50.663959 FixedProfit: 935281\n",
            "Episode: 59/100 RapTime: 0:00:50.713909 FixedProfit: 1245101\n",
            "Episode: 59/100 RapTime: 0:00:50.188951 FixedProfit: 1091065\n",
            "Episode: 60/100 RapTime: 0:00:50.417935 FixedProfit: 1092267\n",
            "Episode: 60/100 RapTime: 0:00:50.763468 FixedProfit: 1150632\n",
            "Episode: 60/100 RapTime: 0:00:50.717744 FixedProfit: 1032555\n",
            "Episode: 60/100 RapTime: 0:00:50.089010 FixedProfit: 977671\n",
            "Episode: 61/100 RapTime: 0:00:51.001810 FixedProfit: 1122629\n",
            "Episode: 61/100 RapTime: 0:00:51.269766 FixedProfit: 1211987\n",
            "Episode: 61/100 RapTime: 0:00:51.071344 FixedProfit: 1124426\n",
            "Episode: 61/100 RapTime: 0:00:50.755226 FixedProfit: 992922\n",
            "Episode: 62/100 RapTime: 0:00:50.861206 FixedProfit: 1422569\n",
            "Episode: 62/100 RapTime: 0:00:50.826989 FixedProfit: 1152337\n",
            "Episode: 62/100 RapTime: 0:00:51.161659 FixedProfit: 933342\n",
            "Episode: 62/100 RapTime: 0:00:50.597701 FixedProfit: 1250545\n",
            "Episode: 63/100 RapTime: 0:00:50.905712 FixedProfit: 1232165\n",
            "Episode: 63/100 RapTime: 0:00:51.169601 FixedProfit: 986455\n",
            "Episode: 63/100 RapTime: 0:00:51.084666 FixedProfit: 1052289\n",
            "Episode: 63/100 RapTime: 0:00:50.434291 FixedProfit: 1080282\n",
            "Episode: 64/100 RapTime: 0:00:51.001529 FixedProfit: 1013428\n",
            "Episode: 64/100 RapTime: 0:00:51.001271 FixedProfit: 1186717\n",
            "Episode: 64/100 RapTime: 0:00:50.860375 FixedProfit: 1112258\n",
            "Episode: 64/100 RapTime: 0:00:50.515324 FixedProfit: 1106199\n",
            "Episode: 65/100 RapTime: 0:00:50.809789 FixedProfit: 1009012\n",
            "Episode: 65/100 RapTime: 0:00:50.802743 FixedProfit: 979557\n",
            "Episode: 65/100 RapTime: 0:00:50.977248 FixedProfit: 1166187\n",
            "Episode: 65/100 RapTime: 0:00:50.282997 FixedProfit: 1112354\n",
            "Episode: 66/100 RapTime: 0:00:50.762188 FixedProfit: 896829\n",
            "Episode: 66/100 RapTime: 0:00:50.303461 FixedProfit: 1111839\n",
            "Episode: 66/100 RapTime: 0:00:50.779857 FixedProfit: 1062198\n",
            "Episode: 66/100 RapTime: 0:00:49.996263 FixedProfit: 1008470\n",
            "Episode: 67/100 RapTime: 0:00:50.852196 FixedProfit: 965006\n",
            "Episode: 67/100 RapTime: 0:00:51.061024 FixedProfit: 1213946\n",
            "Episode: 67/100 RapTime: 0:00:51.005604 FixedProfit: 1153660\n",
            "Episode: 67/100 RapTime: 0:00:50.331021 FixedProfit: 986133\n",
            "Episode: 68/100 RapTime: 0:00:50.279155 FixedProfit: 1079478\n",
            "Episode: 68/100 RapTime: 0:00:50.396335 FixedProfit: 938486\n",
            "Episode: 68/100 RapTime: 0:00:50.490916 FixedProfit: 1129335\n",
            "Episode: 68/100 RapTime: 0:00:49.910080 FixedProfit: 1138373\n",
            "Episode: 69/100 RapTime: 0:00:50.510736 FixedProfit: 1103822\n",
            "Episode: 69/100 RapTime: 0:00:50.670559 FixedProfit: 1004689\n",
            "Episode: 69/100 RapTime: 0:00:50.966124 FixedProfit: 910692\n",
            "Episode: 69/100 RapTime: 0:00:49.944150 FixedProfit: 1037856\n",
            "Episode: 70/100 RapTime: 0:00:50.609707 FixedProfit: 1255074\n",
            "Episode: 70/100 RapTime: 0:00:50.559916 FixedProfit: 1032238\n",
            "Episode: 70/100 RapTime: 0:00:50.703010 FixedProfit: 1093428\n",
            "Episode: 70/100 RapTime: 0:00:49.959724 FixedProfit: 1250859\n",
            "Episode: 71/100 RapTime: 0:00:50.683389 FixedProfit: 991825\n",
            "Episode: 71/100 RapTime: 0:00:50.897761 FixedProfit: 1139292\n",
            "Episode: 71/100 RapTime: 0:00:50.922594 FixedProfit: 1082271\n",
            "Episode: 71/100 RapTime: 0:00:50.206910 FixedProfit: 1194595\n",
            "Episode: 72/100 RapTime: 0:00:50.428754 FixedProfit: 1122663\n",
            "Episode: 72/100 RapTime: 0:00:50.486840 FixedProfit: 1206092\n",
            "Episode: 72/100 RapTime: 0:00:50.433269 FixedProfit: 1127190\n",
            "Episode: 72/100 RapTime: 0:00:49.862886 FixedProfit: 1153148\n",
            "Episode: 73/100 RapTime: 0:00:50.118169 FixedProfit: 992473\n",
            "Episode: 73/100 RapTime: 0:00:50.359583 FixedProfit: 968424\n",
            "Episode: 73/100 RapTime: 0:00:50.511802 FixedProfit: 970734\n",
            "Episode: 73/100 RapTime: 0:00:49.862380 FixedProfit: 1048793\n",
            "Episode: 74/100 RapTime: 0:00:50.562144 FixedProfit: 1165984\n",
            "Episode: 74/100 RapTime: 0:00:50.705210 FixedProfit: 958458\n",
            "Episode: 74/100 RapTime: 0:00:50.609113 FixedProfit: 1117677\n",
            "Episode: 74/100 RapTime: 0:00:49.617924 FixedProfit: 1120167\n",
            "Episode: 75/100 RapTime: 0:00:50.297889 FixedProfit: 1043506\n",
            "Episode: 75/100 RapTime: 0:00:50.525843 FixedProfit: 1222344\n",
            "Episode: 75/100 RapTime: 0:00:50.235789 FixedProfit: 951966\n",
            "Episode: 75/100 RapTime: 0:00:49.908592 FixedProfit: 1182432\n",
            "Episode: 76/100 RapTime: 0:00:50.475332 FixedProfit: 948318\n",
            "Episode: 76/100 RapTime: 0:00:50.535146 FixedProfit: 1174770\n",
            "Episode: 76/100 RapTime: 0:00:50.648591 FixedProfit: 1374500\n",
            "Episode: 76/100 RapTime: 0:00:49.626758 FixedProfit: 1120322\n",
            "Episode: 77/100 RapTime: 0:00:50.689182 FixedProfit: 1020821\n",
            "Episode: 77/100 RapTime: 0:00:50.626604 FixedProfit: 1113782\n",
            "Episode: 77/100 RapTime: 0:00:50.465832 FixedProfit: 1204782\n",
            "Episode: 77/100 RapTime: 0:00:49.754508 FixedProfit: 1110965\n",
            "Episode: 78/100 RapTime: 0:00:50.668866 FixedProfit: 938675\n",
            "Episode: 78/100 RapTime: 0:00:50.751104 FixedProfit: 1104221\n",
            "Episode: 78/100 RapTime: 0:00:50.855741 FixedProfit: 1049528\n",
            "Episode: 78/100 RapTime: 0:00:50.465702 FixedProfit: 1284450\n",
            "Episode: 79/100 RapTime: 0:00:50.405750 FixedProfit: 1207145\n",
            "Episode: 79/100 RapTime: 0:00:50.477938 FixedProfit: 1062664\n",
            "Episode: 79/100 RapTime: 0:00:50.421684 FixedProfit: 1067596\n",
            "Episode: 79/100 RapTime: 0:00:49.872468 FixedProfit: 1108988\n",
            "Episode: 80/100 RapTime: 0:00:50.401896 FixedProfit: 1226260\n",
            "Episode: 80/100 RapTime: 0:00:50.524477 FixedProfit: 1106149\n",
            "Episode: 80/100 RapTime: 0:00:50.375062 FixedProfit: 1197086\n",
            "Episode: 80/100 RapTime: 0:00:49.866369 FixedProfit: 915906\n",
            "Episode: 81/100 RapTime: 0:00:50.212381 FixedProfit: 825971\n",
            "Episode: 81/100 RapTime: 0:00:50.586085 FixedProfit: 1112809\n",
            "Episode: 81/100 RapTime: 0:00:50.375625 FixedProfit: 1227932\n",
            "Episode: 81/100 RapTime: 0:00:49.611175 FixedProfit: 1145745\n",
            "Episode: 82/100 RapTime: 0:00:50.627231 FixedProfit: 1186838\n",
            "Episode: 82/100 RapTime: 0:00:50.912590 FixedProfit: 955850\n",
            "Episode: 82/100 RapTime: 0:00:50.783940 FixedProfit: 1219425\n",
            "Episode: 82/100 RapTime: 0:00:49.961222 FixedProfit: 846566\n",
            "Episode: 83/100 RapTime: 0:00:50.361385 FixedProfit: 1117757\n",
            "Episode: 83/100 RapTime: 0:00:50.712590 FixedProfit: 1076257\n",
            "Episode: 83/100 RapTime: 0:00:50.481404 FixedProfit: 1153316\n",
            "Episode: 83/100 RapTime: 0:00:49.539186 FixedProfit: 1087796\n",
            "Episode: 84/100 RapTime: 0:00:50.431865 FixedProfit: 1125714\n",
            "Episode: 84/100 RapTime: 0:00:50.800225 FixedProfit: 1127626\n",
            "Episode: 84/100 RapTime: 0:00:50.710699 FixedProfit: 1278298\n",
            "Episode: 84/100 RapTime: 0:00:49.749011 FixedProfit: 1246938\n",
            "Episode: 85/100 RapTime: 0:00:50.178341 FixedProfit: 1069691\n",
            "Episode: 85/100 RapTime: 0:00:50.409490 FixedProfit: 963076\n",
            "Episode: 85/100 RapTime: 0:00:50.402063 FixedProfit: 1040771\n",
            "Episode: 85/100 RapTime: 0:00:49.551052 FixedProfit: 1056163\n",
            "Episode: 86/100 RapTime: 0:00:50.382486 FixedProfit: 930847\n",
            "Episode: 86/100 RapTime: 0:00:50.957858 FixedProfit: 1194219\n",
            "Episode: 86/100 RapTime: 0:00:50.599072 FixedProfit: 1066334\n",
            "Episode: 86/100 RapTime: 0:00:49.866217 FixedProfit: 1053998\n",
            "Episode: 87/100 RapTime: 0:00:50.092929 FixedProfit: 1155215\n",
            "Episode: 87/100 RapTime: 0:00:50.248026 FixedProfit: 1111353\n",
            "Episode: 87/100 RapTime: 0:00:49.971537 FixedProfit: 1091982\n",
            "Episode: 87/100 RapTime: 0:00:49.424157 FixedProfit: 997861\n",
            "Episode: 88/100 RapTime: 0:00:50.124459 FixedProfit: 1112714\n",
            "Episode: 88/100 RapTime: 0:00:50.293185 FixedProfit: 1170298\n",
            "Episode: 88/100 RapTime: 0:00:50.604169 FixedProfit: 975895\n",
            "Episode: 88/100 RapTime: 0:00:49.632756 FixedProfit: 1115457\n",
            "Episode: 89/100 RapTime: 0:00:49.974904 FixedProfit: 1114065\n",
            "Episode: 89/100 RapTime: 0:00:50.019488 FixedProfit: 941134\n",
            "Episode: 89/100 RapTime: 0:00:50.267646 FixedProfit: 1096168\n",
            "Episode: 89/100 RapTime: 0:00:49.438602 FixedProfit: 825664\n",
            "Episode: 90/100 RapTime: 0:00:50.017656 FixedProfit: 1193061\n",
            "Episode: 90/100 RapTime: 0:00:50.467435 FixedProfit: 1205295\n",
            "Episode: 90/100 RapTime: 0:00:50.300263 FixedProfit: 1172459\n",
            "Episode: 90/100 RapTime: 0:00:49.605202 FixedProfit: 1298415\n",
            "Episode: 91/100 RapTime: 0:00:50.113645 FixedProfit: 1060091\n",
            "Episode: 91/100 RapTime: 0:00:50.228033 FixedProfit: 1115772\n",
            "Episode: 91/100 RapTime: 0:00:50.326277 FixedProfit: 953027\n",
            "Episode: 91/100 RapTime: 0:00:49.631065 FixedProfit: 1210114\n",
            "Episode: 92/100 RapTime: 0:00:49.772626 FixedProfit: 1152428\n",
            "Episode: 92/100 RapTime: 0:00:50.489377 FixedProfit: 1241883\n",
            "Episode: 92/100 RapTime: 0:00:50.554095 FixedProfit: 1132708\n",
            "Episode: 92/100 RapTime: 0:00:49.549058 FixedProfit: 1357834\n",
            "Episode: 93/100 RapTime: 0:00:49.919120 FixedProfit: 1188759\n",
            "Episode: 93/100 RapTime: 0:00:50.262189 FixedProfit: 1167708\n",
            "Episode: 93/100 RapTime: 0:00:50.365707 FixedProfit: 1141825\n",
            "Episode: 93/100 RapTime: 0:00:49.586266 FixedProfit: 1203110\n",
            "Episode: 94/100 RapTime: 0:00:49.838660 FixedProfit: 1197034\n",
            "Episode: 94/100 RapTime: 0:00:50.251523 FixedProfit: 1213880\n",
            "Episode: 94/100 RapTime: 0:00:50.255600 FixedProfit: 1092502\n",
            "Episode: 94/100 RapTime: 0:00:49.443987 FixedProfit: 1030143\n",
            "Episode: 95/100 RapTime: 0:00:50.039481 FixedProfit: 1102293\n",
            "Episode: 95/100 RapTime: 0:00:50.886069 FixedProfit: 1142491\n",
            "Episode: 95/100 RapTime: 0:00:50.632268 FixedProfit: 1260001\n",
            "Episode: 95/100 RapTime: 0:00:49.877635 FixedProfit: 1165127\n",
            "Episode: 96/100 RapTime: 0:00:49.904293 FixedProfit: 1178502\n",
            "Episode: 96/100 RapTime: 0:00:50.441491 FixedProfit: 1098731\n",
            "Episode: 96/100 RapTime: 0:00:50.743093 FixedProfit: 1333217\n",
            "Episode: 96/100 RapTime: 0:00:49.490560 FixedProfit: 1275574\n",
            "Episode: 97/100 RapTime: 0:00:49.951832 FixedProfit: 1232350\n",
            "Episode: 97/100 RapTime: 0:00:50.250915 FixedProfit: 1095267\n",
            "Episode: 97/100 RapTime: 0:00:50.184958 FixedProfit: 907451\n",
            "Episode: 97/100 RapTime: 0:00:49.101603 FixedProfit: 1086534\n",
            "Episode: 98/100 RapTime: 0:00:49.980469 FixedProfit: 999787\n",
            "Episode: 98/100 RapTime: 0:00:50.229771 FixedProfit: 1105613\n",
            "Episode: 98/100 RapTime: 0:00:50.211214 FixedProfit: 1270022\n",
            "Episode: 98/100 RapTime: 0:00:49.094707 FixedProfit: 978343\n",
            "Episode: 99/100 RapTime: 0:00:50.113782 FixedProfit: 1086889\n",
            "Episode: 99/100 RapTime: 0:00:50.439508 FixedProfit: 933843\n",
            "Episode: 99/100 RapTime: 0:00:50.408985 FixedProfit: 1251766\n",
            "Episode: 99/100 RapTime: 0:00:49.618404 FixedProfit: 1009160\n",
            "Episode: 100/100 RapTime: 0:00:50.119022 FixedProfit: 1082587\n",
            "Episode: 100/100 RapTime: 0:00:50.224726 FixedProfit: 1103943\n",
            "Episode: 100/100 RapTime: 0:00:50.184228 FixedProfit: 1109265\n",
            "Episode: 100/100 RapTime: 0:00:49.156621 FixedProfit: 1084558\n"
          ]
        }
      ]
    }
  ]
}