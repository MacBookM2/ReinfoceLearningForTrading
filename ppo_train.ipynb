{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ppo_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/ppo_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "89e81fd6-4282-481b-c9a5-d684c7493026"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "mode = 'train'\n",
        "name = 'ppo'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_shape = 3\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(3, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain):\n",
        "        self.model = brain.model\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, v = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(3, p=act_p[0].numpy()), v\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model):\n",
        "        self.model = model\n",
        "        self.gamma = 0.9\n",
        "        self.c_gamma = 0.1\n",
        "\n",
        "        self.beta_1 = 0.1\n",
        "        self.beta_2 = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "        advantage = np.asarray([e[\"advantage\"] for e in experiences]).reshape((-1, 1))\n",
        "        prob_batch = np.asarray([e[\"prob\"] for e in experiences])\n",
        "\n",
        "        advantage = (advantage - np.mean(advantage)) / (np.std(advantage) + 1e-8)\n",
        "        onehot_actions = tf.one_hot(action_batch, 3).numpy()\n",
        "\n",
        "        old_pi = tf.reduce_sum(onehot_actions * prob_batch, axis=1, keepdims=True)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "\n",
        "            advantage = advantage - tf.stop_gradient(v)\n",
        "\n",
        "            # π(a|s)とlog(π(a|s))を計算\n",
        "            new_pi = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            new_logpi = tf.math.log(tf.clip_by_value(new_pi, 1e-10, 1.0))\n",
        "\n",
        "            r_t_batch = new_pi / old_pi\n",
        "\n",
        "            losses_clip = self._losses_clip(advantage, r_t_batch)\n",
        "            losses_value = self._losses_value(advantage)\n",
        "            entropy = tf.reduce_sum(new_logpi * new_pi, axis=1, keepdims=True)\n",
        "\n",
        "            total_loss = - losses_clip + self.beta_1 * losses_value - self.beta_2 * entropy\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        # WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\n",
        "        # gradientsがnoneになる時がある。\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        # gradients, _ = tf.clip_by_global_norm(gradients, 0.5)\n",
        "        # self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        self.model.optimizer.apply_gradients((grad, var) for (grad, var) in \n",
        "                                             zip(gradients, self.model.trainable_variables) if grad is not None)\n",
        "\n",
        "    def _losses_value(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _losses_clip(self, advantage, r_t):\n",
        "\n",
        "        r_clip = tf.clip_by_value(r_t, 1 - self.c_gamma, 1 + self.c_gamma)\n",
        "\n",
        "        loss_unclipped = r_t * advantage\n",
        "        loss_clipped = r_clip * advantage\n",
        "\n",
        "        return tf.minimum(loss_clipped, loss_clipped)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1UX215aeoXq"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : List[List[float]] = field(default_factory=list)\n",
        "    action : List[int] = field(default_factory=list)\n",
        "    reward : List[float] = field(default_factory=list)\n",
        "    next_state : List[List[int]] = field(default_factory=list)\n",
        "    done : List[bool] = field(default_factory=list)\n",
        "    prob_bol : List[bool] = field(default_factory=list)\n",
        "    v : List[float] = field(default_factory=list)\n",
        "    advantage : List[float] = field(default_factory=list)\n",
        "    model : str = None\n",
        "    gae_lambda: float = 0.9\n",
        "    gamma: float = 0.9\n",
        "\n",
        "    def random_experiences(self, batch_size):\n",
        "        max_size = len(self.state) - 1\n",
        "        self._make_advantage(max_size)\n",
        "        batch_num = self._random_num(1, max_size, batch_size)\n",
        "        experiences = []\n",
        "        for i in batch_num:\n",
        "            prob = self.probability(self.state[i])\n",
        "            prob = prob.numpy()\n",
        "            prob = prob[0]\n",
        "            experiences.append({\"state\": self.state[i], \"action\": self.action[i], \n",
        "                                \"reward\": self.reward[i], \"next_state\": self.next_state[i], \n",
        "                                \"done\": self.done[i], \"prob\": prob, \"advantage\": self.advantage[i]})\n",
        "        return experiences\n",
        "\n",
        "    def _random_num(self, a, b, k):\n",
        "        ns = []\n",
        "        while len(ns) < k:\n",
        "            n = random.randint(a, b)\n",
        "            if not n in ns:\n",
        "                ns.append(n)\n",
        "        return ns\n",
        "\n",
        "    def _make_advantage(self,max_size):\n",
        "        n_v = copy.deepcopy(self.v)\n",
        "        n_v.pop(0)\n",
        "        _, n_v_last = self.model(self.next_state[max_size].reshape((1,-1)))\n",
        "        n_v_last2 = n_v_last.numpy()\n",
        "        n_v.append(n_v_last2[0][0])\n",
        "\n",
        "        for i in range(len(self.reward)):\n",
        "            gae = 0\n",
        "            t = 0\n",
        "            for j in range(i, len(self.reward)):\n",
        "                delta = self.reward[j] + self.gamma * n_v[j] - self.v[j]\n",
        "                gae += ((self.gae_lambda * self.gamma) ** t) * delta\n",
        "                t += 1\n",
        "            self.advantage.append(gae)\n",
        "\n",
        "    def probability(self, state):\n",
        "        p, _ = self.model(state.reshape((1,-1)))\n",
        "        return p"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, num, mdl_dir, name, batch_size = 128, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            memory = ExperiencesMemory(model = self.actor.model)\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action, v = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    memory.state.append(state)\n",
        "                    memory.action.append(action)\n",
        "                    memory.reward.append(reward)\n",
        "                    memory.next_state.append(next_state)\n",
        "                    memory.done.append(done)\n",
        "                    v2 = v.numpy()\n",
        "                    memory.v.append(v2[0][0])\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            if self.mode == 'train':\n",
        "                experiences = memory.random_experiences(self.batch_size)\n",
        "                self.critic.valuenetwork(experiences)\n",
        "\n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\"\n",
        "                .format(episode + 1, episodes_times, play_time, info['cur_revenue'], \n",
        "                        info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\"\n",
        "                .format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgv85YlVOaum",
        "outputId": "3563bf91-801f-42e0-9994-d97f16c29341"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 50\n",
        "batch_size = 128\n",
        "\n",
        "brain = ParameterServer()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    model = brain.model\n",
        "    actor = Actor(brain)\n",
        "    critic = Critic(model)\n",
        "    main = Main(env, actor, critic, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:00:10.837141 FixedProfit: 1020395Episode: 1/50 RapTime: 0:00:10.841340 FixedProfit: 1294412\n",
            "\n",
            "Episode: 1/50 RapTime: 0:00:10.865866 FixedProfit: 1422162\n",
            "Episode: 1/50 RapTime: 0:00:10.884094 FixedProfit: 1130491\n",
            "Episode: 2/50 RapTime: 0:00:10.338255 FixedProfit: 1110947\n",
            "Episode: 2/50 RapTime: 0:00:10.377610 FixedProfit: 1198493\n",
            "Episode: 2/50 RapTime: 0:00:10.349930 FixedProfit: 1079886\n",
            "Episode: 2/50 RapTime: 0:00:10.391778 FixedProfit: 998386\n",
            "Episode: 3/50 RapTime: 0:00:10.376007 FixedProfit: 1116298\n",
            "Episode: 3/50 RapTime: 0:00:10.363677 FixedProfit: 1056470\n",
            "Episode: 3/50 RapTime: 0:00:10.348878 FixedProfit: 1084651\n",
            "Episode: 3/50 RapTime: 0:00:10.373406 FixedProfit: 962836\n",
            "Episode: 4/50 RapTime: 0:00:10.316939 FixedProfit: 1259321\n",
            "Episode: 4/50 RapTime: 0:00:10.276320 FixedProfit: 741453\n",
            "Episode: 4/50 RapTime: 0:00:10.333666 FixedProfit: 1194484\n",
            "Episode: 4/50 RapTime: 0:00:10.335126 FixedProfit: 1092916\n",
            "Episode: 5/50 RapTime: 0:00:10.281877 FixedProfit: 1092851\n",
            "Episode: 5/50 RapTime: 0:00:10.275966 FixedProfit: 1102720\n",
            "Episode: 5/50 RapTime: 0:00:10.292664 FixedProfit: 963498\n",
            "Episode: 5/50 RapTime: 0:00:10.327269 FixedProfit: 1105945\n",
            "Episode: 6/50 RapTime: 0:00:10.331278 FixedProfit: 977767\n",
            "Episode: 6/50 RapTime: 0:00:10.313656 FixedProfit: 1164739\n",
            "Episode: 6/50 RapTime: 0:00:10.295729 FixedProfit: 1131437\n",
            "Episode: 6/50 RapTime: 0:00:10.380439 FixedProfit: 1163254\n",
            "Episode: 7/50 RapTime: 0:00:10.298666 FixedProfit: 1046646\n",
            "Episode: 7/50 RapTime: 0:00:10.264047 FixedProfit: 1123503\n",
            "Episode: 7/50 RapTime: 0:00:10.308169 FixedProfit: 1132097\n",
            "Episode: 7/50 RapTime: 0:00:10.311953 FixedProfit: 1042604\n",
            "Episode: 8/50 RapTime: 0:00:10.313899 FixedProfit: 1095136\n",
            "Episode: 8/50 RapTime: 0:00:10.348046 FixedProfit: 1094539\n",
            "Episode: 8/50 RapTime: 0:00:10.347593 FixedProfit: 1162349\n",
            "Episode: 8/50 RapTime: 0:00:10.347695 FixedProfit: 1122994\n",
            "Episode: 9/50 RapTime: 0:00:10.233510 FixedProfit: 1120869\n",
            "Episode: 9/50 RapTime: 0:00:10.254514 FixedProfit: 1194173\n",
            "Episode: 9/50 RapTime: 0:00:10.253094 FixedProfit: 1177087\n",
            "Episode: 9/50 RapTime: 0:00:10.257335 FixedProfit: 1119416\n",
            "Episode: 10/50 RapTime: 0:00:10.247708 FixedProfit: 1199912\n",
            "Episode: 10/50 RapTime: 0:00:10.245479 FixedProfit: 1181639\n",
            "Episode: 10/50 RapTime: 0:00:10.315939 FixedProfit: 1112281\n",
            "Episode: 10/50 RapTime: 0:00:10.328573 FixedProfit: 1280601\n",
            "Episode: 11/50 RapTime: 0:00:10.328549 FixedProfit: 1178820\n",
            "Episode: 11/50 RapTime: 0:00:10.379997 FixedProfit: 1401264\n",
            "Episode: 11/50 RapTime: 0:00:10.215664 FixedProfit: 1159307\n",
            "Episode: 11/50 RapTime: 0:00:10.413075 FixedProfit: 1169581\n",
            "Episode: 12/50 RapTime: 0:00:10.334148 FixedProfit: 1154481\n",
            "Episode: 12/50 RapTime: 0:00:10.364534 FixedProfit: 1130301\n",
            "Episode: 12/50 RapTime: 0:00:10.348850 FixedProfit: 1163027\n",
            "Episode: 12/50 RapTime: 0:00:10.365173 FixedProfit: 1164535\n",
            "Episode: 13/50 RapTime: 0:00:10.261682 FixedProfit: 1202990\n",
            "Episode: 13/50 RapTime: 0:00:10.270660 FixedProfit: 1076366\n",
            "Episode: 13/50 RapTime: 0:00:10.283408 FixedProfit: 1205792\n",
            "Episode: 13/50 RapTime: 0:00:10.289182 FixedProfit: 1067314\n",
            "Episode: 14/50 RapTime: 0:00:10.185735 FixedProfit: 1263247\n",
            "Episode: 14/50 RapTime: 0:00:10.171283 FixedProfit: 1173059\n",
            "Episode: 14/50 RapTime: 0:00:10.282119 FixedProfit: 1206398\n",
            "Episode: 14/50 RapTime: 0:00:10.244558 FixedProfit: 1256058\n",
            "Episode: 15/50 RapTime: 0:00:10.332017 FixedProfit: 1039255\n",
            "Episode: 15/50 RapTime: 0:00:10.348236 FixedProfit: 1219835\n",
            "Episode: 15/50 RapTime: 0:00:10.389333 FixedProfit: 1184935\n",
            "Episode: 15/50 RapTime: 0:00:10.371691 FixedProfit: 1329214\n",
            "Episode: 16/50 RapTime: 0:00:10.493733 FixedProfit: 1187020\n",
            "Episode: 16/50 RapTime: 0:00:10.486898 FixedProfit: 1196292\n",
            "Episode: 16/50 RapTime: 0:00:10.412018 FixedProfit: 1267998\n",
            "Episode: 16/50 RapTime: 0:00:10.533942 FixedProfit: 1208968\n",
            "Episode: 17/50 RapTime: 0:00:10.246896 FixedProfit: 1108431\n",
            "Episode: 17/50 RapTime: 0:00:10.308729 FixedProfit: 1143592\n",
            "Episode: 17/50 RapTime: 0:00:10.288223 FixedProfit: 1213942\n",
            "Episode: 17/50 RapTime: 0:00:10.232172 FixedProfit: 1173059\n",
            "Episode: 18/50 RapTime: 0:00:10.301392 FixedProfit: 1208128\n",
            "Episode: 18/50 RapTime: 0:00:10.293133 FixedProfit: 1192980\n",
            "Episode: 18/50 RapTime: 0:00:10.265613 FixedProfit: 1153610\n",
            "Episode: 18/50 RapTime: 0:00:10.329454 FixedProfit: 1194143\n",
            "Episode: 19/50 RapTime: 0:00:10.326513 FixedProfit: 1034236\n",
            "Episode: 19/50 RapTime: 0:00:10.434441 FixedProfit: 1162144\n",
            "Episode: 19/50 RapTime: 0:00:10.427916 FixedProfit: 1197032\n",
            "Episode: 19/50 RapTime: 0:00:10.345370 FixedProfit: 1179158\n",
            "Episode: 20/50 RapTime: 0:00:10.379166 FixedProfit: 1198139\n",
            "Episode: 20/50 RapTime: 0:00:10.364471 FixedProfit: 1148478\n",
            "Episode: 20/50 RapTime: 0:00:10.406985 FixedProfit: 1183321\n",
            "Episode: 20/50 RapTime: 0:00:10.368854 FixedProfit: 1202990\n",
            "Episode: 21/50 RapTime: 0:00:10.246734 FixedProfit: 1196192\n",
            "Episode: 21/50 RapTime: 0:00:10.311575 FixedProfit: 1196192\n",
            "Episode: 21/50 RapTime: 0:00:10.250279 FixedProfit: 1172570\n",
            "Episode: 21/50 RapTime: 0:00:10.312646 FixedProfit: 1212780\n",
            "Episode: 22/50 RapTime: 0:00:10.310181 FixedProfit: 1176228\n",
            "Episode: 22/50 RapTime: 0:00:10.304713 FixedProfit: 1345632\n",
            "Episode: 22/50 RapTime: 0:00:10.340356 FixedProfit: 1194143\n",
            "Episode: 22/50 RapTime: 0:00:10.315396 FixedProfit: 1198139\n",
            "Episode: 23/50 RapTime: 0:00:10.353097 FixedProfit: 1173059\n",
            "Episode: 23/50 RapTime: 0:00:10.317549 FixedProfit: 1144662\n",
            "Episode: 23/50 RapTime: 0:00:10.364598 FixedProfit: 1198139\n",
            "Episode: 23/50 RapTime: 0:00:10.387643 FixedProfit: 1182011\n",
            "Episode: 24/50 RapTime: 0:00:10.318986 FixedProfit: 1184935\n",
            "Episode: 24/50 RapTime: 0:00:10.357145 FixedProfit: 1191954\n",
            "Episode: 24/50 RapTime: 0:00:10.298601 FixedProfit: 1208128\n",
            "Episode: 24/50 RapTime: 0:00:10.380902 FixedProfit: 1183321\n",
            "Episode: 25/50 RapTime: 0:00:10.352630 FixedProfit: 1044315\n",
            "Episode: 25/50 RapTime: 0:00:10.289900 FixedProfit: 1208128\n",
            "Episode: 25/50 RapTime: 0:00:10.326470 FixedProfit: 1202990\n",
            "Episode: 25/50 RapTime: 0:00:10.321301 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:10.367359 FixedProfit: 1213942\n",
            "Episode: 26/50 RapTime: 0:00:10.311590 FixedProfit: 1194143\n",
            "Episode: 26/50 RapTime: 0:00:10.344136 FixedProfit: 1174524\n",
            "Episode: 26/50 RapTime: 0:00:10.377835 FixedProfit: 1195955\n",
            "Episode: 27/50 RapTime: 0:00:10.332188 FixedProfit: 1196192\n",
            "Episode: 27/50 RapTime: 0:00:10.373192 FixedProfit: 1195955\n",
            "Episode: 27/50 RapTime: 0:00:10.363864 FixedProfit: 1124493\n",
            "Episode: 27/50 RapTime: 0:00:10.319734 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:10.354701 FixedProfit: 1208128\n",
            "Episode: 28/50 RapTime: 0:00:10.307298 FixedProfit: 1202990\n",
            "Episode: 28/50 RapTime: 0:00:10.380448 FixedProfit: 1195955\n",
            "Episode: 28/50 RapTime: 0:00:10.397665 FixedProfit: 1145342\n",
            "Episode: 29/50 RapTime: 0:00:10.292046 FixedProfit: 1203346\n",
            "Episode: 29/50 RapTime: 0:00:10.307678 FixedProfit: 1208128\n",
            "Episode: 29/50 RapTime: 0:00:10.368339 FixedProfit: 1194143\n",
            "Episode: 29/50 RapTime: 0:00:10.331716 FixedProfit: 1212768\n",
            "Episode: 30/50 RapTime: 0:00:10.381251 FixedProfit: 1194143\n",
            "Episode: 30/50 RapTime: 0:00:10.359433 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:10.369574 FixedProfit: 1158514\n",
            "Episode: 30/50 RapTime: 0:00:10.308028 FixedProfit: 1192980\n",
            "Episode: 31/50 RapTime: 0:00:10.336075 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:10.341925 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:10.391758 FixedProfit: 1202990\n",
            "Episode: 31/50 RapTime: 0:00:10.398659 FixedProfit: 1198139\n",
            "Episode: 32/50 RapTime: 0:00:10.388678 FixedProfit: 1205279\n",
            "Episode: 32/50 RapTime: 0:00:10.322568 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:10.323771 FixedProfit: 1213942\n",
            "Episode: 32/50 RapTime: 0:00:10.382975 FixedProfit: 1194143\n",
            "Episode: 33/50 RapTime: 0:00:10.379238 FixedProfit: 1192980\n",
            "Episode: 33/50 RapTime: 0:00:10.409428 FixedProfit: 1194143\n",
            "Episode: 33/50 RapTime: 0:00:10.390270 FixedProfit: 1202990\n",
            "Episode: 33/50 RapTime: 0:00:10.336682 FixedProfit: 1205057\n",
            "Episode: 34/50 RapTime: 0:00:10.351341 FixedProfit: 1183321\n",
            "Episode: 34/50 RapTime: 0:00:10.349411 FixedProfit: 1173059\n",
            "Episode: 34/50 RapTime: 0:00:10.322745 FixedProfit: 1208128\n",
            "Episode: 34/50 RapTime: 0:00:10.334743 FixedProfit: 1194814\n",
            "Episode: 35/50 RapTime: 0:00:10.295586 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:10.354865 FixedProfit: 1208128\n",
            "Episode: 35/50 RapTime: 0:00:10.301369 FixedProfit: 1194986\n",
            "Episode: 35/50 RapTime: 0:00:10.359294 FixedProfit: 1195955\n",
            "Episode: 36/50 RapTime: 0:00:10.410856 FixedProfit: 1171327\n",
            "Episode: 36/50 RapTime: 0:00:10.401019 FixedProfit: 1208128\n",
            "Episode: 36/50 RapTime: 0:00:10.435277 FixedProfit: 1208128\n",
            "Episode: 36/50 RapTime: 0:00:10.430706 FixedProfit: 1201703\n",
            "Episode: 37/50 RapTime: 0:00:10.282964 FixedProfit: 1194143\n",
            "Episode: 37/50 RapTime: 0:00:10.324591 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:10.254986 FixedProfit: 1208128\n",
            "Episode: 37/50 RapTime: 0:00:10.316152 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:10.306536 FixedProfit: 1179800\n",
            "Episode: 38/50 RapTime: 0:00:10.340421 FixedProfit: 1173059\n",
            "Episode: 38/50 RapTime: 0:00:10.360133 FixedProfit: 1213942\n",
            "Episode: 38/50 RapTime: 0:00:10.309934 FixedProfit: 1176228\n",
            "Episode: 39/50 RapTime: 0:00:10.345301 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:10.344602 FixedProfit: 1198139\n",
            "Episode: 39/50 RapTime: 0:00:10.383213 FixedProfit: 1216571\n",
            "Episode: 39/50 RapTime: 0:00:10.330600 FixedProfit: 1163082\n",
            "Episode: 40/50 RapTime: 0:00:10.325540 FixedProfit: 1194143\n",
            "Episode: 40/50 RapTime: 0:00:10.290696 FixedProfit: 1194143\n",
            "Episode: 40/50 RapTime: 0:00:10.321226 FixedProfit: 1194143\n",
            "Episode: 40/50 RapTime: 0:00:10.340558 FixedProfit: 1194143\n",
            "Episode: 41/50 RapTime: 0:00:10.267576 FixedProfit: 1163106\n",
            "Episode: 41/50 RapTime: 0:00:10.328766 FixedProfit: 1194143\n",
            "Episode: 41/50 RapTime: 0:00:10.290623 FixedProfit: 1067074\n",
            "Episode: 41/50 RapTime: 0:00:10.309324 FixedProfit: 1161885\n",
            "Episode: 42/50 RapTime: 0:00:10.310271 FixedProfit: 1208128\n",
            "Episode: 42/50 RapTime: 0:00:10.314066 FixedProfit: 1194143\n",
            "Episode: 42/50 RapTime: 0:00:10.362897 FixedProfit: 1212929\n",
            "Episode: 42/50 RapTime: 0:00:10.333619 FixedProfit: 1194143\n",
            "Episode: 43/50 RapTime: 0:00:10.303277 FixedProfit: 1192980\n",
            "Episode: 43/50 RapTime: 0:00:10.277844 FixedProfit: 1202990\n",
            "Episode: 43/50 RapTime: 0:00:10.255518 FixedProfit: 1192980\n",
            "Episode: 43/50 RapTime: 0:00:10.280074 FixedProfit: 1194143\n",
            "Episode: 44/50 RapTime: 0:00:10.253380 FixedProfit: 1179001\n",
            "Episode: 44/50 RapTime: 0:00:10.232996 FixedProfit: 1192980\n",
            "Episode: 44/50 RapTime: 0:00:10.236926 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:10.325598 FixedProfit: 1033580\n",
            "Episode: 45/50 RapTime: 0:00:10.283662 FixedProfit: 1144522\n",
            "Episode: 45/50 RapTime: 0:00:10.370641 FixedProfit: 1194143\n",
            "Episode: 45/50 RapTime: 0:00:10.357470 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:10.445597 FixedProfit: 1196192\n",
            "Episode: 46/50 RapTime: 0:00:10.612087 FixedProfit: 1208128\n",
            "Episode: 46/50 RapTime: 0:00:10.579617 FixedProfit: 1192980\n",
            "Episode: 46/50 RapTime: 0:00:10.632166 FixedProfit: 1208128\n",
            "Episode: 46/50 RapTime: 0:00:10.615438 FixedProfit: 1208128\n",
            "Episode: 47/50 RapTime: 0:00:10.456079 FixedProfit: 1208128\n",
            "Episode: 47/50 RapTime: 0:00:10.429953 FixedProfit: 1183321\n",
            "Episode: 47/50 RapTime: 0:00:10.348268 FixedProfit: 1197165Episode: 47/50 RapTime: 0:00:10.399387 FixedProfit: 1197165\n",
            "\n",
            "Episode: 48/50 RapTime: 0:00:10.541062 FixedProfit: 1079166\n",
            "Episode: 48/50 RapTime: 0:00:10.516813 FixedProfit: 1194143\n",
            "Episode: 48/50 RapTime: 0:00:10.538956 FixedProfit: 1194143\n",
            "Episode: 48/50 RapTime: 0:00:10.542758 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:10.399184 FixedProfit: 1202990\n",
            "Episode: 49/50 RapTime: 0:00:10.424744 FixedProfit: 1194143\n",
            "Episode: 49/50 RapTime: 0:00:10.482680 FixedProfit: 1222057\n",
            "Episode: 49/50 RapTime: 0:00:10.485554 FixedProfit: 1167172\n",
            "Episode: 50/50 RapTime: 0:00:10.483242 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:10.498540 FixedProfit: 1183321\n",
            "Episode: 50/50 RapTime: 0:00:10.314399 FixedProfit: 1183321\n",
            "Episode: 50/50 RapTime: 0:00:10.326213 FixedProfit: 1208128\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}