{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w_q_learning_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMQqpGZQZOsk0OKiHSfKzXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/w_q_learning_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "51ac1cd8-01b9-4535-8cfb-52a2d89a6dc4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_test.csv'\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'w_qlearning_test.csv'\n",
        "\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "rewards_folder = '/content/drive/My Drive/' + exp_dir + 'rl_rewards'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd-UoFozKs9l"
      },
      "source": [
        "def make_scaler(env):\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, n_hidden_layers=1, hidden_dim=32):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "\n",
        "        n_mid = 3\n",
        "        n_state = 3\n",
        "        n_action = 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "\n",
        "        model_2 = Sequential()\n",
        "        model_2.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model_2.add(ReLU()) \n",
        "        model_2.add(Dense(n_mid))\n",
        "        model_2.add(ReLU()) \n",
        "        model_2.add(Dense(n_action))\n",
        "        model_2.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model_2.summary()))\n",
        "        self.model_2 = model_2\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done, s_flag):\n",
        "\n",
        "        next_act_values = self.model.predict(next_state,s_flag)\n",
        "        next_action =np.argmax(next_act_values[0])\n",
        "\n",
        "        if s_flag == 11:\n",
        "            q = self.model.predict(state)  \n",
        "            next_q = self.model_2.predict(next_state)\n",
        "            t = np.copy(q)\n",
        "\n",
        "            if done:\n",
        "                t[:, action] = reward\n",
        "            else:\n",
        "                t[:, action] = reward + self.gamma*np.max(next_q, axis=1)\n",
        "            self.model.train_on_batch(state, t)\n",
        "        else:\n",
        "            q = self.model_2.predict(state)  \n",
        "            next_q = self.model.predict(next_state)\n",
        "            t = np.copy(q)\n",
        "\n",
        "            if done:\n",
        "                t[:, action] = reward\n",
        "            else:\n",
        "                t[:, action] = reward + self.gamma*np.max(next_q, axis=1)\n",
        "            self.model_2.train_on_batch(state, t)\n",
        "\n",
        "    def predict(self, state, s_flag = 12):\n",
        "        values = None\n",
        "        q1 = self.model.predict(state)\n",
        "        q2 = self.model_2.predict(state)\n",
        "        if s_flag == 12:\n",
        "            values = np.array([q1[0,a] + q2[0,a] for a in range(2)])\n",
        "        elif s_flag == 11:\n",
        "            values = np.array([q1[0,a] + q1[0,a] for a in range(2)])\n",
        "        else:\n",
        "            values = np.array([q2[0,a] + q2[0,a] for a in range(2)])\n",
        "        return values\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.model.load_weights(name)\n",
        "        self.model_2.load_weights(name2)\n",
        "\n",
        "    def save(self, name, name2):\n",
        "        self.model.save_weights(name)\n",
        "        self.model_2.save_weights(name2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, brain, state_size=3, action_size=3):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.brain = brain\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "\n",
        "    def act(self, state,s_flag=12):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_size)\n",
        "        act_values = self.brain.predict(state,s_flag)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done, s_flag):\n",
        "        self.brain.train(state, action, reward, next_state, done, s_flag)\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.brain.load(name, name2)\n",
        "\n",
        "    def save(self, name,name2):\n",
        "        self.brain.save(name, name2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "def play_game(env, agent , episodes_times = 1000, mode = 'test', batch_size = 32):\n",
        "    if mode == 'test':\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "    else:\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        done = False\n",
        "        start_time = datetime.now()\n",
        "       \n",
        "        while not done:\n",
        "            s_flag = 12\n",
        "            action = agent.act(state,s_flag)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "\n",
        "            if mode == 'train':\n",
        "                rand = np.random.random()\n",
        "                if rand <= 0.5:\n",
        "                    s_flag = 11\n",
        "                else:\n",
        "                    s_flag = 22\n",
        "                agent.train(state, action, reward, next_state, done, s_flag)\n",
        "            \n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f} TradeTimes: {info['trade_time']} TradeWin: {info['trade_win']}\")\n",
        "        else:\n",
        "            record = pd.Series(info['cur_revenue'], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f}\")\n",
        "    \n",
        "        state = next_state\n",
        "        df_rec = df_rec.append(record, ignore_index=True)\n",
        "    return df_rec"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "3e5f3e91-ae0b-4fcd-e501-2dbd6ce8858b"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "mode = 'test'\n",
        "brain = Brain()\n",
        "agent = Agent(brain=brain)\n",
        "\n",
        "if mode == 'test':\n",
        "    with open(f'{models_folder}/scaler_w_ql.pkl', 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    agent.epsilon = 0.01\n",
        "    agent.load(f'{models_folder}/w_ql.h5',f'{models_folder}/w_ql2.h5')\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "scaler = make_scaler(env)\n",
        "df_rec = play_game(env, agent, episodes_times = episodes_times, mode = mode, batch_size = batch_size)\n",
        "\n",
        "if mode == 'train':\n",
        "    agent.save(f'{models_folder}/w_ql.h5',f'{models_folder}/w_ql2.h5')\n",
        "    with open(f'{models_folder}/scaler_w_ql.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "df_rec.to_csv(csv_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/100 RapTime: 0:00:52.981661 FixedProfit: 1525117 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 2/100 RapTime: 0:00:52.455309 FixedProfit: 1545838 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 3/100 RapTime: 0:00:52.620653 FixedProfit: 1540980 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 4/100 RapTime: 0:00:53.697524 FixedProfit: 1537187 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 5/100 RapTime: 0:00:52.679712 FixedProfit: 1530548 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 6/100 RapTime: 0:00:54.324495 FixedProfit: 1505527 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 7/100 RapTime: 0:00:51.970112 FixedProfit: 1522064 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 8/100 RapTime: 0:00:52.424373 FixedProfit: 1493591 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 9/100 RapTime: 0:00:52.856163 FixedProfit: 1561562 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 10/100 RapTime: 0:00:52.345601 FixedProfit: 1520496 TradeTimes: 8 TradeWin: 5\n",
            "Episode: 11/100 RapTime: 0:00:52.332199 FixedProfit: 1487993 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 12/100 RapTime: 0:00:53.867475 FixedProfit: 1541193 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 13/100 RapTime: 0:00:52.740586 FixedProfit: 1552810 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 14/100 RapTime: 0:00:52.308840 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/100 RapTime: 0:00:52.759871 FixedProfit: 1542003 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 16/100 RapTime: 0:00:52.972143 FixedProfit: 1515822 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 17/100 RapTime: 0:00:53.224268 FixedProfit: 1631116 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 18/100 RapTime: 0:00:54.574651 FixedProfit: 1546474 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 19/100 RapTime: 0:00:53.592284 FixedProfit: 1564072 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 20/100 RapTime: 0:00:53.546465 FixedProfit: 1545159 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 21/100 RapTime: 0:00:53.611535 FixedProfit: 1538156 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 22/100 RapTime: 0:00:53.451927 FixedProfit: 1548463 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 23/100 RapTime: 0:00:53.607566 FixedProfit: 1578832 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 24/100 RapTime: 0:00:55.235916 FixedProfit: 1538289 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 25/100 RapTime: 0:00:53.239431 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 26/100 RapTime: 0:00:53.627770 FixedProfit: 1535771 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 27/100 RapTime: 0:00:54.133367 FixedProfit: 1504052 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 28/100 RapTime: 0:00:54.385297 FixedProfit: 1471103 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 29/100 RapTime: 0:00:54.575191 FixedProfit: 1536935 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 30/100 RapTime: 0:00:54.132032 FixedProfit: 1560542 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 31/100 RapTime: 0:00:54.308356 FixedProfit: 1541560 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 32/100 RapTime: 0:00:53.638891 FixedProfit: 1541109 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 33/100 RapTime: 0:00:53.373108 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 34/100 RapTime: 0:00:53.894385 FixedProfit: 1705365 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 35/100 RapTime: 0:00:54.371521 FixedProfit: 1502908 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 36/100 RapTime: 0:00:53.693917 FixedProfit: 1536934 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 37/100 RapTime: 0:00:53.549544 FixedProfit: 1516082 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 38/100 RapTime: 0:00:53.349760 FixedProfit: 1542539 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 39/100 RapTime: 0:00:53.760273 FixedProfit: 1554014 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 40/100 RapTime: 0:00:52.795411 FixedProfit: 1537048 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 41/100 RapTime: 0:00:53.024936 FixedProfit: 1514210 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 42/100 RapTime: 0:00:54.858847 FixedProfit: 1691873 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 43/100 RapTime: 0:00:52.523868 FixedProfit: 1564341 TradeTimes: 7 TradeWin: 7\n",
            "Episode: 44/100 RapTime: 0:00:53.006661 FixedProfit: 1609047 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 45/100 RapTime: 0:00:53.502717 FixedProfit: 1551616 TradeTimes: 8 TradeWin: 8\n",
            "Episode: 46/100 RapTime: 0:00:53.912237 FixedProfit: 1525469 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 47/100 RapTime: 0:00:53.881526 FixedProfit: 1558113 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 48/100 RapTime: 0:00:52.809495 FixedProfit: 1500883 TradeTimes: 8 TradeWin: 8\n",
            "Episode: 49/100 RapTime: 0:00:54.648681 FixedProfit: 1526095 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 50/100 RapTime: 0:00:54.482420 FixedProfit: 1542791 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 51/100 RapTime: 0:00:53.433932 FixedProfit: 1553423 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 52/100 RapTime: 0:00:53.608143 FixedProfit: 1515618 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 53/100 RapTime: 0:00:54.066144 FixedProfit: 1542447 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 54/100 RapTime: 0:00:54.539960 FixedProfit: 1540828 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 55/100 RapTime: 0:00:53.931896 FixedProfit: 1484941 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 56/100 RapTime: 0:00:54.878761 FixedProfit: 1536761 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 57/100 RapTime: 0:00:54.380258 FixedProfit: 1413620 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 58/100 RapTime: 0:00:54.077207 FixedProfit: 1543209 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 59/100 RapTime: 0:00:53.565907 FixedProfit: 1524659 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 60/100 RapTime: 0:00:53.332667 FixedProfit: 1575121 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 61/100 RapTime: 0:00:55.119982 FixedProfit: 1538776 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 62/100 RapTime: 0:00:53.663285 FixedProfit: 1571128 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 63/100 RapTime: 0:00:53.397626 FixedProfit: 1606550 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 64/100 RapTime: 0:00:54.435683 FixedProfit: 1583488 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 65/100 RapTime: 0:00:54.693615 FixedProfit: 1545463 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 66/100 RapTime: 0:00:54.423081 FixedProfit: 1537730 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 67/100 RapTime: 0:00:53.892388 FixedProfit: 1555640 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 68/100 RapTime: 0:00:54.133613 FixedProfit: 1542659 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 69/100 RapTime: 0:00:54.182551 FixedProfit: 1540500 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 70/100 RapTime: 0:00:53.689075 FixedProfit: 1525848 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 71/100 RapTime: 0:00:53.440735 FixedProfit: 1532677 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 72/100 RapTime: 0:00:55.693582 FixedProfit: 1512487 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 73/100 RapTime: 0:00:52.970952 FixedProfit: 1624177 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 74/100 RapTime: 0:00:53.605115 FixedProfit: 1572354 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 75/100 RapTime: 0:00:54.455742 FixedProfit: 1558212 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 76/100 RapTime: 0:00:54.081521 FixedProfit: 1522662 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 77/100 RapTime: 0:00:54.139739 FixedProfit: 1504458 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 78/100 RapTime: 0:00:54.110391 FixedProfit: 1534293 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 79/100 RapTime: 0:00:54.426806 FixedProfit: 1526541 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 80/100 RapTime: 0:00:54.062828 FixedProfit: 1578256 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 81/100 RapTime: 0:00:53.768644 FixedProfit: 1523506 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 82/100 RapTime: 0:00:52.906928 FixedProfit: 1507685 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 83/100 RapTime: 0:00:54.119089 FixedProfit: 1541039 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 84/100 RapTime: 0:00:55.152988 FixedProfit: 1551684 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 85/100 RapTime: 0:00:52.739017 FixedProfit: 1560554 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 86/100 RapTime: 0:00:54.393791 FixedProfit: 1584659 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 87/100 RapTime: 0:00:54.037103 FixedProfit: 1608358 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 88/100 RapTime: 0:00:53.988159 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 89/100 RapTime: 0:00:53.734639 FixedProfit: 1518205 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 90/100 RapTime: 0:00:53.424053 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 91/100 RapTime: 0:00:55.069245 FixedProfit: 1566144 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 92/100 RapTime: 0:00:53.832947 FixedProfit: 1541279 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 93/100 RapTime: 0:00:53.644115 FixedProfit: 1539143 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 94/100 RapTime: 0:00:53.291239 FixedProfit: 1535041 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 95/100 RapTime: 0:00:53.880469 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 96/100 RapTime: 0:00:53.928531 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 97/100 RapTime: 0:00:53.889025 FixedProfit: 1556993 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 98/100 RapTime: 0:00:53.710896 FixedProfit: 1513282 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 99/100 RapTime: 0:00:53.886981 FixedProfit: 1483670 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 100/100 RapTime: 0:00:53.119833 FixedProfit: 1530458 TradeTimes: 3 TradeWin: 3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}