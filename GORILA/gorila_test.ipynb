{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gorila_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM5aYG7VNJAsTgMp6VRyuzz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/gorila_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "1a7fc02a-ea41-4899-8d23-d82472b446f6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "mode = 'test'\n",
        "name = 'gorila'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1RNmtkaZ2W"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        mastermodel = Sequential()\n",
        "        mastermodel.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_mid))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_action))\n",
        "        mastermodel.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((mastermodel.summary()))\n",
        "        self.mastermodel = mastermodel\n",
        "    \n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "        self.masterbrain = masterbrain\n",
        "\n",
        "    def load(self, name):\n",
        "        self.masterbrain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.masterbrain.save(name)\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def train_on_batch(self, state, target_full):\n",
        "        self.model.train_on_batch(state, target_full)\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w1_BMH7hLQ8"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrEa4wpGG1DF"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, memory, max_size, batch_size = 32):\n",
        "\n",
        "        self.brain   = brain\n",
        "        self.memory  = memory\n",
        "        self.epsilon = 1.0\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.brain.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42NIN-PGBOc8"
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, brain, memory, batch_size = 32):\n",
        "\n",
        "        self.brain  = brain\n",
        "        self.memory = memory\n",
        "\n",
        "        self.gamma       = 0.95\n",
        "        self.epsilon     = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r           = 0.995\n",
        "        self.batch_size  = batch_size\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.brain.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.brain.predict(states)\n",
        "\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.brain.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, learner, num, mdl_dir, name, episodes_times = 1000, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.actor          = actor\n",
        "        self.learner        = learner\n",
        "        self.num            = str(num)\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "        self.actor.layering()\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.actor.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.actor.store_transition(state, action, reward, next_state, done)\n",
        "                    self.learner.learn()\n",
        "                \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                learner.integration()\n",
        "                actor.layering()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name, self.num))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name, self.num))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "070a8e6a-874f-4bf9-9b3b-76ada24254eb"
      },
      "source": [
        "initial_money  = 1000000\n",
        "episodes_times = 25\n",
        "batch_size     = 32\n",
        "max_size       = 1000\n",
        "\n",
        "masterbrain    = ParameterServer()\n",
        "memory         = ReplayMemory(max_size, batch_size)\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env     = Environment(df, initial_money=initial_money, mode = mode)\n",
        "    brain   = Brain(masterbrain)\n",
        "    actor   = Actor(brain, memory, max_size, batch_size)\n",
        "    learner = Learner(brain, memory, batch_size)\n",
        "    main    = Main(env, actor, learner, i, mdl_dir, name, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_5 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_6 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_7 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_8 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_9 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/25 RapTime: 0:01:53.851204 FixedProfit: 1003977 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:01:54.221834 FixedProfit: 1010680 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 1/25 RapTime: 0:01:55.177466 FixedProfit: 1006889 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:01:56.740617 FixedProfit: 1013468 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 2/25 RapTime: 0:01:56.675258 FixedProfit: 1013761 TradeTimes: 7 TradeWin: 3\n",
            "Episode: 2/25 RapTime: 0:01:53.923953 FixedProfit: 1034707 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 2/25 RapTime: 0:01:56.446392 FixedProfit: 1021311 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 2/25 RapTime: 0:01:56.183527 FixedProfit: 987001 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:01:55.517009 FixedProfit: 1022567 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 3/25 RapTime: 0:01:57.257385 FixedProfit: 979471 TradeTimes: 6 TradeWin: 2\n",
            "Episode: 3/25 RapTime: 0:01:58.474028 FixedProfit: 895381 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 3/25 RapTime: 0:01:58.518462 FixedProfit: 1130965 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 4/25 RapTime: 0:01:58.560122 FixedProfit: 1002268 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:01:57.213120 FixedProfit: 1016403 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 4/25 RapTime: 0:01:56.195335 FixedProfit: 964602 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 4/25 RapTime: 0:01:59.481936 FixedProfit: 998647 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 5/25 RapTime: 0:01:56.853423 FixedProfit: 1002696 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 5/25 RapTime: 0:01:58.225560 FixedProfit: 1018035 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 5/25 RapTime: 0:02:00.250090 FixedProfit: 996603 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:02:00.004980 FixedProfit: 994954 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 6/25 RapTime: 0:01:58.847134 FixedProfit: 1018673 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 6/25 RapTime: 0:01:58.262286 FixedProfit: 1001773 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 6/25 RapTime: 0:02:00.009203 FixedProfit: 1012909 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 6/25 RapTime: 0:01:56.333543 FixedProfit: 989472 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:02:00.414678 FixedProfit: 998514 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 7/25 RapTime: 0:01:59.513089 FixedProfit: 1010688 TradeTimes: 8 TradeWin: 5\n",
            "Episode: 7/25 RapTime: 0:02:00.522855 FixedProfit: 1006863 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 7/25 RapTime: 0:02:00.704165 FixedProfit: 1021003 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 8/25 RapTime: 0:02:00.088758 FixedProfit: 1006854 TradeTimes: 7 TradeWin: 4\n",
            "Episode: 8/25 RapTime: 0:01:59.497920 FixedProfit: 993394 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 8/25 RapTime: 0:02:03.757873 FixedProfit: 984008 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:01:59.387363 FixedProfit: 966616 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 9/25 RapTime: 0:01:59.566326 FixedProfit: 997038 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 9/25 RapTime: 0:02:01.456714 FixedProfit: 1001569 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:02:00.887064 FixedProfit: 1011457 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 9/25 RapTime: 0:02:00.882783 FixedProfit: 1013299 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 10/25 RapTime: 0:01:58.623159 FixedProfit: 964746 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 10/25 RapTime: 0:01:58.463907 FixedProfit: 988439 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 10/25 RapTime: 0:02:00.019166 FixedProfit: 1020175 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 10/25 RapTime: 0:02:00.643723 FixedProfit: 1032990 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 11/25 RapTime: 0:02:10.095010 FixedProfit: 1016110 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 11/25 RapTime: 0:02:08.701511 FixedProfit: 1034339 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 11/25 RapTime: 0:02:12.265431 FixedProfit: 997148 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 11/25 RapTime: 0:02:10.174249 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:02:05.214321 FixedProfit: 992616 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:02:06.008018 FixedProfit: 996572 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 12/25 RapTime: 0:02:04.382302 FixedProfit: 976365 TradeTimes: 5 TradeWin: 0\n",
            "Episode: 12/25 RapTime: 0:02:03.836861 FixedProfit: 1013359 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 13/25 RapTime: 0:01:59.040947 FixedProfit: 1043478 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 13/25 RapTime: 0:02:00.942027 FixedProfit: 939407 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 13/25 RapTime: 0:02:01.452809 FixedProfit: 1028767 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 13/25 RapTime: 0:02:02.140366 FixedProfit: 956645 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 14/25 RapTime: 0:02:03.555546 FixedProfit: 961560 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:01:59.772276 FixedProfit: 995596 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 14/25 RapTime: 0:01:59.750125 FixedProfit: 945189 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 14/25 RapTime: 0:02:05.223851 FixedProfit: 990979 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 15/25 RapTime: 0:02:00.843930 FixedProfit: 1013456 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 15/25 RapTime: 0:01:59.898834 FixedProfit: 1010830 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 15/25 RapTime: 0:02:02.418066 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:02:01.155365 FixedProfit: 1001632 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 16/25 RapTime: 0:02:02.823537 FixedProfit: 1007495 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 16/25 RapTime: 0:02:02.948310 FixedProfit: 1032874 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 16/25 RapTime: 0:02:03.958354 FixedProfit: 997142 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:02:03.090795 FixedProfit: 1021938 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 17/25 RapTime: 0:02:02.378163 FixedProfit: 1019133 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 17/25 RapTime: 0:02:03.486935 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:02:03.302594 FixedProfit: 995977 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:02:00.697263 FixedProfit: 1002444 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 18/25 RapTime: 0:02:00.817479 FixedProfit: 1025709 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 18/25 RapTime: 0:02:02.623193 FixedProfit: 991743 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 18/25 RapTime: 0:02:02.869205 FixedProfit: 1024586 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 18/25 RapTime: 0:02:04.254628 FixedProfit: 1022265 TradeTimes: 6 TradeWin: 2\n",
            "Episode: 19/25 RapTime: 0:02:03.893037 FixedProfit: 986140 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 19/25 RapTime: 0:02:04.229085 FixedProfit: 971894 TradeTimes: 4 TradeWin: 0\n",
            "Episode: 19/25 RapTime: 0:02:05.021467 FixedProfit: 989570 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 19/25 RapTime: 0:02:04.193340 FixedProfit: 999213 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 20/25 RapTime: 0:02:04.958118 FixedProfit: 984424 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:02:03.001090 FixedProfit: 1003468 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:02:05.857985 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:02:05.078521 FixedProfit: 1011481 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 21/25 RapTime: 0:02:02.289655 FixedProfit: 987312 TradeTimes: 6 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:02:04.082411 FixedProfit: 1010127 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 21/25 RapTime: 0:02:02.911185 FixedProfit: 1007574 TradeTimes: 7 TradeWin: 3\n",
            "Episode: 21/25 RapTime: 0:02:05.858785 FixedProfit: 1004277 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:02:05.920609 FixedProfit: 988434 TradeTimes: 4 TradeWin: 0\n",
            "Episode: 22/25 RapTime: 0:02:08.384068 FixedProfit: 1037180 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 22/25 RapTime: 0:02:06.732136 FixedProfit: 1062268 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 22/25 RapTime: 0:02:05.803740 FixedProfit: 1013977 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 23/25 RapTime: 0:02:07.270209 FixedProfit: 975524 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 23/25 RapTime: 0:02:04.893583 FixedProfit: 1003464 TradeTimes: 6 TradeWin: 2\n",
            "Episode: 23/25 RapTime: 0:02:05.119386 FixedProfit: 983450 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 23/25 RapTime: 0:02:07.010672 FixedProfit: 1014765 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 24/25 RapTime: 0:02:06.633766 FixedProfit: 1004683 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 24/25 RapTime: 0:02:06.519645 FixedProfit: 983394 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 24/25 RapTime: 0:02:07.970499 FixedProfit: 1000358 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:02:07.764541 FixedProfit: 1002596 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 25/25 RapTime: 0:02:08.789857 FixedProfit: 1013514 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:02:05.997659 FixedProfit: 1007504 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:02:03.827393 FixedProfit: 973921 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 25/25 RapTime: 0:01:58.526193 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
