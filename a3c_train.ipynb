{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a3c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "516f4093-3b29-469e-da51-6a5a349dbcc5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "mode = 'train'\n",
        "name = 'a3c'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class MasterBrain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model(input_, [actor, critic])\n",
        "        mastermodel.compile(optimizer=Adam(lr=lr))\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP-xlKoLAwG7"
      },
      "source": [
        "class Leaner:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "def play_game(env, actor, critic, scaler, episodes_times = 25, batch_size = 32, mode = 'train'):\n",
        "\n",
        "    actor.layering()\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        state = state.flatten()\n",
        "        done = False\n",
        "        start_time = datetime.now()\n",
        "        experiences = []\n",
        "\n",
        "        while not done:\n",
        "            action = actor.policynetwork(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            next_state = next_state.flatten()\n",
        "\n",
        "            if mode == 'train':\n",
        "                experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                if len(experiences) == batch_size:\n",
        "                    critic.valuenetwork(experiences)\n",
        "                    experiences = []\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            actor.integration()\n",
        "            actor.layering()\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)\n",
        "\n",
        "class Main:\n",
        "    def __init__(self, env, actor, critic, num, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "        \n",
        "        self.actor.layering()\n",
        "\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            experiences = []\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if mode == 'train':\n",
        "                    experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                    if len(experiences) == self.batch_size:\n",
        "                        self.critic.valuenetwork(experiences)\n",
        "                        experiences = []\n",
        "\n",
        "                state = next_state\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                actor.integration()\n",
        "                actor.layering()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6361fe09-23b8-4704-8922-63771bdad85b"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 50\n",
        "batch_size = 32\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    actor = Actor(brain)\n",
        "    critic = Critic(model)\n",
        "    main = Main(env, actor, critic, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/50 RapTime: 0:00:07.881006 FixedProfit: 1034956\n",
            "Episode: 1/50 RapTime: 0:00:07.880295 FixedProfit: 1422922\n",
            "Episode: 1/50 RapTime: 0:00:07.895315 FixedProfit: 933188\n",
            "Episode: 1/50 RapTime: 0:00:07.900219 FixedProfit: 1065806\n",
            "Episode: 2/50 RapTime: 0:00:07.440526 FixedProfit: 1138167\n",
            "Episode: 2/50 RapTime: 0:00:07.506792 FixedProfit: 1234571\n",
            "Episode: 2/50 RapTime: 0:00:07.514661 FixedProfit: 1289901\n",
            "Episode: 2/50 RapTime: 0:00:07.531822 FixedProfit: 1117315\n",
            "Episode: 3/50 RapTime: 0:00:07.466657 FixedProfit: 1197165\n",
            "Episode: 3/50 RapTime: 0:00:07.502484 FixedProfit: 1141254\n",
            "Episode: 3/50 RapTime: 0:00:07.563610 FixedProfit: 1143150\n",
            "Episode: 3/50 RapTime: 0:00:07.492343 FixedProfit: 1024594\n",
            "Episode: 4/50 RapTime: 0:00:07.542159 FixedProfit: 1197165\n",
            "Episode: 4/50 RapTime: 0:00:07.476941 FixedProfit: 1086457\n",
            "Episode: 4/50 RapTime: 0:00:07.548300 FixedProfit: 1393174\n",
            "Episode: 4/50 RapTime: 0:00:07.622474 FixedProfit: 1167318\n",
            "Episode: 5/50 RapTime: 0:00:07.477610 FixedProfit: 1197165\n",
            "Episode: 5/50 RapTime: 0:00:07.526921 FixedProfit: 1031788\n",
            "Episode: 5/50 RapTime: 0:00:07.475046 FixedProfit: 1009820\n",
            "Episode: 5/50 RapTime: 0:00:07.468468 FixedProfit: 1160366\n",
            "Episode: 6/50 RapTime: 0:00:07.463159 FixedProfit: 1197165\n",
            "Episode: 6/50 RapTime: 0:00:07.463389 FixedProfit: 1149763\n",
            "Episode: 6/50 RapTime: 0:00:07.448763 FixedProfit: 1100149\n",
            "Episode: 6/50 RapTime: 0:00:07.493006 FixedProfit: 1347147\n",
            "Episode: 7/50 RapTime: 0:00:07.522709 FixedProfit: 1197165\n",
            "Episode: 7/50 RapTime: 0:00:07.505764 FixedProfit: 1218447\n",
            "Episode: 7/50 RapTime: 0:00:07.530326 FixedProfit: 1265687\n",
            "Episode: 7/50 RapTime: 0:00:07.570155 FixedProfit: 1239132\n",
            "Episode: 8/50 RapTime: 0:00:07.608424 FixedProfit: 1197165\n",
            "Episode: 8/50 RapTime: 0:00:07.578242 FixedProfit: 992580\n",
            "Episode: 8/50 RapTime: 0:00:07.606221 FixedProfit: 1088941\n",
            "Episode: 8/50 RapTime: 0:00:07.620788 FixedProfit: 1297287\n",
            "Episode: 9/50 RapTime: 0:00:07.592312 FixedProfit: 1197165\n",
            "Episode: 9/50 RapTime: 0:00:07.619270 FixedProfit: 1140363\n",
            "Episode: 9/50 RapTime: 0:00:07.653045 FixedProfit: 1120673\n",
            "Episode: 9/50 RapTime: 0:00:07.604644 FixedProfit: 1089252\n",
            "Episode: 10/50 RapTime: 0:00:07.557440 FixedProfit: 1197165\n",
            "Episode: 10/50 RapTime: 0:00:07.477228 FixedProfit: 1070555\n",
            "Episode: 10/50 RapTime: 0:00:07.533097 FixedProfit: 1279740\n",
            "Episode: 10/50 RapTime: 0:00:07.576997 FixedProfit: 1060423\n",
            "Episode: 11/50 RapTime: 0:00:07.519354 FixedProfit: 1197165\n",
            "Episode: 11/50 RapTime: 0:00:07.546264 FixedProfit: 1163785\n",
            "Episode: 11/50 RapTime: 0:00:07.566472 FixedProfit: 1024876\n",
            "Episode: 11/50 RapTime: 0:00:07.578508 FixedProfit: 1192176\n",
            "Episode: 12/50 RapTime: 0:00:07.543762 FixedProfit: 1197165\n",
            "Episode: 12/50 RapTime: 0:00:07.580843 FixedProfit: 1237903\n",
            "Episode: 12/50 RapTime: 0:00:07.569094 FixedProfit: 983648\n",
            "Episode: 12/50 RapTime: 0:00:07.633723 FixedProfit: 1026740\n",
            "Episode: 13/50 RapTime: 0:00:07.477814 FixedProfit: 1197165\n",
            "Episode: 13/50 RapTime: 0:00:07.469855 FixedProfit: 1102571\n",
            "Episode: 13/50 RapTime: 0:00:07.474107 FixedProfit: 1090758\n",
            "Episode: 13/50 RapTime: 0:00:07.453933 FixedProfit: 1028016\n",
            "Episode: 14/50 RapTime: 0:00:07.427809 FixedProfit: 1197165\n",
            "Episode: 14/50 RapTime: 0:00:07.414148 FixedProfit: 975856\n",
            "Episode: 14/50 RapTime: 0:00:07.423457 FixedProfit: 1042011\n",
            "Episode: 14/50 RapTime: 0:00:07.439370 FixedProfit: 1142075\n",
            "Episode: 15/50 RapTime: 0:00:07.441345 FixedProfit: 1197165\n",
            "Episode: 15/50 RapTime: 0:00:07.375906 FixedProfit: 1037427\n",
            "Episode: 15/50 RapTime: 0:00:07.460102 FixedProfit: 1176683\n",
            "Episode: 15/50 RapTime: 0:00:07.464598 FixedProfit: 1058195\n",
            "Episode: 16/50 RapTime: 0:00:07.505707 FixedProfit: 1197165\n",
            "Episode: 16/50 RapTime: 0:00:07.429687 FixedProfit: 1112063\n",
            "Episode: 16/50 RapTime: 0:00:07.504299 FixedProfit: 1053428\n",
            "Episode: 16/50 RapTime: 0:00:07.481344 FixedProfit: 1000736\n",
            "Episode: 17/50 RapTime: 0:00:07.461890 FixedProfit: 1197165\n",
            "Episode: 17/50 RapTime: 0:00:07.577623 FixedProfit: 1104252\n",
            "Episode: 17/50 RapTime: 0:00:07.500236 FixedProfit: 1009344\n",
            "Episode: 17/50 RapTime: 0:00:07.502152 FixedProfit: 1162552\n",
            "Episode: 18/50 RapTime: 0:00:07.448370 FixedProfit: 1197165\n",
            "Episode: 18/50 RapTime: 0:00:07.525056 FixedProfit: 1159612\n",
            "Episode: 18/50 RapTime: 0:00:07.460944 FixedProfit: 1121494\n",
            "Episode: 18/50 RapTime: 0:00:07.476819 FixedProfit: 1087525\n",
            "Episode: 19/50 RapTime: 0:00:07.699411 FixedProfit: 1197165\n",
            "Episode: 19/50 RapTime: 0:00:07.651666 FixedProfit: 1078740\n",
            "Episode: 19/50 RapTime: 0:00:07.711502 FixedProfit: 1289388\n",
            "Episode: 19/50 RapTime: 0:00:07.729876 FixedProfit: 1178420\n",
            "Episode: 20/50 RapTime: 0:00:07.773026 FixedProfit: 1197165\n",
            "Episode: 20/50 RapTime: 0:00:07.860591 FixedProfit: 1163300\n",
            "Episode: 20/50 RapTime: 0:00:07.772836 FixedProfit: 1190685\n",
            "Episode: 20/50 RapTime: 0:00:07.815112 FixedProfit: 1040723\n",
            "Episode: 21/50 RapTime: 0:00:07.539476 FixedProfit: 1197165\n",
            "Episode: 21/50 RapTime: 0:00:07.563836 FixedProfit: 1109751\n",
            "Episode: 21/50 RapTime: 0:00:07.627574 FixedProfit: 1041226\n",
            "Episode: 21/50 RapTime: 0:00:07.586841 FixedProfit: 1058278\n",
            "Episode: 22/50 RapTime: 0:00:07.551090 FixedProfit: 1197165\n",
            "Episode: 22/50 RapTime: 0:00:07.522106 FixedProfit: 1002780\n",
            "Episode: 22/50 RapTime: 0:00:07.542180 FixedProfit: 1099005\n",
            "Episode: 22/50 RapTime: 0:00:07.516301 FixedProfit: 978149\n",
            "Episode: 23/50 RapTime: 0:00:07.370725 FixedProfit: 1197165\n",
            "Episode: 23/50 RapTime: 0:00:07.349104 FixedProfit: 1057198\n",
            "Episode: 23/50 RapTime: 0:00:07.412567 FixedProfit: 1054466\n",
            "Episode: 23/50 RapTime: 0:00:07.397358 FixedProfit: 989766\n",
            "Episode: 24/50 RapTime: 0:00:07.609987 FixedProfit: 1197165\n",
            "Episode: 24/50 RapTime: 0:00:07.614554 FixedProfit: 1027761\n",
            "Episode: 24/50 RapTime: 0:00:07.564967 FixedProfit: 1148190\n",
            "Episode: 24/50 RapTime: 0:00:07.651067 FixedProfit: 1207332\n",
            "Episode: 25/50 RapTime: 0:00:07.526306 FixedProfit: 1197165\n",
            "Episode: 25/50 RapTime: 0:00:07.608077 FixedProfit: 1112872\n",
            "Episode: 25/50 RapTime: 0:00:07.537385 FixedProfit: 1068074\n",
            "Episode: 25/50 RapTime: 0:00:07.554622 FixedProfit: 1178105\n",
            "Episode: 26/50 RapTime: 0:00:07.552623 FixedProfit: 1197165\n",
            "Episode: 26/50 RapTime: 0:00:07.513390 FixedProfit: 1030047\n",
            "Episode: 26/50 RapTime: 0:00:07.559317 FixedProfit: 1125015\n",
            "Episode: 26/50 RapTime: 0:00:07.617951 FixedProfit: 1017124\n",
            "Episode: 27/50 RapTime: 0:00:07.545629 FixedProfit: 1197165\n",
            "Episode: 27/50 RapTime: 0:00:07.592430 FixedProfit: 1331519\n",
            "Episode: 27/50 RapTime: 0:00:07.609700 FixedProfit: 1122425\n",
            "Episode: 27/50 RapTime: 0:00:07.560262 FixedProfit: 1167225\n",
            "Episode: 28/50 RapTime: 0:00:07.601406 FixedProfit: 1197165\n",
            "Episode: 28/50 RapTime: 0:00:07.618802 FixedProfit: 1024226\n",
            "Episode: 28/50 RapTime: 0:00:07.554387 FixedProfit: 1210814\n",
            "Episode: 28/50 RapTime: 0:00:07.644241 FixedProfit: 1043595\n",
            "Episode: 29/50 RapTime: 0:00:07.585011 FixedProfit: 1197165\n",
            "Episode: 29/50 RapTime: 0:00:07.578829 FixedProfit: 1241941\n",
            "Episode: 29/50 RapTime: 0:00:07.584832 FixedProfit: 1090336\n",
            "Episode: 29/50 RapTime: 0:00:07.634077 FixedProfit: 1081290\n",
            "Episode: 30/50 RapTime: 0:00:07.587088 FixedProfit: 1197165\n",
            "Episode: 30/50 RapTime: 0:00:07.566476 FixedProfit: 1113810\n",
            "Episode: 30/50 RapTime: 0:00:07.574531 FixedProfit: 1022015\n",
            "Episode: 30/50 RapTime: 0:00:07.602507 FixedProfit: 1005225\n",
            "Episode: 31/50 RapTime: 0:00:07.434351 FixedProfit: 1197165\n",
            "Episode: 31/50 RapTime: 0:00:07.457326 FixedProfit: 1027843\n",
            "Episode: 31/50 RapTime: 0:00:07.368892 FixedProfit: 1212078\n",
            "Episode: 31/50 RapTime: 0:00:07.414992 FixedProfit: 1020611\n",
            "Episode: 32/50 RapTime: 0:00:07.400152 FixedProfit: 1197165\n",
            "Episode: 32/50 RapTime: 0:00:07.391426 FixedProfit: 1079066\n",
            "Episode: 32/50 RapTime: 0:00:07.402234 FixedProfit: 1138339\n",
            "Episode: 32/50 RapTime: 0:00:07.477647 FixedProfit: 1016228\n",
            "Episode: 33/50 RapTime: 0:00:07.424314 FixedProfit: 1197165\n",
            "Episode: 33/50 RapTime: 0:00:07.402262 FixedProfit: 1134993\n",
            "Episode: 33/50 RapTime: 0:00:07.407113 FixedProfit: 1259674\n",
            "Episode: 33/50 RapTime: 0:00:07.390907 FixedProfit: 1038499\n",
            "Episode: 34/50 RapTime: 0:00:07.535220 FixedProfit: 1197165\n",
            "Episode: 34/50 RapTime: 0:00:07.552154 FixedProfit: 1068806\n",
            "Episode: 34/50 RapTime: 0:00:07.528050 FixedProfit: 1222865\n",
            "Episode: 34/50 RapTime: 0:00:07.630108 FixedProfit: 1289319\n",
            "Episode: 35/50 RapTime: 0:00:07.528562 FixedProfit: 1197165\n",
            "Episode: 35/50 RapTime: 0:00:07.563424 FixedProfit: 1087512\n",
            "Episode: 35/50 RapTime: 0:00:07.495293 FixedProfit: 986208\n",
            "Episode: 35/50 RapTime: 0:00:07.522543 FixedProfit: 1244602\n",
            "Episode: 36/50 RapTime: 0:00:07.534167 FixedProfit: 1197165\n",
            "Episode: 36/50 RapTime: 0:00:07.571241 FixedProfit: 1010252Episode: 36/50 RapTime: 0:00:07.569555 FixedProfit: 1220694\n",
            "\n",
            "Episode: 36/50 RapTime: 0:00:07.506380 FixedProfit: 1051981\n",
            "Episode: 37/50 RapTime: 0:00:07.536741 FixedProfit: 1197165\n",
            "Episode: 37/50 RapTime: 0:00:07.575633 FixedProfit: 1330154\n",
            "Episode: 37/50 RapTime: 0:00:07.600396 FixedProfit: 1140570\n",
            "Episode: 37/50 RapTime: 0:00:07.534400 FixedProfit: 1028957\n",
            "Episode: 38/50 RapTime: 0:00:07.530436 FixedProfit: 1197165\n",
            "Episode: 38/50 RapTime: 0:00:07.476624 FixedProfit: 1267459\n",
            "Episode: 38/50 RapTime: 0:00:07.532728 FixedProfit: 1177456\n",
            "Episode: 38/50 RapTime: 0:00:07.603709 FixedProfit: 1086149\n",
            "Episode: 39/50 RapTime: 0:00:07.531616 FixedProfit: 1197165\n",
            "Episode: 39/50 RapTime: 0:00:07.585131 FixedProfit: 1298120\n",
            "Episode: 39/50 RapTime: 0:00:07.560441 FixedProfit: 1096545\n",
            "Episode: 39/50 RapTime: 0:00:07.556693 FixedProfit: 1110156\n",
            "Episode: 40/50 RapTime: 0:00:07.460581 FixedProfit: 1197165\n",
            "Episode: 40/50 RapTime: 0:00:07.380280 FixedProfit: 932523\n",
            "Episode: 40/50 RapTime: 0:00:07.391321 FixedProfit: 1121080\n",
            "Episode: 40/50 RapTime: 0:00:07.425020 FixedProfit: 1074894\n",
            "Episode: 41/50 RapTime: 0:00:07.428569 FixedProfit: 1197165\n",
            "Episode: 41/50 RapTime: 0:00:07.393532 FixedProfit: 982084\n",
            "Episode: 41/50 RapTime: 0:00:07.431445 FixedProfit: 1146711\n",
            "Episode: 41/50 RapTime: 0:00:07.414963 FixedProfit: 1061121\n",
            "Episode: 42/50 RapTime: 0:00:07.488670 FixedProfit: 1197165\n",
            "Episode: 42/50 RapTime: 0:00:07.475955 FixedProfit: 1043474\n",
            "Episode: 42/50 RapTime: 0:00:07.486752 FixedProfit: 1046925\n",
            "Episode: 42/50 RapTime: 0:00:07.499004 FixedProfit: 903688\n",
            "Episode: 43/50 RapTime: 0:00:07.529668 FixedProfit: 1197165\n",
            "Episode: 43/50 RapTime: 0:00:07.611341 FixedProfit: 1247219\n",
            "Episode: 43/50 RapTime: 0:00:07.620748 FixedProfit: 1165892\n",
            "Episode: 43/50 RapTime: 0:00:07.576398 FixedProfit: 1104143\n",
            "Episode: 44/50 RapTime: 0:00:07.462060 FixedProfit: 1197165\n",
            "Episode: 44/50 RapTime: 0:00:07.436040 FixedProfit: 1115648\n",
            "Episode: 44/50 RapTime: 0:00:07.440929 FixedProfit: 1125980\n",
            "Episode: 44/50 RapTime: 0:00:07.428748 FixedProfit: 1209302\n",
            "Episode: 45/50 RapTime: 0:00:07.478435 FixedProfit: 1197165\n",
            "Episode: 45/50 RapTime: 0:00:07.478298 FixedProfit: 1199835\n",
            "Episode: 45/50 RapTime: 0:00:07.453140 FixedProfit: 1097641\n",
            "Episode: 45/50 RapTime: 0:00:07.428130 FixedProfit: 1214731\n",
            "Episode: 46/50 RapTime: 0:00:07.353377 FixedProfit: 1197165\n",
            "Episode: 46/50 RapTime: 0:00:07.370273 FixedProfit: 1211643\n",
            "Episode: 46/50 RapTime: 0:00:07.440139 FixedProfit: 1130749\n",
            "Episode: 46/50 RapTime: 0:00:07.434906 FixedProfit: 1221772\n",
            "Episode: 47/50 RapTime: 0:00:07.498736 FixedProfit: 1197165\n",
            "Episode: 47/50 RapTime: 0:00:07.493215 FixedProfit: 1337310\n",
            "Episode: 47/50 RapTime: 0:00:07.528751 FixedProfit: 1207356\n",
            "Episode: 47/50 RapTime: 0:00:07.503636 FixedProfit: 1173603\n",
            "Episode: 48/50 RapTime: 0:00:07.533550 FixedProfit: 1197165\n",
            "Episode: 48/50 RapTime: 0:00:07.525934 FixedProfit: 1140448\n",
            "Episode: 48/50 RapTime: 0:00:07.538072 FixedProfit: 1027389\n",
            "Episode: 48/50 RapTime: 0:00:07.561330 FixedProfit: 985203\n",
            "Episode: 49/50 RapTime: 0:00:07.565078 FixedProfit: 1197165\n",
            "Episode: 49/50 RapTime: 0:00:07.530485 FixedProfit: 1027500\n",
            "Episode: 49/50 RapTime: 0:00:07.605560 FixedProfit: 1255756\n",
            "Episode: 49/50 RapTime: 0:00:07.483470 FixedProfit: 1096252\n",
            "Episode: 50/50 RapTime: 0:00:07.576815 FixedProfit: 1197165\n",
            "Episode: 50/50 RapTime: 0:00:07.379045 FixedProfit: 1037636\n",
            "Episode: 50/50 RapTime: 0:00:07.371984 FixedProfit: 970241\n",
            "Episode: 50/50 RapTime: 0:00:07.228302 FixedProfit: 1059952\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}