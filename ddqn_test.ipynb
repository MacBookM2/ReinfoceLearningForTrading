{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddqn_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMpPbkVbrmRqEgSEnBH3+Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/ddqn_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "a5c9fa76-57a7-4f49-ee69-235a364b49b4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "mode = 'test'\n",
        "name = 'ddqn'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        model1 = Sequential()\n",
        "        model1.add(Dense(3, input_shape=(3,)))\n",
        "        model1.add(ReLU()) \n",
        "        model1.add(Dense(3))\n",
        "        model1.add(ReLU()) \n",
        "        model1.add(Dense(3))\n",
        "        model1.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        model1.summary()\n",
        "        self.model1 = model1\n",
        "\n",
        "        model2 = Sequential()\n",
        "        model2.add(Dense(3, input_shape=(3,)))\n",
        "        model2.add(ReLU()) \n",
        "        model2.add(Dense(3))\n",
        "        model2.add(ReLU()) \n",
        "        model2.add(Dense(3))\n",
        "        model2.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        model2.summary()\n",
        "        self.model2 = model2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent(Brain, ReplayMemory):\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        Brain.__init__(self)\n",
        "        ReplayMemory.__init__(self, max_size, batch_size)\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state,s_flag=12):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.predict(state,s_flag)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def predict(self, state, s_flag = 12):\n",
        "        values = None\n",
        "        q1 = self.model1.predict(state)\n",
        "        q2 = self.model2.predict(state)\n",
        "        if s_flag == 12:\n",
        "            values = np.array([q1[0,a] + q2[0,a] for a in range(2)])\n",
        "        elif s_flag == 11:\n",
        "            values = np.array([q1[0,a] + q1[0,a] for a in range(2)])\n",
        "        else:\n",
        "            values = np.array([q2[0,a] + q2[0,a] for a in range(2)])\n",
        "        return values\n",
        "\n",
        "    def replay(self, s_flag):\n",
        "        if self.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "\n",
        "        '''\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "        target_full = self.model.predict(states)\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "        '''\n",
        "\n",
        "        next_act_values = self.model1.predict(next_states,s_flag)\n",
        "        next_action =np.argmax(next_act_values)\n",
        "\n",
        "        if s_flag == 11:\n",
        "            q = self.model1.predict(states)  \n",
        "            next_q = self.model2.predict(next_states)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, actions] = rewards + (1 - done) * self.gamma*np.max(next_q, axis=1)\n",
        "            self.model1.train_on_batch(states, target)\n",
        "        else:\n",
        "            q = self.model2.predict(states)  \n",
        "            next_q = self.model1.predict(next_states)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, actions] = rewards + (1 - done) * self.gamma*np.max(next_q, axis=1)\n",
        "            self.model2.train_on_batch(states, target)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.model1.load_weights(name)\n",
        "        self.model2.load_weights(name2)\n",
        "\n",
        "    def save(self, name, name2):\n",
        "        self.model1.save_weights(name)\n",
        "        self.model2.save_weights(name2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                s_flag = 12\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    rand = np.random.random()\n",
        "                    if rand <= 0.5:\n",
        "                        s_flag = 11\n",
        "                    else:\n",
        "                        s_flag = 22\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                    self.agent.replay(s_flag)                \n",
        "            play_time = datetime.now() - start_time\n",
        "\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        self.agent.save('{}/{}_1.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "5dbca9d5-47ac-4fdf-9687-2d329229f3ae"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "agent = Agent(max_size, batch_size)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:55.989014 FixedProfit: 1538663 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 2/100 RapTime: 0:00:54.302740 FixedProfit: 1533242 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 3/100 RapTime: 0:00:54.647390 FixedProfit: 1533612 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 4/100 RapTime: 0:00:54.541043 FixedProfit: 1512020 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 5/100 RapTime: 0:00:54.651359 FixedProfit: 1558866 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 6/100 RapTime: 0:00:54.747872 FixedProfit: 1537927 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 7/100 RapTime: 0:00:54.613573 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/100 RapTime: 0:00:54.699290 FixedProfit: 1496870 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 9/100 RapTime: 0:00:54.531667 FixedProfit: 1544804 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 10/100 RapTime: 0:00:54.897593 FixedProfit: 1544772 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 11/100 RapTime: 0:00:54.781776 FixedProfit: 1596916 TradeTimes: 9 TradeWin: 8\n",
            "Episode: 12/100 RapTime: 0:00:54.603960 FixedProfit: 1541191 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 13/100 RapTime: 0:00:54.565047 FixedProfit: 1552655 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 14/100 RapTime: 0:00:54.210458 FixedProfit: 1534499 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 15/100 RapTime: 0:00:54.553264 FixedProfit: 1531754 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 16/100 RapTime: 0:00:54.414965 FixedProfit: 1538254 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 17/100 RapTime: 0:00:54.264356 FixedProfit: 1500802 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 18/100 RapTime: 0:00:54.495904 FixedProfit: 1530643 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 19/100 RapTime: 0:00:53.873196 FixedProfit: 1586517 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 20/100 RapTime: 0:00:54.018099 FixedProfit: 1518526 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 21/100 RapTime: 0:00:54.793164 FixedProfit: 1530487 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 22/100 RapTime: 0:00:54.450242 FixedProfit: 1528169 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 23/100 RapTime: 0:00:54.261824 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/100 RapTime: 0:00:53.892459 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/100 RapTime: 0:00:54.136929 FixedProfit: 1610342 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 26/100 RapTime: 0:00:54.112453 FixedProfit: 1552867 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 27/100 RapTime: 0:00:54.592761 FixedProfit: 1547809 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 28/100 RapTime: 0:00:54.350697 FixedProfit: 1542164 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 29/100 RapTime: 0:00:54.011460 FixedProfit: 1549366 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 30/100 RapTime: 0:00:53.281150 FixedProfit: 1541775 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 31/100 RapTime: 0:00:53.223743 FixedProfit: 1542721 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 32/100 RapTime: 0:00:53.271094 FixedProfit: 1545241 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 33/100 RapTime: 0:00:53.989424 FixedProfit: 1559170 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 34/100 RapTime: 0:00:53.787506 FixedProfit: 1544115 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 35/100 RapTime: 0:00:54.231088 FixedProfit: 1470617 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 36/100 RapTime: 0:00:52.953515 FixedProfit: 1535456 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 37/100 RapTime: 0:00:53.102573 FixedProfit: 1542069 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 38/100 RapTime: 0:00:53.341809 FixedProfit: 1549708 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 39/100 RapTime: 0:00:54.328364 FixedProfit: 1539174 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 40/100 RapTime: 0:00:54.509219 FixedProfit: 1552471 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 41/100 RapTime: 0:00:54.107672 FixedProfit: 1608119 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 42/100 RapTime: 0:00:54.880898 FixedProfit: 1535057 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 43/100 RapTime: 0:00:54.043548 FixedProfit: 1510850 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 44/100 RapTime: 0:00:54.407377 FixedProfit: 1510475 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 45/100 RapTime: 0:00:53.927247 FixedProfit: 1553568 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 46/100 RapTime: 0:00:54.020233 FixedProfit: 1574728 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 47/100 RapTime: 0:00:54.319426 FixedProfit: 1572541 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 48/100 RapTime: 0:00:53.575400 FixedProfit: 1618135 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 49/100 RapTime: 0:00:54.189500 FixedProfit: 1565011 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 50/100 RapTime: 0:00:54.267206 FixedProfit: 1541285 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 51/100 RapTime: 0:00:54.150732 FixedProfit: 1533683 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 52/100 RapTime: 0:00:54.208482 FixedProfit: 1399561 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 53/100 RapTime: 0:00:54.185562 FixedProfit: 1534106 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 54/100 RapTime: 0:00:54.301989 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 55/100 RapTime: 0:00:54.409593 FixedProfit: 1550898 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 56/100 RapTime: 0:00:54.192997 FixedProfit: 1534675 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 57/100 RapTime: 0:00:54.439373 FixedProfit: 1493198 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 58/100 RapTime: 0:00:54.506710 FixedProfit: 1540945 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 59/100 RapTime: 0:00:54.161178 FixedProfit: 1566610 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 60/100 RapTime: 0:00:54.156395 FixedProfit: 1540381 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 61/100 RapTime: 0:00:54.571670 FixedProfit: 1582798 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 62/100 RapTime: 0:00:54.497986 FixedProfit: 1530400 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 63/100 RapTime: 0:00:54.688430 FixedProfit: 1490551 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 64/100 RapTime: 0:00:54.644457 FixedProfit: 1552355 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 65/100 RapTime: 0:00:54.626177 FixedProfit: 1553377 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 66/100 RapTime: 0:00:55.129416 FixedProfit: 1537586 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 67/100 RapTime: 0:00:54.894154 FixedProfit: 1630401 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 68/100 RapTime: 0:00:54.911583 FixedProfit: 1548826 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 69/100 RapTime: 0:00:54.445286 FixedProfit: 1545563 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 70/100 RapTime: 0:00:54.305688 FixedProfit: 1523377 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 71/100 RapTime: 0:00:54.461530 FixedProfit: 1525423 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 72/100 RapTime: 0:00:54.640833 FixedProfit: 1511302 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 73/100 RapTime: 0:00:54.354706 FixedProfit: 1519969 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 74/100 RapTime: 0:00:54.332199 FixedProfit: 1680913 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 75/100 RapTime: 0:00:54.008074 FixedProfit: 1530952 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 76/100 RapTime: 0:00:54.314370 FixedProfit: 1617831 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 77/100 RapTime: 0:00:53.887382 FixedProfit: 1567997 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 78/100 RapTime: 0:00:55.739629 FixedProfit: 1525648 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 79/100 RapTime: 0:00:54.469342 FixedProfit: 1515203 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 80/100 RapTime: 0:00:54.258599 FixedProfit: 1537036 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 81/100 RapTime: 0:00:54.219567 FixedProfit: 1560655 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 82/100 RapTime: 0:00:53.550296 FixedProfit: 1563170 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 83/100 RapTime: 0:00:54.191522 FixedProfit: 1583776 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 84/100 RapTime: 0:00:54.348282 FixedProfit: 1541863 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 85/100 RapTime: 0:00:54.371881 FixedProfit: 1504135 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 86/100 RapTime: 0:00:53.905330 FixedProfit: 1552034 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 87/100 RapTime: 0:00:53.959619 FixedProfit: 1553620 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 88/100 RapTime: 0:00:53.368467 FixedProfit: 1545100 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 89/100 RapTime: 0:00:53.686171 FixedProfit: 1553178 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 90/100 RapTime: 0:00:53.958902 FixedProfit: 1492495 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 91/100 RapTime: 0:00:53.631298 FixedProfit: 1545202 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 92/100 RapTime: 0:00:53.494420 FixedProfit: 1583007 TradeTimes: 11 TradeWin: 9\n",
            "Episode: 93/100 RapTime: 0:00:53.887233 FixedProfit: 1688590 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 94/100 RapTime: 0:00:54.132693 FixedProfit: 1547702 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 95/100 RapTime: 0:00:53.779977 FixedProfit: 1533056 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 96/100 RapTime: 0:00:53.675528 FixedProfit: 1579948 TradeTimes: 7 TradeWin: 4\n",
            "Episode: 97/100 RapTime: 0:00:53.685461 FixedProfit: 1534403 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 98/100 RapTime: 0:00:54.111338 FixedProfit: 1547119 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 99/100 RapTime: 0:00:53.956197 FixedProfit: 1455230 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 100/100 RapTime: 0:00:54.596963 FixedProfit: 1529917 TradeTimes: 2 TradeWin: 2\n"
          ]
        }
      ]
    }
  ]
}