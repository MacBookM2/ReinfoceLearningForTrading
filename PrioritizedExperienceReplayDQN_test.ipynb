{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrioritizedExperienceReplayDQN_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMEvknD3JfJmZq7pBf+/bkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/PrioritizedExperienceReplayDQN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "32fef7fc-d6a3-4f4e-ae2f-0896911723eb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from statistics import mean\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "mode = 'test'\n",
        "name = 'per_dqn'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNPHarYI9zGY"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(3, input_shape=(3,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(3))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(3))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZLHqi3CpnxI"
      },
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.tderrors_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done, tderror):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.tderrors_memory[self.cntr] = tderror\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def findall(self):\n",
        "        return self.states_memory,self.next_states_memory,self.acts_memory,self.rewards_memory,self.done_memory,self.tderrors_memory\n",
        "\n",
        "    def update_memory_tderror(self, val):\n",
        "        self.tderrors_memory = val"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, model, memory, max_size=500, batch_size=32):\n",
        "        self.model = model\n",
        "        self.memory = memory\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done, tderror):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done, tderror)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if self.memory.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.model.predict(states)\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def pioritized_experience_replay(self):\n",
        "\n",
        "        sum_ab_tderror = self._absolute_tderror()\n",
        "        td_list = np.random.uniform(0, sum_ab_tderror, self.batch_size)\n",
        "        td_list = np.sort(td_list)\n",
        "\n",
        "        num_np = np.array([], dtype=np.int16)\n",
        "        i, sum_ab_tderror_tmp = 0, 0\n",
        "        states, next_states, actions, rewards, done, tderror = self.memory.findall()\n",
        "        for item in td_list:\n",
        "            while sum_ab_tderror_tmp < item:\n",
        "                sum_ab_tderror_tmp += abs(tderror[i]) + 0.0001\n",
        "                i += 1\n",
        "            num_np = np.append(num_np, i)\n",
        "\n",
        "        states = states[num_np]\n",
        "        next_states = next_states[num_np]\n",
        "        actions = actions[num_np]\n",
        "        rewards = rewards[num_np]\n",
        "        done = done[num_np]\n",
        "\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "        target_full = self.model.predict(states)\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def tderror(self, state, action, reward, next_state, done):\n",
        "        next_action = np.argmax(self.model.predict(next_state)[0])\n",
        "        target = reward + self.gamma * self.model.predict(next_state)[0][next_action]\n",
        "        tderror = target - self.model.predict(state)[0][action]\n",
        "        return tderror\n",
        "\n",
        "    def _absolute_tderror(self):\n",
        "        absolute_tderror = 0\n",
        "        tderror = self.memory.tderrors_memory\n",
        "        for i in range(0, (len(tderror)-1)):\n",
        "            absolute_tderror += abs(tderror[i]) + 0.0001\n",
        "        return absolute_tderror\n",
        "\n",
        "    def update_tderror(self):\n",
        "        states, next_states, acts, rewards, done, tderror = self.memory.findall()\n",
        "        next_action = np.argmax(self.model.predict(next_states)[0])\n",
        "        target = rewards + self.gamma * self.model.predict(next_states)[0][next_action]\n",
        "        TDerror = target - self.model.predict(next_states)[0][acts]\n",
        "        self.memory.update_memory_tderror(TDerror)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        total_reward = [0]\n",
        "        #total_reward = [1000000]\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    tderror = self.agent.tderror(state, action, reward, next_state, done)\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done, tderror)\n",
        "\n",
        "                    if mean(total_reward) < 1050000:\n",
        "                        self.agent.replay()\n",
        "                    else:\n",
        "                        self.agent.pioritized_experience_replay()\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                self.agent.update_tderror()\n",
        "                total_reward.append(info['cur_revenue'])\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.agent.save('{}/{}.h5'.format(self.mdl_dir, self.name))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "6add13cf-e193-4f5b-b8d2-614eabca16d2"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "\n",
        "brain = Brain()\n",
        "model = brain.model\n",
        "memory = Memory()\n",
        "agent = Agent(model, memory, max_size, batch_size)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:37.722056 FixedProfit: 1529722 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 2/100 RapTime: 0:00:36.043996 FixedProfit: 1570099 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 3/100 RapTime: 0:00:36.480921 FixedProfit: 1534624 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 4/100 RapTime: 0:00:36.053469 FixedProfit: 1535869 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 5/100 RapTime: 0:00:36.574552 FixedProfit: 1541988 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 6/100 RapTime: 0:00:35.798746 FixedProfit: 1525713 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 7/100 RapTime: 0:00:36.374098 FixedProfit: 1572880 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 8/100 RapTime: 0:00:35.847864 FixedProfit: 1544978 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 9/100 RapTime: 0:00:35.726296 FixedProfit: 1525851 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 10/100 RapTime: 0:00:35.958926 FixedProfit: 1546425 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 11/100 RapTime: 0:00:36.280309 FixedProfit: 1515032 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 12/100 RapTime: 0:00:35.501123 FixedProfit: 1363509 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 13/100 RapTime: 0:00:36.028348 FixedProfit: 1549563 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 14/100 RapTime: 0:00:35.760197 FixedProfit: 1548461 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 15/100 RapTime: 0:00:35.693052 FixedProfit: 1531795 TradeTimes: 9 TradeWin: 7\n",
            "Episode: 16/100 RapTime: 0:00:35.518217 FixedProfit: 1537808 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 17/100 RapTime: 0:00:36.084828 FixedProfit: 1505097 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 18/100 RapTime: 0:00:35.478831 FixedProfit: 1541983 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 19/100 RapTime: 0:00:35.753331 FixedProfit: 1532507 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 20/100 RapTime: 0:00:35.654506 FixedProfit: 1564058 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 21/100 RapTime: 0:00:36.059092 FixedProfit: 1553253 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 22/100 RapTime: 0:00:35.470831 FixedProfit: 1539168 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 23/100 RapTime: 0:00:36.191267 FixedProfit: 1528085 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 24/100 RapTime: 0:00:36.230294 FixedProfit: 1491426 TradeTimes: 7 TradeWin: 7\n",
            "Episode: 25/100 RapTime: 0:00:36.130315 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 26/100 RapTime: 0:00:35.924729 FixedProfit: 1552015 TradeTimes: 7 TradeWin: 7\n",
            "Episode: 27/100 RapTime: 0:00:35.289286 FixedProfit: 1522941 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 28/100 RapTime: 0:00:35.787921 FixedProfit: 1533762 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 29/100 RapTime: 0:00:35.555052 FixedProfit: 1546174 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 30/100 RapTime: 0:00:35.336937 FixedProfit: 1564132 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 31/100 RapTime: 0:00:35.208214 FixedProfit: 1521694 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 32/100 RapTime: 0:00:35.250806 FixedProfit: 1530357 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 33/100 RapTime: 0:00:35.270856 FixedProfit: 1525405 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 34/100 RapTime: 0:00:34.970762 FixedProfit: 1523580 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 35/100 RapTime: 0:00:34.792935 FixedProfit: 1495656 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 36/100 RapTime: 0:00:35.011677 FixedProfit: 1525612 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 37/100 RapTime: 0:00:34.802214 FixedProfit: 1556821 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 38/100 RapTime: 0:00:34.995789 FixedProfit: 1540963 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 39/100 RapTime: 0:00:34.619763 FixedProfit: 1535346 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 40/100 RapTime: 0:00:34.937673 FixedProfit: 1525875 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 41/100 RapTime: 0:00:34.753590 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 42/100 RapTime: 0:00:35.021336 FixedProfit: 1524231 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 43/100 RapTime: 0:00:34.787217 FixedProfit: 1544875 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 44/100 RapTime: 0:00:34.961512 FixedProfit: 1543142 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 45/100 RapTime: 0:00:34.503974 FixedProfit: 1532976 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 46/100 RapTime: 0:00:35.227288 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 47/100 RapTime: 0:00:34.870474 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 48/100 RapTime: 0:00:35.380930 FixedProfit: 1567665 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 49/100 RapTime: 0:00:35.241872 FixedProfit: 1552227 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 50/100 RapTime: 0:00:35.651559 FixedProfit: 1526478 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 51/100 RapTime: 0:00:35.429235 FixedProfit: 1547577 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 52/100 RapTime: 0:00:35.327716 FixedProfit: 1537368 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 53/100 RapTime: 0:00:34.866461 FixedProfit: 1609745 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 54/100 RapTime: 0:00:35.444946 FixedProfit: 1533449 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 55/100 RapTime: 0:00:34.757034 FixedProfit: 1529487 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 56/100 RapTime: 0:00:35.171077 FixedProfit: 1596151 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 57/100 RapTime: 0:00:34.969397 FixedProfit: 1540935 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 58/100 RapTime: 0:00:35.117796 FixedProfit: 1502460 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 59/100 RapTime: 0:00:35.389930 FixedProfit: 1509983 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 60/100 RapTime: 0:00:34.939554 FixedProfit: 1562565 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 61/100 RapTime: 0:00:35.329667 FixedProfit: 1543022 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 62/100 RapTime: 0:00:35.080894 FixedProfit: 1562846 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 63/100 RapTime: 0:00:34.932681 FixedProfit: 1536828 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 64/100 RapTime: 0:00:34.578216 FixedProfit: 1530307 TradeTimes: 8 TradeWin: 7\n",
            "Episode: 65/100 RapTime: 0:00:35.401403 FixedProfit: 1606502 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 66/100 RapTime: 0:00:35.322267 FixedProfit: 1499119 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 67/100 RapTime: 0:00:35.912307 FixedProfit: 1542520 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 68/100 RapTime: 0:00:35.376898 FixedProfit: 1532489 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 69/100 RapTime: 0:00:35.680389 FixedProfit: 1526388 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 70/100 RapTime: 0:00:35.792638 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 71/100 RapTime: 0:00:35.958139 FixedProfit: 1501928 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 72/100 RapTime: 0:00:35.902832 FixedProfit: 1552304 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 73/100 RapTime: 0:00:35.891325 FixedProfit: 1532661 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 74/100 RapTime: 0:00:35.555329 FixedProfit: 1527362 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 75/100 RapTime: 0:00:35.832089 FixedProfit: 1541318 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 76/100 RapTime: 0:00:35.762040 FixedProfit: 1538849 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 77/100 RapTime: 0:00:35.959317 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 78/100 RapTime: 0:00:35.487069 FixedProfit: 1546996 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 79/100 RapTime: 0:00:36.053989 FixedProfit: 1559943 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 80/100 RapTime: 0:00:35.777921 FixedProfit: 1532885 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 81/100 RapTime: 0:00:35.819812 FixedProfit: 1541454 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 82/100 RapTime: 0:00:36.126514 FixedProfit: 1569531 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 83/100 RapTime: 0:00:36.028147 FixedProfit: 1576500 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 84/100 RapTime: 0:00:36.297762 FixedProfit: 1571123 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 85/100 RapTime: 0:00:36.151349 FixedProfit: 1522557 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 86/100 RapTime: 0:00:36.276399 FixedProfit: 1555601 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 87/100 RapTime: 0:00:35.917608 FixedProfit: 1437344 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 88/100 RapTime: 0:00:36.136401 FixedProfit: 1552490 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 89/100 RapTime: 0:00:36.016524 FixedProfit: 1529147 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 90/100 RapTime: 0:00:36.016974 FixedProfit: 1498764 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 91/100 RapTime: 0:00:36.052922 FixedProfit: 1537171 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 92/100 RapTime: 0:00:36.115606 FixedProfit: 1549152 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 93/100 RapTime: 0:00:36.020078 FixedProfit: 1492204 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 94/100 RapTime: 0:00:36.356360 FixedProfit: 1499183 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 95/100 RapTime: 0:00:35.980388 FixedProfit: 1527144 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 96/100 RapTime: 0:00:36.148379 FixedProfit: 1529930 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 97/100 RapTime: 0:00:35.724858 FixedProfit: 1578881 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 98/100 RapTime: 0:00:35.653519 FixedProfit: 1503738 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 99/100 RapTime: 0:00:35.962018 FixedProfit: 1546010 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 100/100 RapTime: 0:00:35.972033 FixedProfit: 1540206 TradeTimes: 3 TradeWin: 3\n"
          ]
        }
      ]
    }
  ]
}