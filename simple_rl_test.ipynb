{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_rl_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOHGF+FHxzI2I7SonQvI5XW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/simple_rl_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "c91cd216-763c-4a19-b559-372cde1cd8ec"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_test.csv'\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'sp500_train.csv'\n",
        "\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "rewards_folder = '/content/drive/My Drive/' + exp_dir + 'rl_rewards'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=1000, mode = 'test'):\n",
        "\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        prev_revenue = self._get_revenue()\n",
        "\n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        '''\n",
        "        0 = sell\n",
        "        1 = hold\n",
        "        2 = buy\n",
        "        売買ルール：\n",
        "        1.空売りは認めない\n",
        "        2.ポジションを持っている場合、追加注文を出せない。\n",
        "        3.トレーニング期間は最後のステップでポジションを全て売却する。\n",
        "        4.ポジションは全売り\n",
        "        '''\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "### The experience replay memory ###\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "    # obs_dim = 3\n",
        "    # act_dim = 3\n",
        "    # size = 500\n",
        "    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.acts_buf = np.zeros(size, dtype=np.uint8)\n",
        "    self.rews_buf = np.zeros(size, dtype=np.float32)\n",
        "    self.done_buf = np.zeros(size, dtype=np.uint8)\n",
        "    self.ptr, self.size, self.max_size = 0, 0, size\n",
        "    '''\n",
        "    self.obs1_buf.shape = (500, 3)\n",
        "    '''\n",
        "  def store(self, obs, act, rew, next_obs, done):\n",
        "    self.obs1_buf[self.ptr] = obs\n",
        "    self.obs2_buf[self.ptr] = next_obs\n",
        "    self.acts_buf[self.ptr] = act\n",
        "    self.rews_buf[self.ptr] = rew\n",
        "    self.done_buf[self.ptr] = done\n",
        "    self.ptr = (self.ptr+1) % self.max_size\n",
        "    self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "  def sample_batch(self, batch_size=32):\n",
        "    idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "    return dict(s=self.obs1_buf[idxs],\n",
        "                s2=self.obs2_buf[idxs],\n",
        "                a=self.acts_buf[idxs],\n",
        "                r=self.rews_buf[idxs],\n",
        "                d=self.done_buf[idxs])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, n_hidden_layers=1, hidden_dim=32):\n",
        "\n",
        "        n_mid = 3\n",
        "        n_state = 3\n",
        "        n_action = 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULBV5XpsbOjq"
      },
      "source": [
        "def get_scaler(env):\n",
        "  # return scikit-learn scaler object to scale the states\n",
        "  # Note: you could also populate the replay buffer here\n",
        "\n",
        "  states = []\n",
        "  for _ in range(env.df_total_steps):\n",
        "    action = np.random.choice(env.action_space)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    states.append(state)\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(states)\n",
        "  return scaler"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "  def __init__(self, state_size, action_size, brain):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = ReplayBuffer(state_size, action_size, size=500)\n",
        "    self.gamma = 0.95  # discount rate\n",
        "    self.epsilon = 1.0  # exploration rate\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.model = brain.model\n",
        "\n",
        "  def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "    self.memory.store(state, action, reward, next_state, done)\n",
        "\n",
        "  def act(self, state):\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return np.random.choice(self.action_size)\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "  def replay(self, batch_size=32):\n",
        "    # first check if replay buffer contains enough data\n",
        "    if self.memory.size < batch_size:\n",
        "      return\n",
        "\n",
        "    # sample a batch of data from the replay memory\n",
        "    minibatch = self.memory.sample_batch(batch_size)\n",
        "    states = minibatch['s']\n",
        "    actions = minibatch['a']\n",
        "    rewards = minibatch['r']\n",
        "    next_states = minibatch['s2']\n",
        "    done = minibatch['d']\n",
        "\n",
        "    # Calculate the tentative target: Q(s',a)\n",
        "    target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "\n",
        "    # target[done] = rewards[done]\n",
        "\n",
        "    target_full = self.model.predict(states)\n",
        "\n",
        "    '''\n",
        "    batch_size = 32\n",
        "    np.arange(batch_size)\n",
        "    array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
        "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
        "\n",
        "    actions:[0 1 2 1 1 2 2 0 0 1 0 2 2 0 1 1 0 0 1 1 0 0 2 2 2 0 1 1 2 2 1 1]\n",
        "    '''\n",
        "    \n",
        "    target_full[np.arange(batch_size), actions] = target\n",
        "\n",
        "    '''\n",
        "    Run one training step,ここでエラーが出ている。\n",
        "    Runs a single gradient update on a single batch of data.\n",
        "    1つのデータバッチに対して1回の勾配更新を実行します。\n",
        "    states = [2420. 15.27 8.40000001]\n",
        "\n",
        "    returns\n",
        "    スカラーの学習損失（モデルが単一の出力を持ち、メトリクスを持たない場合）、\n",
        "    またはスカラーのリスト（モデルが複数の出力やメトリクスを持つ場合）。\n",
        "    model.metrics_namesという属性は、スカラー出力の表示ラベルを与えます。\n",
        "\n",
        "    model.fitは1つ以上のエポックを学習します。model.train_on_batchは、その名の通り、1つのバッチのみを学習します。\n",
        "    具体的な例として、10枚の画像を使ってモデルを学習する場合を考えてみましょう。\n",
        "    model.fitは10枚の画像すべてで学習するので、勾配を5回更新することになります。\n",
        "    model.train_on_batchは、バッチでしかモデルを与えていないので、勾配の更新は1回です。\n",
        "    model.train_on_batchは、バッチでモデルを与えるだけなので、バッチサイズが2の場合は、2枚の画像を与えます。\n",
        "    model.fitがmodel.train_on_batchを呼び出していると仮定すると、model.train_on_batchは複数回、おそらくループで呼び出されることになります。\n",
        "\n",
        "    '''\n",
        "    self.model.train_on_batch(states, target_full)\n",
        "\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "def play_game(env, agent , episodes_times = 50, mode = 'test', batch_size = 32):\n",
        "    if mode == 'test':\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "    else:\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        done = False\n",
        "        start_time = datetime.now()\n",
        "       \n",
        "        while not done:\n",
        "            #乱数で1,2,3を出力\n",
        "            # action = agent.act(state)\n",
        "            # action = np.random.randint(3, size=1)[0]\n",
        "\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "\n",
        "            if mode == 'train':\n",
        "                agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                agent.replay(batch_size)\n",
        "            \n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f} TradeTimes: {info['trade_time']} TradeWin: {info['trade_win']}\")\n",
        "        else:\n",
        "            record = pd.Series(info['cur_revenue'], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f}\")\n",
        "\n",
        "        \n",
        "        state = next_state\n",
        "        df_rec = df_rec.append(record, ignore_index=True)\n",
        "    return df_rec"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "1bd695a9-529c-4c0f-c370-f4eb3825978b"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "mode = 'test'\n",
        "state_size = 3\n",
        "action_size = 3\n",
        "brain = Brain()\n",
        "agent = Agent(state_size, action_size, brain)\n",
        "# テストの場合は上書き処理\n",
        "if mode == 'test':\n",
        "  # then load the previous scaler\n",
        "  with open(f'{models_folder}/scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "  # make sure epsilon is not 1!\n",
        "  # no need to run multiple episodes if epsilon = 0, it's deterministic\n",
        "  agent.epsilon = 0.01\n",
        "\n",
        "  # load trained weights\n",
        "  agent.load(f'{models_folder}/dqn.h5')\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "scaler = get_scaler(env)\n",
        "df_rec = play_game(env, agent , episodes_times = episodes_times, mode = mode,batch_size = batch_size)\n",
        "'''\n",
        "print(df_rec.describe())\n",
        "if mode == 'test':\n",
        "    df_rec = df_rec.drop(['TradeTimes','TradeWin'], axis=1)\n",
        "df_rec.to_csv(csv_path)\n",
        "df_rec.plot()\n",
        "plt.show()\n",
        "'''\n",
        "# save the weights when we are done\n",
        "if mode == 'train':\n",
        "  # save the DQN\n",
        "  agent.save(f'{models_folder}/dqn.h5')\n",
        "\n",
        "    # save the scaler\n",
        "  with open(f'{models_folder}/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "df_rec.to_csv(csv_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/100 RapTime: 0:00:34.126828 FixedProfit: 1010732 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 2/100 RapTime: 0:00:34.481575 FixedProfit: 988659 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 3/100 RapTime: 0:00:34.444751 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 4/100 RapTime: 0:00:34.337043 FixedProfit: 1010802 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 5/100 RapTime: 0:00:34.470344 FixedProfit: 968153 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 6/100 RapTime: 0:00:34.317192 FixedProfit: 971664 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 7/100 RapTime: 0:00:34.154963 FixedProfit: 1008408 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 8/100 RapTime: 0:00:34.028449 FixedProfit: 990601 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 9/100 RapTime: 0:00:34.413385 FixedProfit: 998545 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 10/100 RapTime: 0:00:33.812557 FixedProfit: 949635 TradeTimes: 7 TradeWin: 4\n",
            "Episode: 11/100 RapTime: 0:00:34.607397 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 12/100 RapTime: 0:00:36.054663 FixedProfit: 1005654 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 13/100 RapTime: 0:00:36.317931 FixedProfit: 993370 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 14/100 RapTime: 0:00:35.582260 FixedProfit: 993394 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 15/100 RapTime: 0:00:35.007633 FixedProfit: 953434 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 16/100 RapTime: 0:00:35.420857 FixedProfit: 943679 TradeTimes: 7 TradeWin: 3\n",
            "Episode: 17/100 RapTime: 0:00:34.872170 FixedProfit: 993176 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 18/100 RapTime: 0:00:35.641397 FixedProfit: 985906 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 19/100 RapTime: 0:00:35.441204 FixedProfit: 1009158 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 20/100 RapTime: 0:00:35.237094 FixedProfit: 1008494 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/100 RapTime: 0:00:34.694232 FixedProfit: 1006103 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 22/100 RapTime: 0:00:34.583861 FixedProfit: 980009 TradeTimes: 9 TradeWin: 3\n",
            "Episode: 23/100 RapTime: 0:00:35.036022 FixedProfit: 987658 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 24/100 RapTime: 0:00:35.293879 FixedProfit: 988714 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 25/100 RapTime: 0:00:35.036616 FixedProfit: 979085 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 26/100 RapTime: 0:00:35.046329 FixedProfit: 1001670 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 27/100 RapTime: 0:00:34.788650 FixedProfit: 1018536 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 28/100 RapTime: 0:00:35.049593 FixedProfit: 1059942 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 29/100 RapTime: 0:00:34.822856 FixedProfit: 1000300 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 30/100 RapTime: 0:00:34.153166 FixedProfit: 997371 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 31/100 RapTime: 0:00:34.517195 FixedProfit: 1008240 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 32/100 RapTime: 0:00:34.498547 FixedProfit: 1069955 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 33/100 RapTime: 0:00:35.693497 FixedProfit: 1077241 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 34/100 RapTime: 0:00:36.605233 FixedProfit: 1022013 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 35/100 RapTime: 0:00:34.481471 FixedProfit: 1001266 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 36/100 RapTime: 0:00:34.459575 FixedProfit: 995274 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 37/100 RapTime: 0:00:34.274387 FixedProfit: 1007254 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 38/100 RapTime: 0:00:34.242398 FixedProfit: 1042118 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 39/100 RapTime: 0:00:34.172042 FixedProfit: 1002577 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 40/100 RapTime: 0:00:34.354782 FixedProfit: 1035142 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 41/100 RapTime: 0:00:34.109137 FixedProfit: 999798 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 42/100 RapTime: 0:00:34.363240 FixedProfit: 1004145 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 43/100 RapTime: 0:00:34.344636 FixedProfit: 1011422 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 44/100 RapTime: 0:00:34.300084 FixedProfit: 1019321 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 45/100 RapTime: 0:00:34.398568 FixedProfit: 959850 TradeTimes: 7 TradeWin: 2\n",
            "Episode: 46/100 RapTime: 0:00:34.253488 FixedProfit: 993106 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 47/100 RapTime: 0:00:34.533442 FixedProfit: 1006536 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 48/100 RapTime: 0:00:33.913113 FixedProfit: 986559 TradeTimes: 3 TradeWin: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8ccc2fbdd698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_money\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_money\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepisodes_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisodes_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m '''\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a14823a5f0d3>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(env, agent, episodes_times, mode, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# action = np.random.randint(3, size=1)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7e064d953cbd>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# returns action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1721\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 719\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3121\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}