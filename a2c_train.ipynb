{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a2c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "f279bd34-0398-4d67-9bfb-75d7875b5deb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'a2c_train.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m51Mu4xy9-Nj"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self,model,n_action = 3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.alpha = 0.5\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_rewards = self._discounted_rewards(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "\n",
        "            policy_loss = self._policy_loss(selected_action_probs, discounted_rewards, v)\n",
        "            value_loss = self._value_loss(discounted_rewards,v)         \n",
        "            policy_entropy = self._policy_entropy(discounted_rewards,v,selected_action_probs)\n",
        "\n",
        "            total_loss = -policy_loss + self.alpha * value_loss - self.beta * policy_entropy\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.model.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "    def _discounted_rewards(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_rewards = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_rewards.append(G)\n",
        "        discounted_rewards.reverse()\n",
        "        discounted_rewards = np.asarray(discounted_rewards).reshape((-1, 1))\n",
        "        discounted_rewards -= np.mean(discounted_rewards)\n",
        "        return discounted_rewards\n",
        "\n",
        "    def _policy_loss(self, selected_action_probs, discounted_rewards, v):\n",
        "        advantage = discounted_rewards - tf.stop_gradient(v)\n",
        "        return tf.math.log(selected_action_probs) * advantage\n",
        "\n",
        "    def _value_loss(self, discounted_rewards, v):\n",
        "        return tf.reduce_mean((discounted_rewards - v) ** 2, axis=1, keepdims=True)\n",
        "\n",
        "    def _policy_entropy(self, discounted_rewards, v, selected_action_probs):\n",
        "        return tf.reduce_sum(tf.math.log(selected_action_probs) * selected_action_probs, axis=1, keepdims=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "def play_game(env, actor,critic, scaler, episodes_times = 100, batch_size = 32, mode = 'train'):\n",
        "    if mode == 'test':\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "    else:\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    experiences = []\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        state = state.flatten()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        while not done:\n",
        "            \n",
        "            action = actor.policynetwork(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            next_state = next_state.flatten()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if mode == 'train':\n",
        "                experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                if len(experiences) == batch_size:\n",
        "                    critic.valuenetwork(experiences)\n",
        "                    experiences = []\n",
        "\n",
        "\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f} TradeTimes: {info['trade_time']} TradeWin: {info['trade_win']}\")\n",
        "        else:\n",
        "            record = pd.Series(info['cur_revenue'], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f}\")\n",
        "\n",
        "        state = next_state\n",
        "        df_rec = df_rec.append(record, ignore_index=True)\n",
        "\n",
        "    return df_rec"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f3bc12-f018-45ad-9651-2dc25d5107f0"
      },
      "source": [
        "initial_money=1000000\n",
        "mode = 'train'\n",
        "\n",
        "brain = Brain()\n",
        "model = brain.model\n",
        "\n",
        "actor = Actor(model)\n",
        "critic = Critic(model)\n",
        "\n",
        "if mode == 'test':\n",
        "    brain.load(f'{models_folder}/a2c_model.h5')\n",
        "\n",
        "env = Environment(df, initial_money = initial_money, mode=mode)\n",
        "scaler = make_scaler(env)\n",
        "\n",
        "df_rec = play_game(env, actor, critic, scaler, mode = mode)\n",
        "\n",
        "if mode == 'train':\n",
        "    brain.save(f'{models_folder}/a2c_model.h5')\n",
        "\n",
        "df_rec.to_csv(csv_path)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:01.576312 FixedProfit: 1244298\n",
            "Episode: 2/100 RapTime: 0:00:01.315341 FixedProfit: 1215251\n",
            "Episode: 3/100 RapTime: 0:00:01.282812 FixedProfit: 1034583\n",
            "Episode: 4/100 RapTime: 0:00:01.314612 FixedProfit: 1227636\n",
            "Episode: 5/100 RapTime: 0:00:01.303053 FixedProfit: 963297\n",
            "Episode: 6/100 RapTime: 0:00:01.302014 FixedProfit: 1097596\n",
            "Episode: 7/100 RapTime: 0:00:01.296191 FixedProfit: 1280120\n",
            "Episode: 8/100 RapTime: 0:00:01.301757 FixedProfit: 1214935\n",
            "Episode: 9/100 RapTime: 0:00:01.302002 FixedProfit: 1072259\n",
            "Episode: 10/100 RapTime: 0:00:01.282158 FixedProfit: 1106527\n",
            "Episode: 11/100 RapTime: 0:00:01.293329 FixedProfit: 1215213\n",
            "Episode: 12/100 RapTime: 0:00:01.292580 FixedProfit: 1183305\n",
            "Episode: 13/100 RapTime: 0:00:01.298437 FixedProfit: 1388032\n",
            "Episode: 14/100 RapTime: 0:00:01.262642 FixedProfit: 1100744\n",
            "Episode: 15/100 RapTime: 0:00:01.210213 FixedProfit: 1031384\n",
            "Episode: 16/100 RapTime: 0:00:01.236332 FixedProfit: 1107910\n",
            "Episode: 17/100 RapTime: 0:00:01.229067 FixedProfit: 1098149\n",
            "Episode: 18/100 RapTime: 0:00:01.218035 FixedProfit: 1057648\n",
            "Episode: 19/100 RapTime: 0:00:01.227108 FixedProfit: 908331\n",
            "Episode: 20/100 RapTime: 0:00:01.225406 FixedProfit: 1093126\n",
            "Episode: 21/100 RapTime: 0:00:01.241349 FixedProfit: 1120068\n",
            "Episode: 22/100 RapTime: 0:00:01.298439 FixedProfit: 1176062\n",
            "Episode: 23/100 RapTime: 0:00:01.279129 FixedProfit: 1223903\n",
            "Episode: 24/100 RapTime: 0:00:01.285068 FixedProfit: 1114909\n",
            "Episode: 25/100 RapTime: 0:00:01.280226 FixedProfit: 976106\n",
            "Episode: 26/100 RapTime: 0:00:01.304039 FixedProfit: 981821\n",
            "Episode: 27/100 RapTime: 0:00:01.297811 FixedProfit: 1314528\n",
            "Episode: 28/100 RapTime: 0:00:01.285341 FixedProfit: 1344676\n",
            "Episode: 29/100 RapTime: 0:00:01.287205 FixedProfit: 981719\n",
            "Episode: 30/100 RapTime: 0:00:01.284218 FixedProfit: 1274690\n",
            "Episode: 31/100 RapTime: 0:00:01.283708 FixedProfit: 1140311\n",
            "Episode: 32/100 RapTime: 0:00:01.308881 FixedProfit: 1103334\n",
            "Episode: 33/100 RapTime: 0:00:01.294397 FixedProfit: 1205309\n",
            "Episode: 34/100 RapTime: 0:00:01.302479 FixedProfit: 1159911\n",
            "Episode: 35/100 RapTime: 0:00:01.288125 FixedProfit: 1180506\n",
            "Episode: 36/100 RapTime: 0:00:01.298987 FixedProfit: 1118175\n",
            "Episode: 37/100 RapTime: 0:00:01.292437 FixedProfit: 1129741\n",
            "Episode: 38/100 RapTime: 0:00:01.318903 FixedProfit: 1094047\n",
            "Episode: 39/100 RapTime: 0:00:01.311195 FixedProfit: 1360220\n",
            "Episode: 40/100 RapTime: 0:00:01.264830 FixedProfit: 1039110\n",
            "Episode: 41/100 RapTime: 0:00:01.215885 FixedProfit: 1304997\n",
            "Episode: 42/100 RapTime: 0:00:01.218411 FixedProfit: 1158700\n",
            "Episode: 43/100 RapTime: 0:00:01.206959 FixedProfit: 996898\n",
            "Episode: 44/100 RapTime: 0:00:01.224839 FixedProfit: 1158712\n",
            "Episode: 45/100 RapTime: 0:00:01.220815 FixedProfit: 1225536\n",
            "Episode: 46/100 RapTime: 0:00:01.225737 FixedProfit: 979851\n",
            "Episode: 47/100 RapTime: 0:00:01.213582 FixedProfit: 1079184\n",
            "Episode: 48/100 RapTime: 0:00:01.219733 FixedProfit: 1039334\n",
            "Episode: 49/100 RapTime: 0:00:01.231029 FixedProfit: 945129\n",
            "Episode: 50/100 RapTime: 0:00:01.221010 FixedProfit: 1036142\n",
            "Episode: 51/100 RapTime: 0:00:01.228726 FixedProfit: 1026593\n",
            "Episode: 52/100 RapTime: 0:00:01.230641 FixedProfit: 1112987\n",
            "Episode: 53/100 RapTime: 0:00:01.225543 FixedProfit: 1375810\n",
            "Episode: 54/100 RapTime: 0:00:01.246795 FixedProfit: 1055828\n",
            "Episode: 55/100 RapTime: 0:00:01.217342 FixedProfit: 1043848\n",
            "Episode: 56/100 RapTime: 0:00:01.231121 FixedProfit: 1062769\n",
            "Episode: 57/100 RapTime: 0:00:01.239621 FixedProfit: 1260942\n",
            "Episode: 58/100 RapTime: 0:00:01.261638 FixedProfit: 1130674\n",
            "Episode: 59/100 RapTime: 0:00:01.267356 FixedProfit: 1086549\n",
            "Episode: 60/100 RapTime: 0:00:01.239776 FixedProfit: 1468140\n",
            "Episode: 61/100 RapTime: 0:00:01.203682 FixedProfit: 1097553\n",
            "Episode: 62/100 RapTime: 0:00:01.226924 FixedProfit: 1218614\n",
            "Episode: 63/100 RapTime: 0:00:01.221995 FixedProfit: 1088739\n",
            "Episode: 64/100 RapTime: 0:00:01.229077 FixedProfit: 1110238\n",
            "Episode: 65/100 RapTime: 0:00:01.215596 FixedProfit: 974416\n",
            "Episode: 66/100 RapTime: 0:00:01.232948 FixedProfit: 1173233\n",
            "Episode: 67/100 RapTime: 0:00:01.238875 FixedProfit: 1039874\n",
            "Episode: 68/100 RapTime: 0:00:01.222412 FixedProfit: 1111262\n",
            "Episode: 69/100 RapTime: 0:00:01.214539 FixedProfit: 998340\n",
            "Episode: 70/100 RapTime: 0:00:01.244147 FixedProfit: 1237221\n",
            "Episode: 71/100 RapTime: 0:00:01.273552 FixedProfit: 1203633\n",
            "Episode: 72/100 RapTime: 0:00:01.238135 FixedProfit: 1336772\n",
            "Episode: 73/100 RapTime: 0:00:01.253661 FixedProfit: 1174211\n",
            "Episode: 74/100 RapTime: 0:00:01.315967 FixedProfit: 1038057\n",
            "Episode: 75/100 RapTime: 0:00:01.353051 FixedProfit: 1280095\n",
            "Episode: 76/100 RapTime: 0:00:01.312515 FixedProfit: 1040987\n",
            "Episode: 77/100 RapTime: 0:00:01.350426 FixedProfit: 1166151\n",
            "Episode: 78/100 RapTime: 0:00:01.301563 FixedProfit: 1159136\n",
            "Episode: 79/100 RapTime: 0:00:01.296649 FixedProfit: 1175889\n",
            "Episode: 80/100 RapTime: 0:00:01.304847 FixedProfit: 947385\n",
            "Episode: 81/100 RapTime: 0:00:01.311494 FixedProfit: 1125274\n",
            "Episode: 82/100 RapTime: 0:00:01.283815 FixedProfit: 1118006\n",
            "Episode: 83/100 RapTime: 0:00:01.268400 FixedProfit: 1096405\n",
            "Episode: 84/100 RapTime: 0:00:01.302667 FixedProfit: 1127432\n",
            "Episode: 85/100 RapTime: 0:00:01.295863 FixedProfit: 1059103\n",
            "Episode: 86/100 RapTime: 0:00:01.291575 FixedProfit: 1030844\n",
            "Episode: 87/100 RapTime: 0:00:01.290931 FixedProfit: 1141108\n",
            "Episode: 88/100 RapTime: 0:00:01.283797 FixedProfit: 1311655\n",
            "Episode: 89/100 RapTime: 0:00:01.285130 FixedProfit: 1177448\n",
            "Episode: 90/100 RapTime: 0:00:01.309104 FixedProfit: 969450\n",
            "Episode: 91/100 RapTime: 0:00:01.308505 FixedProfit: 1356777\n",
            "Episode: 92/100 RapTime: 0:00:01.298081 FixedProfit: 1099974\n",
            "Episode: 93/100 RapTime: 0:00:01.306323 FixedProfit: 1104630\n",
            "Episode: 94/100 RapTime: 0:00:01.259163 FixedProfit: 1160912\n",
            "Episode: 95/100 RapTime: 0:00:01.213284 FixedProfit: 1173196\n",
            "Episode: 96/100 RapTime: 0:00:01.228689 FixedProfit: 1213311\n",
            "Episode: 97/100 RapTime: 0:00:01.223579 FixedProfit: 1344730\n",
            "Episode: 98/100 RapTime: 0:00:01.221497 FixedProfit: 1052784\n",
            "Episode: 99/100 RapTime: 0:00:01.217890 FixedProfit: 1155817\n",
            "Episode: 100/100 RapTime: 0:00:01.232016 FixedProfit: 1006237\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}