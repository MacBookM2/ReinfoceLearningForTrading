{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gorila_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMBq2XtwnZ7qFgATqXREjWD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/gorila_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "f7b318a6-1a78-4342-e665-8de0626ce1a1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "mode = 'test'\n",
        "name = 'gorila'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=1000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.now_step        = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1RNmtkaZ2W"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        mastermodel = Sequential()\n",
        "        mastermodel.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_mid))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_action))\n",
        "        mastermodel.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((mastermodel.summary()))\n",
        "        self.mastermodel = mastermodel\n",
        "    \n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "        self.masterbrain = masterbrain\n",
        "\n",
        "    def load(self, name):\n",
        "        self.masterbrain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.masterbrain.save(name)\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def train_on_batch(self, state, target_full):\n",
        "        self.model.train_on_batch(state, target_full)\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w1_BMH7hLQ8"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size3 = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size3, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size3, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size3, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size3, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size3, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size3\n",
        "        self.size = min(self.size+1, self.max_size3)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrEa4wpGG1DF"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, memory, max_size, batch_size = 32):\n",
        "\n",
        "        self.brain   = brain\n",
        "        self.memory  = memory\n",
        "        self.epsilon = 1.0\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.brain.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42NIN-PGBOc8"
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, brain, memory, batch_size = 32):\n",
        "\n",
        "        self.brain  = brain\n",
        "        self.memory = memory\n",
        "\n",
        "        self.gamma       = 0.95\n",
        "        self.epsilon     = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r           = 0.995\n",
        "        self.batch_size  = batch_size\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.brain.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.brain.predict(states)\n",
        "\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.brain.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, learner, num, mdl_dir, name, episodes_times = 1000, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.actor          = actor\n",
        "        self.learner        = learner\n",
        "        self.num            = str(num)\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "        self.actor.layering()\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.actor.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.actor.store_transition(state, action, reward, next_state, done)\n",
        "                    self.learner.learn()\n",
        "                \n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                learner.integration()\n",
        "                actor.layering()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name, self.num))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name, self.num))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "cf4de76f-5348-4a62-ee6b-0ee26fff8657"
      },
      "source": [
        "initial_money  = 1000000\n",
        "episodes_times = 25\n",
        "batch_size     = 32\n",
        "max_size       = 500\n",
        "\n",
        "masterbrain    = ParameterServer()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env     = Environment(df, initial_money=initial_money, mode = mode)\n",
        "    brain   = Brain(masterbrain)\n",
        "    memory  = ReplayMemory(max_size, batch_size)\n",
        "    actor   = Actor(brain, memory, max_size, batch_size)\n",
        "    learner = Learner(brain, memory, batch_size)\n",
        "    main    = Main(env, actor, learner, i, mdl_dir, name, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_5 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_6 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_7 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_8 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_9 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/25 RapTime: 0:01:51.204688 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 1/25 RapTime: 0:01:51.691056 FixedProfit: 842989 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 1/25 RapTime: 0:01:52.076312 FixedProfit: 1119871 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 1/25 RapTime: 0:01:55.359501 FixedProfit: 1147937 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 2/25 RapTime: 0:01:49.954893 FixedProfit: 1141175 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 2/25 RapTime: 0:01:51.053915 FixedProfit: 1379335 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 2/25 RapTime: 0:01:53.723404 FixedProfit: 1158977 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 2/25 RapTime: 0:01:52.160097 FixedProfit: 1290752 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:01:51.500913 FixedProfit: 1428070 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 3/25 RapTime: 0:01:53.554013 FixedProfit: 1334662 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 3/25 RapTime: 0:01:51.804850 FixedProfit: 999953 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 3/25 RapTime: 0:01:54.914927 FixedProfit: 1283803 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 4/25 RapTime: 0:01:52.862295 FixedProfit: 1082918 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:01:52.860146 FixedProfit: 1526316 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:01:54.486248 FixedProfit: 1375102 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 4/25 RapTime: 0:01:54.658567 FixedProfit: 1079946 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:01:54.291790 FixedProfit: 1028656 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:01:54.375635 FixedProfit: 1053475 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:01:52.323120 FixedProfit: 1219401 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 5/25 RapTime: 0:01:52.940099 FixedProfit: 1037510 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 6/25 RapTime: 0:01:55.162683 FixedProfit: 1409517 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:01:54.599506 FixedProfit: 1064926 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 6/25 RapTime: 0:01:55.312898 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 6/25 RapTime: 0:01:56.936106 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 7/25 RapTime: 0:01:53.565520 FixedProfit: 1300145 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 7/25 RapTime: 0:01:54.545439 FixedProfit: 1159593 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 7/25 RapTime: 0:01:54.789269 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 7/25 RapTime: 0:01:54.869743 FixedProfit: 1102476 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 8/25 RapTime: 0:01:54.201820 FixedProfit: 1201967 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 8/25 RapTime: 0:01:54.804170 FixedProfit: 1163701 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 8/25 RapTime: 0:01:55.122673 FixedProfit: 1379115 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 8/25 RapTime: 0:01:57.931509 FixedProfit: 1236957 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:01:53.048001 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 9/25 RapTime: 0:01:57.796954 FixedProfit: 1113261 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 9/25 RapTime: 0:01:54.667292 FixedProfit: 1126464 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 9/25 RapTime: 0:01:57.534682 FixedProfit: 1011862 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:01:55.779184 FixedProfit: 966488 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 10/25 RapTime: 0:01:55.711071 FixedProfit: 1262461 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:01:56.877388 FixedProfit: 954185 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:01:56.328102 FixedProfit: 1409924 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 11/25 RapTime: 0:01:55.847301 FixedProfit: 1218781 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 11/25 RapTime: 0:01:55.893539 FixedProfit: 1367349 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 11/25 RapTime: 0:01:58.268289 FixedProfit: 1380836 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 11/25 RapTime: 0:01:56.247644 FixedProfit: 1399645 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:01:59.781684 FixedProfit: 1222962 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 12/25 RapTime: 0:01:58.273800 FixedProfit: 1019954 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:01:56.095323 FixedProfit: 1063875 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:01:56.211676 FixedProfit: 1284558 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 13/25 RapTime: 0:01:56.732577 FixedProfit: 1784854 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:01:58.561623 FixedProfit: 1224399 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:01:57.645730 FixedProfit: 1275720 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:01:55.660352 FixedProfit: 1284560 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 14/25 RapTime: 0:01:56.183763 FixedProfit: 1194015 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:01:56.621246 FixedProfit: 1417015 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 14/25 RapTime: 0:01:58.849559 FixedProfit: 1026522 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:01:57.793826 FixedProfit: 1296207 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:02:00.111678 FixedProfit: 1236860 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 15/25 RapTime: 0:01:59.279347 FixedProfit: 973782 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 15/25 RapTime: 0:01:58.656361 FixedProfit: 990211 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 15/25 RapTime: 0:01:57.794640 FixedProfit: 1601838 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 16/25 RapTime: 0:01:59.611390 FixedProfit: 1428070 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 16/25 RapTime: 0:01:58.901784 FixedProfit: 913871 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 16/25 RapTime: 0:02:00.578673 FixedProfit: 1177815 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:01:59.467122 FixedProfit: 1292569 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 17/25 RapTime: 0:01:59.245798 FixedProfit: 967876 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 17/25 RapTime: 0:01:58.709796 FixedProfit: 1228849 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:02:01.242458 FixedProfit: 1302349 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 17/25 RapTime: 0:02:00.145205 FixedProfit: 1132590 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 18/25 RapTime: 0:02:02.878750 FixedProfit: 1300566 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:02:00.258567 FixedProfit: 1258808 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:02:01.494850 FixedProfit: 1174268 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 18/25 RapTime: 0:02:03.116769 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 19/25 RapTime: 0:02:00.577858 FixedProfit: 1162704 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:01:59.023419 FixedProfit: 1162892 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:02:01.953149 FixedProfit: 1211350 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 19/25 RapTime: 0:02:03.589090 FixedProfit: 1163751 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:02:02.552959 FixedProfit: 1120388 TradeTimes: 1 TradeWin: 0\n",
            "Episode: 20/25 RapTime: 0:02:02.629148 FixedProfit: 1100562 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 20/25 RapTime: 0:02:03.042736 FixedProfit: 1000000 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 20/25 RapTime: 0:02:01.977828 FixedProfit: 1201358 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:02:02.639965 FixedProfit: 1319477 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:02:04.446141 FixedProfit: 1143980 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 21/25 RapTime: 0:02:01.924785 FixedProfit: 1402243 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 21/25 RapTime: 0:02:01.609045 FixedProfit: 1463334 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 22/25 RapTime: 0:02:02.431629 FixedProfit: 1191440 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:02:01.661191 FixedProfit: 1125993 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 22/25 RapTime: 0:02:03.373223 FixedProfit: 1516661 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:02:06.264999 FixedProfit: 1136143 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 23/25 RapTime: 0:02:05.700086 FixedProfit: 1455576 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 23/25 RapTime: 0:02:04.054986 FixedProfit: 1055787 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 23/25 RapTime: 0:02:05.034261 FixedProfit: 1272296 TradeTimes: 0 TradeWin: 0\n",
            "Episode: 23/25 RapTime: 0:02:04.912089 FixedProfit: 1152678 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:02:05.399003 FixedProfit: 1383217 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 24/25 RapTime: 0:02:02.954848 FixedProfit: 1225100 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 24/25 RapTime: 0:02:04.101274 FixedProfit: 1388002 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:02:04.143440 FixedProfit: 1361149 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 25/25 RapTime: 0:02:02.587358 FixedProfit: 1510145 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:02:02.415643 FixedProfit: 1094516 TradeTimes: 2 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:01:58.250315 FixedProfit: 855023 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:01:52.612571 FixedProfit: 1485253 TradeTimes: 1 TradeWin: 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}