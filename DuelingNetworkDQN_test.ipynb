{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DuelingNetworkDQN_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPgwgs2mJOIh+OoCNftL/6C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/DuelingNetworkDQN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "4f1d3d7f-1d37-43d6-9dad-57c974fbaa15"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU, Conv1D, Input, Lambda\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "mode = 'test'\n",
        "name = 'dn_dqn'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evsq8JqfWNoj"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        learning_rate = 0.00001\n",
        "        neurons_per_layer = 24\n",
        "\n",
        "        input = Input(shape=(3,))\n",
        "        common = Dense(neurons_per_layer*2, activation='relu')(input)\n",
        "        common = Dense(neurons_per_layer*4, activation='relu')(common)\n",
        "\n",
        "        common = Dense(4, activation='linear')(common)\n",
        "        output = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - 0.0*K.mean(a[:, 1:], keepdims=True),output_shape=(3,))(common)\n",
        "\n",
        "        model = keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "        model.compile(loss=Huber(), optimizer=optimizer)\n",
        "        model.summary()\n",
        "        self.model = model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent(Brain, ReplayMemory):\n",
        "    def __init__(self, max_size=500, batch_size=32):\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "        self.batch_size = batch_size\n",
        "        Brain.__init__(self)\n",
        "        ReplayMemory.__init__(self, max_size, batch_size)\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if self.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.model.predict(states)\n",
        "\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.model.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 200, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.agent          = agent\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "                    self.agent.replay()                \n",
        "            play_time = datetime.now() - start_time\n",
        "            if self.mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.agent.save('{}/{}.h5'.format(self.mdl_dir, self.name))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "3d2c1334-8b7b-41eb-edb0-63d5c5f184c5"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "max_size = 500\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "agent = Agent(max_size, batch_size)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 48)                192       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 96)                4704      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 388       \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 5,284\n",
            "Trainable params: 5,284\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:26.570543 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/100 RapTime: 0:00:26.047868 FixedProfit: 1009238 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 3/100 RapTime: 0:00:26.330754 FixedProfit: 985224 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 4/100 RapTime: 0:00:26.386021 FixedProfit: 993949 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 5/100 RapTime: 0:00:26.508839 FixedProfit: 1007357 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 6/100 RapTime: 0:00:26.315044 FixedProfit: 942227 TradeTimes: 6 TradeWin: 1\n",
            "Episode: 7/100 RapTime: 0:00:26.361852 FixedProfit: 980503 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 8/100 RapTime: 0:00:26.074190 FixedProfit: 996816 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 9/100 RapTime: 0:00:26.331359 FixedProfit: 980976 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 10/100 RapTime: 0:00:26.129128 FixedProfit: 949703 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 11/100 RapTime: 0:00:26.455938 FixedProfit: 1000489 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 12/100 RapTime: 0:00:26.246918 FixedProfit: 1004454 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 13/100 RapTime: 0:00:26.278927 FixedProfit: 1011098 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 14/100 RapTime: 0:00:26.394961 FixedProfit: 1001941 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 15/100 RapTime: 0:00:26.224399 FixedProfit: 1043499 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 16/100 RapTime: 0:00:26.468257 FixedProfit: 990880 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 17/100 RapTime: 0:00:26.022237 FixedProfit: 945940 TradeTimes: 5 TradeWin: 0\n",
            "Episode: 18/100 RapTime: 0:00:26.244677 FixedProfit: 1021412 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 19/100 RapTime: 0:00:26.095925 FixedProfit: 993329 TradeTimes: 8 TradeWin: 5\n",
            "Episode: 20/100 RapTime: 0:00:26.103816 FixedProfit: 1001705 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 21/100 RapTime: 0:00:26.215221 FixedProfit: 955170 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 22/100 RapTime: 0:00:26.265883 FixedProfit: 1008612 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 23/100 RapTime: 0:00:25.830837 FixedProfit: 1006559 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 24/100 RapTime: 0:00:26.196236 FixedProfit: 1005391 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 25/100 RapTime: 0:00:26.227061 FixedProfit: 986533 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 26/100 RapTime: 0:00:26.356955 FixedProfit: 997541 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 27/100 RapTime: 0:00:26.277054 FixedProfit: 1002387 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 28/100 RapTime: 0:00:26.634413 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 29/100 RapTime: 0:00:26.218536 FixedProfit: 998086 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 30/100 RapTime: 0:00:26.166559 FixedProfit: 996839 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 31/100 RapTime: 0:00:26.152067 FixedProfit: 1014323 TradeTimes: 6 TradeWin: 4\n",
            "Episode: 32/100 RapTime: 0:00:26.320582 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 33/100 RapTime: 0:00:26.305990 FixedProfit: 1002168 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 34/100 RapTime: 0:00:26.513101 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 35/100 RapTime: 0:00:26.344183 FixedProfit: 1009158 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 36/100 RapTime: 0:00:26.184559 FixedProfit: 996488 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 37/100 RapTime: 0:00:26.290146 FixedProfit: 1000243 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 38/100 RapTime: 0:00:26.459842 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 39/100 RapTime: 0:00:26.432328 FixedProfit: 1000072 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 40/100 RapTime: 0:00:26.437700 FixedProfit: 1001444 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 41/100 RapTime: 0:00:26.573152 FixedProfit: 1034773 TradeTimes: 6 TradeWin: 6\n",
            "Episode: 42/100 RapTime: 0:00:25.993166 FixedProfit: 969263 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 43/100 RapTime: 0:00:26.474646 FixedProfit: 998997 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 44/100 RapTime: 0:00:26.280717 FixedProfit: 1001102 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 45/100 RapTime: 0:00:26.283076 FixedProfit: 1033439 TradeTimes: 5 TradeWin: 5\n",
            "Episode: 46/100 RapTime: 0:00:26.247212 FixedProfit: 1015309 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 47/100 RapTime: 0:00:26.361433 FixedProfit: 1001063 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 48/100 RapTime: 0:00:26.041694 FixedProfit: 1016718 TradeTimes: 6 TradeWin: 5\n",
            "Episode: 49/100 RapTime: 0:00:26.513764 FixedProfit: 1001773 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 50/100 RapTime: 0:00:26.422678 FixedProfit: 968973 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 51/100 RapTime: 0:00:26.547767 FixedProfit: 999692 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 52/100 RapTime: 0:00:26.360317 FixedProfit: 1015718 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 53/100 RapTime: 0:00:26.386355 FixedProfit: 1005287 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 54/100 RapTime: 0:00:26.271981 FixedProfit: 1010483 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 55/100 RapTime: 0:00:26.247017 FixedProfit: 991401 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 56/100 RapTime: 0:00:26.455512 FixedProfit: 1012937 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 57/100 RapTime: 0:00:26.231088 FixedProfit: 1016620 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 58/100 RapTime: 0:00:26.358160 FixedProfit: 989778 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 59/100 RapTime: 0:00:26.310158 FixedProfit: 975119 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 60/100 RapTime: 0:00:26.216259 FixedProfit: 1019656 TradeTimes: 4 TradeWin: 3\n",
            "Episode: 61/100 RapTime: 0:00:26.154079 FixedProfit: 1065592 TradeTimes: 4 TradeWin: 4\n",
            "Episode: 62/100 RapTime: 0:00:26.313301 FixedProfit: 1075108 TradeTimes: 6 TradeWin: 2\n",
            "Episode: 63/100 RapTime: 0:00:26.267084 FixedProfit: 1000267 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 64/100 RapTime: 0:00:26.494032 FixedProfit: 986072 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 65/100 RapTime: 0:00:26.319241 FixedProfit: 1005033 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 66/100 RapTime: 0:00:26.412200 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 67/100 RapTime: 0:00:26.182075 FixedProfit: 1000532 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 68/100 RapTime: 0:00:26.436164 FixedProfit: 1000509 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 69/100 RapTime: 0:00:26.440691 FixedProfit: 1006731 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 70/100 RapTime: 0:00:26.120333 FixedProfit: 981765 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 71/100 RapTime: 0:00:26.495617 FixedProfit: 1022786 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 72/100 RapTime: 0:00:26.234781 FixedProfit: 991590 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 73/100 RapTime: 0:00:26.327275 FixedProfit: 949589 TradeTimes: 7 TradeWin: 4\n",
            "Episode: 74/100 RapTime: 0:00:26.343105 FixedProfit: 979844 TradeTimes: 4 TradeWin: 1\n",
            "Episode: 75/100 RapTime: 0:00:26.523629 FixedProfit: 1002593 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 76/100 RapTime: 0:00:26.148875 FixedProfit: 981956 TradeTimes: 3 TradeWin: 1\n",
            "Episode: 77/100 RapTime: 0:00:26.373206 FixedProfit: 936774 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 78/100 RapTime: 0:00:26.127456 FixedProfit: 1018375 TradeTimes: 7 TradeWin: 6\n",
            "Episode: 79/100 RapTime: 0:00:26.433282 FixedProfit: 987875 TradeTimes: 2 TradeWin: 0\n",
            "Episode: 80/100 RapTime: 0:00:26.261276 FixedProfit: 1000000 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 81/100 RapTime: 0:00:26.391987 FixedProfit: 1056974 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 82/100 RapTime: 0:00:26.045281 FixedProfit: 1012556 TradeTimes: 5 TradeWin: 4\n",
            "Episode: 83/100 RapTime: 0:00:26.214487 FixedProfit: 969382 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 84/100 RapTime: 0:00:26.500125 FixedProfit: 1007606 TradeTimes: 3 TradeWin: 3\n",
            "Episode: 85/100 RapTime: 0:00:26.035845 FixedProfit: 1012628 TradeTimes: 7 TradeWin: 5\n",
            "Episode: 86/100 RapTime: 0:00:26.309665 FixedProfit: 985523 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 87/100 RapTime: 0:00:26.532765 FixedProfit: 1000594 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 88/100 RapTime: 0:00:26.397162 FixedProfit: 1002417 TradeTimes: 4 TradeWin: 2\n",
            "Episode: 89/100 RapTime: 0:00:26.291172 FixedProfit: 1010923 TradeTimes: 6 TradeWin: 3\n",
            "Episode: 90/100 RapTime: 0:00:26.426681 FixedProfit: 991463 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 91/100 RapTime: 0:00:26.165696 FixedProfit: 995446 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 92/100 RapTime: 0:00:26.464223 FixedProfit: 967159 TradeTimes: 3 TradeWin: 0\n",
            "Episode: 93/100 RapTime: 0:00:26.327814 FixedProfit: 1010536 TradeTimes: 2 TradeWin: 2\n",
            "Episode: 94/100 RapTime: 0:00:26.209792 FixedProfit: 1018090 TradeTimes: 8 TradeWin: 6\n",
            "Episode: 95/100 RapTime: 0:00:26.201536 FixedProfit: 970201 TradeTimes: 8 TradeWin: 3\n",
            "Episode: 96/100 RapTime: 0:00:26.202134 FixedProfit: 996519 TradeTimes: 3 TradeWin: 2\n",
            "Episode: 97/100 RapTime: 0:00:26.065469 FixedProfit: 1023069 TradeTimes: 5 TradeWin: 2\n",
            "Episode: 98/100 RapTime: 0:00:26.164823 FixedProfit: 1001232 TradeTimes: 5 TradeWin: 3\n",
            "Episode: 99/100 RapTime: 0:00:26.541141 FixedProfit: 979426 TradeTimes: 5 TradeWin: 1\n",
            "Episode: 100/100 RapTime: 0:00:26.178472 FixedProfit: 1038436 TradeTimes: 7 TradeWin: 6\n"
          ]
        }
      ]
    }
  ]
}