{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sarsa_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM+CnE41zBRWnaS5cAXbbPJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/sarsa_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "8be6380c-6e6e-4485-e556-02e5cf7485c7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'sarsa_train.csv'\n",
        "\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "rewards_folder = '/content/drive/My Drive/' + exp_dir + 'rl_rewards'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd-UoFozKs9l"
      },
      "source": [
        "def make_scaler(env):\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, n_hidden_layers=1, hidden_dim=32):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "\n",
        "        n_mid = 3\n",
        "        n_state = 3\n",
        "        n_action = 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, state, action, reward, next_state, next_act, done):\n",
        "        q = self.model.predict(state)\n",
        "        #next_q = self.model.predict(next_state)\n",
        "        t = np.copy(q)\n",
        "        if done:\n",
        "            t[:, action] = reward\n",
        "        else:\n",
        "            t[:, action] = reward + self.gamma*next_act\n",
        "        self.model.train_on_batch(state, t)\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, brain, state_size=3, action_size=3):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.brain = brain\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "\n",
        "    def act(self, state, act, reward, info, next_state, done, mode = 'train'):\n",
        "\n",
        "        next_act = self._next_act(state)\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.brain.train(state, act, reward, next_state, next_act, done)\n",
        "\n",
        "        return next_act\n",
        "\n",
        "    def _next_act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_size)\n",
        "        act_values = self.brain.predict(state)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "def play_game(env, agent , episodes_times = 1000, mode = 'test', batch_size = 32):\n",
        "    if mode == 'test':\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "    else:\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        done = False\n",
        "        start_time = datetime.now()\n",
        "        reward = 0.0\n",
        "        info = None\n",
        "        next_state = copy.copy(state)\n",
        "        next_act = 1\n",
        "        act = 1\n",
        "       \n",
        "        while not done:\n",
        "            action = agent.act(state, act, reward, info, next_state, done, mode)\n",
        "            state = next_state\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            act = copy.copy(action)\n",
        "            \n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f} TradeTimes: {info['trade_time']} TradeWin: {info['trade_win']}\")\n",
        "        else:\n",
        "            record = pd.Series(info['cur_revenue'], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f}\")\n",
        "    \n",
        "        df_rec = df_rec.append(record, ignore_index=True)\n",
        "    return df_rec"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "fe1d0813-5d8b-40a7-e2d1-813c80059f15"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 100\n",
        "batch_size = 32\n",
        "mode = 'train'\n",
        "brain = Brain()\n",
        "agent = Agent(brain=brain)\n",
        "\n",
        "if mode == 'test':\n",
        "    with open(f'{models_folder}/scaler_sarsa.pkl', 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "    agent.epsilon = 0.01\n",
        "    agent.load(f'{models_folder}/dqn_sarsa.h5')\n",
        "\n",
        "env = Environment(df, initial_money=initial_money, mode = mode)\n",
        "scaler = make_scaler(env)\n",
        "df_rec = play_game(env, agent , episodes_times = episodes_times, mode = mode, batch_size = batch_size)\n",
        "\n",
        "if mode == 'train':\n",
        "    agent.save(f'{models_folder}/dqn_sarsa.h5')\n",
        "    with open(f'{models_folder}/scaler_sarsa.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "df_rec.to_csv(csv_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/100 RapTime: 0:00:28.350522 FixedProfit: 1028863\n",
            "Episode: 2/100 RapTime: 0:00:27.197432 FixedProfit: 1078174\n",
            "Episode: 3/100 RapTime: 0:00:26.604897 FixedProfit: 1160297\n",
            "Episode: 4/100 RapTime: 0:00:26.818866 FixedProfit: 1079961\n",
            "Episode: 5/100 RapTime: 0:00:26.367085 FixedProfit: 1063442\n",
            "Episode: 6/100 RapTime: 0:00:27.475329 FixedProfit: 1152278\n",
            "Episode: 7/100 RapTime: 0:00:27.457927 FixedProfit: 954641\n",
            "Episode: 8/100 RapTime: 0:00:27.258542 FixedProfit: 1001884\n",
            "Episode: 9/100 RapTime: 0:00:26.307287 FixedProfit: 1103396\n",
            "Episode: 10/100 RapTime: 0:00:26.874186 FixedProfit: 1235700\n",
            "Episode: 11/100 RapTime: 0:00:27.032952 FixedProfit: 1263366\n",
            "Episode: 12/100 RapTime: 0:00:27.647411 FixedProfit: 1146026\n",
            "Episode: 13/100 RapTime: 0:00:27.992174 FixedProfit: 1256532\n",
            "Episode: 14/100 RapTime: 0:00:27.601454 FixedProfit: 1239901\n",
            "Episode: 15/100 RapTime: 0:00:27.947449 FixedProfit: 1129412\n",
            "Episode: 16/100 RapTime: 0:00:28.160424 FixedProfit: 1310815\n",
            "Episode: 17/100 RapTime: 0:00:28.115450 FixedProfit: 983666\n",
            "Episode: 18/100 RapTime: 0:00:27.710496 FixedProfit: 996725\n",
            "Episode: 19/100 RapTime: 0:00:27.714346 FixedProfit: 936244\n",
            "Episode: 20/100 RapTime: 0:00:27.022594 FixedProfit: 796395\n",
            "Episode: 21/100 RapTime: 0:00:27.402470 FixedProfit: 1188797\n",
            "Episode: 22/100 RapTime: 0:00:27.085536 FixedProfit: 1041950\n",
            "Episode: 23/100 RapTime: 0:00:26.561286 FixedProfit: 1042574\n",
            "Episode: 24/100 RapTime: 0:00:26.540434 FixedProfit: 1002825\n",
            "Episode: 25/100 RapTime: 0:00:25.839125 FixedProfit: 969840\n",
            "Episode: 26/100 RapTime: 0:00:27.136122 FixedProfit: 1160622\n",
            "Episode: 27/100 RapTime: 0:00:26.856103 FixedProfit: 1425253\n",
            "Episode: 28/100 RapTime: 0:00:26.866321 FixedProfit: 975469\n",
            "Episode: 29/100 RapTime: 0:00:26.494582 FixedProfit: 937835\n",
            "Episode: 30/100 RapTime: 0:00:26.201395 FixedProfit: 1131283\n",
            "Episode: 31/100 RapTime: 0:00:27.020856 FixedProfit: 1047661\n",
            "Episode: 32/100 RapTime: 0:00:26.737588 FixedProfit: 1125438\n",
            "Episode: 33/100 RapTime: 0:00:26.425309 FixedProfit: 1039723\n",
            "Episode: 34/100 RapTime: 0:00:27.151428 FixedProfit: 1122651\n",
            "Episode: 35/100 RapTime: 0:00:27.101992 FixedProfit: 1380670\n",
            "Episode: 36/100 RapTime: 0:00:26.152567 FixedProfit: 1046354\n",
            "Episode: 37/100 RapTime: 0:00:27.369018 FixedProfit: 1117739\n",
            "Episode: 38/100 RapTime: 0:00:27.482434 FixedProfit: 924844\n",
            "Episode: 39/100 RapTime: 0:00:26.731524 FixedProfit: 1093503\n",
            "Episode: 40/100 RapTime: 0:00:26.773708 FixedProfit: 1053830\n",
            "Episode: 41/100 RapTime: 0:00:26.414329 FixedProfit: 1186015\n",
            "Episode: 42/100 RapTime: 0:00:27.273575 FixedProfit: 1088182\n",
            "Episode: 43/100 RapTime: 0:00:26.814299 FixedProfit: 1119162\n",
            "Episode: 44/100 RapTime: 0:00:26.759133 FixedProfit: 1049217\n",
            "Episode: 45/100 RapTime: 0:00:26.828850 FixedProfit: 898925\n",
            "Episode: 46/100 RapTime: 0:00:27.400535 FixedProfit: 1179830\n",
            "Episode: 47/100 RapTime: 0:00:26.327316 FixedProfit: 1008581\n",
            "Episode: 48/100 RapTime: 0:00:27.649338 FixedProfit: 895526\n",
            "Episode: 49/100 RapTime: 0:00:26.864871 FixedProfit: 995275\n",
            "Episode: 50/100 RapTime: 0:00:26.357963 FixedProfit: 1325389\n",
            "Episode: 51/100 RapTime: 0:00:27.224181 FixedProfit: 994798\n",
            "Episode: 52/100 RapTime: 0:00:27.353208 FixedProfit: 892788\n",
            "Episode: 53/100 RapTime: 0:00:28.063510 FixedProfit: 1137020\n",
            "Episode: 54/100 RapTime: 0:00:28.225435 FixedProfit: 1089631\n",
            "Episode: 55/100 RapTime: 0:00:27.579091 FixedProfit: 953007\n",
            "Episode: 56/100 RapTime: 0:00:27.413336 FixedProfit: 1135518\n",
            "Episode: 57/100 RapTime: 0:00:27.405415 FixedProfit: 1291207\n",
            "Episode: 58/100 RapTime: 0:00:27.680699 FixedProfit: 951190\n",
            "Episode: 59/100 RapTime: 0:00:27.803018 FixedProfit: 1046880\n",
            "Episode: 60/100 RapTime: 0:00:27.078527 FixedProfit: 1390547\n",
            "Episode: 61/100 RapTime: 0:00:27.213894 FixedProfit: 1390967\n",
            "Episode: 62/100 RapTime: 0:00:27.211846 FixedProfit: 1004775\n",
            "Episode: 63/100 RapTime: 0:00:26.843243 FixedProfit: 1028947\n",
            "Episode: 64/100 RapTime: 0:00:27.409983 FixedProfit: 1247724\n",
            "Episode: 65/100 RapTime: 0:00:26.582297 FixedProfit: 962916\n",
            "Episode: 66/100 RapTime: 0:00:26.318467 FixedProfit: 1027588\n",
            "Episode: 67/100 RapTime: 0:00:26.163851 FixedProfit: 1272737\n",
            "Episode: 68/100 RapTime: 0:00:26.061273 FixedProfit: 1126595\n",
            "Episode: 69/100 RapTime: 0:00:26.717188 FixedProfit: 1167345\n",
            "Episode: 70/100 RapTime: 0:00:26.644297 FixedProfit: 1113966\n",
            "Episode: 71/100 RapTime: 0:00:25.723979 FixedProfit: 1121935\n",
            "Episode: 72/100 RapTime: 0:00:26.106818 FixedProfit: 1012031\n",
            "Episode: 73/100 RapTime: 0:00:26.910941 FixedProfit: 891417\n",
            "Episode: 74/100 RapTime: 0:00:26.068447 FixedProfit: 1050486\n",
            "Episode: 75/100 RapTime: 0:00:26.245089 FixedProfit: 1175546\n",
            "Episode: 76/100 RapTime: 0:00:25.672467 FixedProfit: 1233968\n",
            "Episode: 77/100 RapTime: 0:00:25.914497 FixedProfit: 1173809\n",
            "Episode: 78/100 RapTime: 0:00:26.255113 FixedProfit: 1111790\n",
            "Episode: 79/100 RapTime: 0:00:26.430871 FixedProfit: 1075492\n",
            "Episode: 80/100 RapTime: 0:00:26.504326 FixedProfit: 1092485\n",
            "Episode: 81/100 RapTime: 0:00:26.324924 FixedProfit: 1108399\n",
            "Episode: 82/100 RapTime: 0:00:25.961517 FixedProfit: 1152139\n",
            "Episode: 83/100 RapTime: 0:00:25.474190 FixedProfit: 1121837\n",
            "Episode: 84/100 RapTime: 0:00:26.230729 FixedProfit: 1159062\n",
            "Episode: 85/100 RapTime: 0:00:25.867930 FixedProfit: 1364322\n",
            "Episode: 86/100 RapTime: 0:00:25.738437 FixedProfit: 1096802\n",
            "Episode: 87/100 RapTime: 0:00:26.180384 FixedProfit: 1129259\n",
            "Episode: 88/100 RapTime: 0:00:25.420155 FixedProfit: 1170596\n",
            "Episode: 89/100 RapTime: 0:00:26.663289 FixedProfit: 1004185\n",
            "Episode: 90/100 RapTime: 0:00:25.643754 FixedProfit: 1034255\n",
            "Episode: 91/100 RapTime: 0:00:26.128871 FixedProfit: 1173318\n",
            "Episode: 92/100 RapTime: 0:00:26.579100 FixedProfit: 1258192\n",
            "Episode: 93/100 RapTime: 0:00:27.162082 FixedProfit: 1072552\n",
            "Episode: 94/100 RapTime: 0:00:25.859796 FixedProfit: 1188576\n",
            "Episode: 95/100 RapTime: 0:00:26.160154 FixedProfit: 1306770\n",
            "Episode: 96/100 RapTime: 0:00:25.855310 FixedProfit: 1196741\n",
            "Episode: 97/100 RapTime: 0:00:25.444558 FixedProfit: 1062718\n",
            "Episode: 98/100 RapTime: 0:00:26.227134 FixedProfit: 1111875\n",
            "Episode: 99/100 RapTime: 0:00:24.764951 FixedProfit: 1158444\n",
            "Episode: 100/100 RapTime: 0:00:25.260318 FixedProfit: 891816\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}