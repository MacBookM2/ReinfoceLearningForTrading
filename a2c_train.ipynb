{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a2c_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "c3dc23b6-e2dc-4200-9e00-9dc4d4a5ed9c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_train.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'a2c_train.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m51Mu4xy9-Nj"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, model, n_action = 3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "def play_game(env, actor,critic, scaler, episodes_times = 100, batch_size = 32, mode = 'train'):\n",
        "    if mode == 'test':\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "    else:\n",
        "        df_rec = pd.DataFrame(index=[], columns=['FixedProfit'])\n",
        "\n",
        "    experiences = []\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        state = state.flatten()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        while not done:\n",
        "            \n",
        "            action = actor.policynetwork(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            next_state = next_state.flatten()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if mode == 'train':\n",
        "                experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                if len(experiences) == batch_size:\n",
        "                    critic.valuenetwork(experiences)\n",
        "                    experiences = []\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f} TradeTimes: {info['trade_time']} TradeWin: {info['trade_win']}\")\n",
        "        else:\n",
        "            record = pd.Series(info['cur_revenue'], index=df_rec.columns)\n",
        "            print(f\"Episode: {episode + 1}/{episodes_times} RapTime: {play_time} FixedProfit: {info['cur_revenue']:.0f}\")\n",
        "\n",
        "        df_rec = df_rec.append(record, ignore_index=True)\n",
        "\n",
        "    return df_rec"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a560496a-c838-4b87-9a58-22ab5849504f"
      },
      "source": [
        "initial_money=1000000\n",
        "mode = 'train'\n",
        "\n",
        "brain = Brain()\n",
        "model = brain.model\n",
        "\n",
        "actor = Actor(model)\n",
        "critic = Critic(model)\n",
        "\n",
        "if mode == 'test':\n",
        "    brain.load(f'{models_folder}/a2c_model.h5')\n",
        "\n",
        "env = Environment(df, initial_money = initial_money, mode=mode)\n",
        "scaler = make_scaler(env)\n",
        "\n",
        "df_rec = play_game(env, actor, critic, scaler, mode = mode)\n",
        "\n",
        "if mode == 'train':\n",
        "    brain.save(f'{models_folder}/a2c_model.h5')\n",
        "\n",
        "df_rec.to_csv(csv_path)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/100 RapTime: 0:00:01.453031 FixedProfit: 1136983\n",
            "Episode: 2/100 RapTime: 0:00:01.178060 FixedProfit: 1020843\n",
            "Episode: 3/100 RapTime: 0:00:01.171871 FixedProfit: 1121850\n",
            "Episode: 4/100 RapTime: 0:00:01.210268 FixedProfit: 1267588\n",
            "Episode: 5/100 RapTime: 0:00:01.192360 FixedProfit: 1160737\n",
            "Episode: 6/100 RapTime: 0:00:01.163037 FixedProfit: 1183321\n",
            "Episode: 7/100 RapTime: 0:00:01.183601 FixedProfit: 1197165\n",
            "Episode: 8/100 RapTime: 0:00:01.156933 FixedProfit: 1202190\n",
            "Episode: 9/100 RapTime: 0:00:01.159915 FixedProfit: 1208128\n",
            "Episode: 10/100 RapTime: 0:00:01.189997 FixedProfit: 1197165\n",
            "Episode: 11/100 RapTime: 0:00:01.199367 FixedProfit: 1197165\n",
            "Episode: 12/100 RapTime: 0:00:01.151007 FixedProfit: 1194143\n",
            "Episode: 13/100 RapTime: 0:00:01.209607 FixedProfit: 1208128\n",
            "Episode: 14/100 RapTime: 0:00:01.183402 FixedProfit: 1202990\n",
            "Episode: 15/100 RapTime: 0:00:01.153542 FixedProfit: 1208128\n",
            "Episode: 16/100 RapTime: 0:00:01.163708 FixedProfit: 1197165\n",
            "Episode: 17/100 RapTime: 0:00:01.170231 FixedProfit: 1194143\n",
            "Episode: 18/100 RapTime: 0:00:01.159504 FixedProfit: 1197165\n",
            "Episode: 19/100 RapTime: 0:00:01.165712 FixedProfit: 1208128\n",
            "Episode: 20/100 RapTime: 0:00:01.167385 FixedProfit: 1208128\n",
            "Episode: 21/100 RapTime: 0:00:01.184474 FixedProfit: 1208128\n",
            "Episode: 22/100 RapTime: 0:00:01.162361 FixedProfit: 1173059\n",
            "Episode: 23/100 RapTime: 0:00:01.167140 FixedProfit: 1194143\n",
            "Episode: 24/100 RapTime: 0:00:01.166562 FixedProfit: 1197165\n",
            "Episode: 25/100 RapTime: 0:00:01.164788 FixedProfit: 1194143\n",
            "Episode: 26/100 RapTime: 0:00:01.163506 FixedProfit: 1208128\n",
            "Episode: 27/100 RapTime: 0:00:01.167145 FixedProfit: 1194143\n",
            "Episode: 28/100 RapTime: 0:00:01.149181 FixedProfit: 1208128\n",
            "Episode: 29/100 RapTime: 0:00:01.162376 FixedProfit: 1197165\n",
            "Episode: 30/100 RapTime: 0:00:01.160699 FixedProfit: 1194143\n",
            "Episode: 31/100 RapTime: 0:00:01.164391 FixedProfit: 1197165\n",
            "Episode: 32/100 RapTime: 0:00:01.167465 FixedProfit: 1197165\n",
            "Episode: 33/100 RapTime: 0:00:01.216942 FixedProfit: 1208128\n",
            "Episode: 34/100 RapTime: 0:00:01.232436 FixedProfit: 1197165\n",
            "Episode: 35/100 RapTime: 0:00:01.230106 FixedProfit: 1173059\n",
            "Episode: 36/100 RapTime: 0:00:01.231639 FixedProfit: 1197165\n",
            "Episode: 37/100 RapTime: 0:00:01.261737 FixedProfit: 1197165\n",
            "Episode: 38/100 RapTime: 0:00:01.259372 FixedProfit: 1197165\n",
            "Episode: 39/100 RapTime: 0:00:01.253448 FixedProfit: 1197165\n",
            "Episode: 40/100 RapTime: 0:00:01.229928 FixedProfit: 1197165\n",
            "Episode: 41/100 RapTime: 0:00:01.249798 FixedProfit: 1197165\n",
            "Episode: 42/100 RapTime: 0:00:01.217505 FixedProfit: 1197165\n",
            "Episode: 43/100 RapTime: 0:00:01.238242 FixedProfit: 1197165\n",
            "Episode: 44/100 RapTime: 0:00:01.241511 FixedProfit: 1197165\n",
            "Episode: 45/100 RapTime: 0:00:01.226890 FixedProfit: 1197165\n",
            "Episode: 46/100 RapTime: 0:00:01.246825 FixedProfit: 1197165\n",
            "Episode: 47/100 RapTime: 0:00:01.245105 FixedProfit: 1197165\n",
            "Episode: 48/100 RapTime: 0:00:01.240095 FixedProfit: 1197165\n",
            "Episode: 49/100 RapTime: 0:00:01.240990 FixedProfit: 1197165\n",
            "Episode: 50/100 RapTime: 0:00:01.247459 FixedProfit: 1197165\n",
            "Episode: 51/100 RapTime: 0:00:01.240925 FixedProfit: 1197165\n",
            "Episode: 52/100 RapTime: 0:00:01.222388 FixedProfit: 1197165\n",
            "Episode: 53/100 RapTime: 0:00:01.202971 FixedProfit: 1197165\n",
            "Episode: 54/100 RapTime: 0:00:01.162519 FixedProfit: 1197165\n",
            "Episode: 55/100 RapTime: 0:00:01.174373 FixedProfit: 1197165\n",
            "Episode: 56/100 RapTime: 0:00:01.155676 FixedProfit: 1208128\n",
            "Episode: 57/100 RapTime: 0:00:01.180577 FixedProfit: 1197165\n",
            "Episode: 58/100 RapTime: 0:00:01.144608 FixedProfit: 1208128\n",
            "Episode: 59/100 RapTime: 0:00:01.142023 FixedProfit: 1213942\n",
            "Episode: 60/100 RapTime: 0:00:01.164953 FixedProfit: 1177016\n",
            "Episode: 61/100 RapTime: 0:00:01.171657 FixedProfit: 1194143\n",
            "Episode: 62/100 RapTime: 0:00:01.166757 FixedProfit: 1183321\n",
            "Episode: 63/100 RapTime: 0:00:01.188512 FixedProfit: 1195955\n",
            "Episode: 64/100 RapTime: 0:00:01.179501 FixedProfit: 1197165\n",
            "Episode: 65/100 RapTime: 0:00:01.152204 FixedProfit: 1183321\n",
            "Episode: 66/100 RapTime: 0:00:01.175169 FixedProfit: 1184935\n",
            "Episode: 67/100 RapTime: 0:00:01.171863 FixedProfit: 1213942\n",
            "Episode: 68/100 RapTime: 0:00:01.162972 FixedProfit: 1194143\n",
            "Episode: 69/100 RapTime: 0:00:01.156568 FixedProfit: 1194143\n",
            "Episode: 70/100 RapTime: 0:00:01.171579 FixedProfit: 1173059\n",
            "Episode: 71/100 RapTime: 0:00:01.168556 FixedProfit: 1197165\n",
            "Episode: 72/100 RapTime: 0:00:01.155716 FixedProfit: 1183321\n",
            "Episode: 73/100 RapTime: 0:00:01.175134 FixedProfit: 1197165\n",
            "Episode: 74/100 RapTime: 0:00:01.145617 FixedProfit: 1194143\n",
            "Episode: 75/100 RapTime: 0:00:01.143354 FixedProfit: 1183321\n",
            "Episode: 76/100 RapTime: 0:00:01.167776 FixedProfit: 1197165\n",
            "Episode: 77/100 RapTime: 0:00:01.147941 FixedProfit: 1173059\n",
            "Episode: 78/100 RapTime: 0:00:01.170180 FixedProfit: 1213942\n",
            "Episode: 79/100 RapTime: 0:00:01.138478 FixedProfit: 1197165\n",
            "Episode: 80/100 RapTime: 0:00:01.157369 FixedProfit: 1202990\n",
            "Episode: 81/100 RapTime: 0:00:01.176227 FixedProfit: 1194143\n",
            "Episode: 82/100 RapTime: 0:00:01.160564 FixedProfit: 1197165\n",
            "Episode: 83/100 RapTime: 0:00:01.158479 FixedProfit: 1208128\n",
            "Episode: 84/100 RapTime: 0:00:01.160286 FixedProfit: 1173059\n",
            "Episode: 85/100 RapTime: 0:00:01.183113 FixedProfit: 1194143\n",
            "Episode: 86/100 RapTime: 0:00:01.159063 FixedProfit: 1208128\n",
            "Episode: 87/100 RapTime: 0:00:01.170119 FixedProfit: 1197165\n",
            "Episode: 88/100 RapTime: 0:00:01.178053 FixedProfit: 1208128\n",
            "Episode: 89/100 RapTime: 0:00:01.228490 FixedProfit: 1197165\n",
            "Episode: 90/100 RapTime: 0:00:01.234997 FixedProfit: 1208128\n",
            "Episode: 91/100 RapTime: 0:00:01.254406 FixedProfit: 1197165\n",
            "Episode: 92/100 RapTime: 0:00:01.264713 FixedProfit: 1173059\n",
            "Episode: 93/100 RapTime: 0:00:01.242329 FixedProfit: 1197165\n",
            "Episode: 94/100 RapTime: 0:00:01.243020 FixedProfit: 1197165\n",
            "Episode: 95/100 RapTime: 0:00:01.269957 FixedProfit: 1197165\n",
            "Episode: 96/100 RapTime: 0:00:01.274429 FixedProfit: 1173059\n",
            "Episode: 97/100 RapTime: 0:00:01.256745 FixedProfit: 1208128\n",
            "Episode: 98/100 RapTime: 0:00:01.272200 FixedProfit: 1197165\n",
            "Episode: 99/100 RapTime: 0:00:01.269145 FixedProfit: 1197165\n",
            "Episode: 100/100 RapTime: 0:00:01.297177 FixedProfit: 1197165\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}