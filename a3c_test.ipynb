{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a3c_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "24f408b1-6646-43c4-df7d-8f40ac8e34de"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + 'sp500_test.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "models_folder = '/content/drive/My Drive/' + exp_dir + 'rl_models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + 'a3c_test.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m51Mu4xy9-Nj"
      },
      "source": [
        "def make_scaler(env):\n",
        "\n",
        "    states = []\n",
        "    for _ in range(env.df_total_steps):\n",
        "        action = np.random.choice(env.action_space)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_money = initial_money\n",
        "        self.mode = mode\n",
        "        self.trade_time = None\n",
        "        self.trade_win = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time = 0\n",
        "        self.trade_win = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step = self.df_total_steps\n",
        "        self.now_step = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class MasterBrain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model(input_, [actor, critic])\n",
        "        mastermodel.compile(optimizer=Adam(lr=lr))\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "def play_game(env, actor, critic, scaler, episodes_times = 25, batch_size = 32, mode = 'train'):\n",
        "\n",
        "    experiences = []\n",
        "    episode_rewards = []\n",
        "    actor.layering()\n",
        "\n",
        "    for episode in range(episodes_times):\n",
        "        state = env.reset()\n",
        "        state = scaler.transform([state])\n",
        "        state = state.flatten()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        while not done:\n",
        "            action = actor.policynetwork(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = scaler.transform([next_state])\n",
        "            next_state = next_state.flatten()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            if mode == 'train':\n",
        "                experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                if len(experiences) == batch_size:\n",
        "                    critic.valuenetwork(experiences)\n",
        "                    experiences = []\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        play_time = datetime.now() - start_time\n",
        "        if mode == 'test':\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            actor.integration()\n",
        "            actor.layering()\n",
        "            print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "            with open(csv_path, 'a') as f:\n",
        "                row = str(info['cur_revenue'])\n",
        "                print(row, file=f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce22610e-4852-433e-fb56-de4343ccf6cd"
      },
      "source": [
        "initial_money=1000000\n",
        "mode = 'test'\n",
        "episodes_times = 25\n",
        "batch_size = 32\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "if mode == 'test':\n",
        "    masterbrain.load(f'{models_folder}/a3c_model.h5')\n",
        "\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "        print(row, file=f)\n",
        "else:\n",
        "    with open(csv_path, 'w') as f:\n",
        "        row = 'FixedProfit'\n",
        "        print(row, file=f)\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    e = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    a = Actor(brain)\n",
        "    c = Critic(model)\n",
        "    s = make_scaler(e)\n",
        "    arr = [e,a,c,s]\n",
        "    envs.append(arr)\n",
        "\n",
        "datas = []\n",
        "with PoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = play_game(env[0],env[1],env[2],env[3], episodes_times, batch_size, mode)\n",
        "        datas.append(executor.submit(job))\n",
        "\n",
        "if mode == 'train':\n",
        "    masterbrain.save(f'{models_folder}/a3c_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/25 RapTime: 0:00:07.302092 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:00:07.330682 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:00:07.357990 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:00:07.402294 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:00:07.117855 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:00:07.186715 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:00:07.204822 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:00:07.167751 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:07.022100 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:06.977066 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:07.039475 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:07.070458 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:00:06.758019 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:00:06.715604 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:00:06.734576 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:00:06.814007 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:06.706618 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:06.844321 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:06.791182 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:06.769794 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:00:06.734161 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:00:06.701571 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:00:06.748488 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:00:06.739952 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:00:07.009432 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:00:07.046708 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:00:07.003471 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:00:07.084231 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:07.196129 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:07.253633 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:07.213203 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:07.213465 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:07.183437 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:07.217555 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:07.208618 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:07.227381 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:07.100896 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:07.086423 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:07.079503 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:07.000550 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:06.743439 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:06.809418 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:06.830677 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:06.819793 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:07.140498 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:07.086177 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:07.139512 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:07.158291 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:07.065046 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:07.088638 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:07.082798 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:07.151765 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:07.086352 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:07.101701 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:07.150269 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:07.036099 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:06.903797 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:06.936073 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:07.001046 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:06.932820 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:06.766923 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:06.741662 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:06.752699 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:06.774351 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:06.688971 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:06.781160 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:06.732516 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:06.771841 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:06.750982 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:06.754320 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:06.727212 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:06.738930 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:00:06.686659 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:00:06.689790 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:00:06.721620 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:00:06.736073 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:06.725014 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:06.756905 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:06.734792 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:06.718858 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:06.871782 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:06.963810 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:06.920916 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:06.928101 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:07.152302 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:07.132038 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:07.159808 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:07.133139 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:00:07.130283 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:00:07.139705 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:00:07.120888 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:00:07.166917 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:07.123856 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:07.177814 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:07.132976 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:07.154786 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:00:06.818792 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:00:06.768428 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:00:06.695430 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:00:06.522407 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}