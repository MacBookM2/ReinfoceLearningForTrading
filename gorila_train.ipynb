{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gorila_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPaQRYY3cpkWUrB5iO49hFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/gorila_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "5de0c1f4-4a36-4e57-c7ba-be323c01b79a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop()\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "mode = 'train'\n",
        "name = 'gorila'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=1000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2])\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.now_step        = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "        else:\n",
        "            if self.action_space[0] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1 \n",
        "            if self.action_space[2] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1RNmtkaZ2W"
      },
      "source": [
        "class ParameterServer:\n",
        "    def __init__(self):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        mastermodel = Sequential()\n",
        "        mastermodel.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_mid))\n",
        "        mastermodel.add(ReLU()) \n",
        "        mastermodel.add(Dense(n_action))\n",
        "        mastermodel.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((mastermodel.summary()))\n",
        "        self.mastermodel = mastermodel\n",
        "    \n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain):\n",
        "\n",
        "        n_mid, n_state, n_action = 3, 3, 3\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(n_mid, input_shape=(n_state,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_mid))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(n_action))\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "\n",
        "        print((model.summary()))\n",
        "        self.model = model\n",
        "        self.masterbrain = masterbrain\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save(name)\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def train_on_batch(self, state, target_full):\n",
        "        self.model.train_on_batch(state, target_full)\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w1_BMH7hLQ8"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size, batch_size=32):\n",
        "\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size3 = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self.states_memory = np.zeros([self.max_size3, 3], dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size3, 3], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size3, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size3, dtype=np.float32)\n",
        "        self.done_memory = np.zeros(self.max_size3, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, act, reward, next_state, done):\n",
        "        self.states_memory[self.cntr] = state\n",
        "        self.next_states_memory[self.cntr] = next_state\n",
        "        self.acts_memory[self.cntr] = act\n",
        "        self.rewards_memory[self.cntr] = reward\n",
        "        self.done_memory[self.cntr] = done\n",
        "        self.cntr = (self.cntr+1) % self.max_size3\n",
        "        self.size = min(self.size+1, self.max_size3)\n",
        "\n",
        "    def random_sampling(self):\n",
        "        mb_index = np.random.choice(self.size, self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.states_memory[mb_index],self.next_states_memory[mb_index],\n",
        "                 self.acts_memory[mb_index],self.rewards_memory[mb_index],\n",
        "                 self.done_memory[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "\n",
        "        return dict1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrEa4wpGG1DF"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, memory, max_size, batch_size = 32):\n",
        "\n",
        "        self.brain   = brain\n",
        "        self.memory  = memory\n",
        "        self.epsilon = 1.0\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self.brain.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42NIN-PGBOc8"
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, brain, memory, batch_size = 32):\n",
        "\n",
        "        self.brain  = brain\n",
        "        self.memory = memory\n",
        "\n",
        "        self.gamma       = 0.95\n",
        "        self.epsilon     = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r           = 0.995\n",
        "        self.batch_size  = batch_size\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.size < self.batch_size:\n",
        "            return\n",
        "\n",
        "        m_batch = self.memory.random_sampling()\n",
        "        states, next_states, actions, rewards, done = m_batch['state'], m_batch['next_state'], m_batch['act'], m_batch['reward'], m_batch['done']\n",
        "        target = rewards + (1 - done) * self.gamma * np.amax(self.brain.predict(next_states), axis=1)\n",
        "\n",
        "        target_full = self.brain.predict(states)\n",
        "\n",
        "        target_full[np.arange(self.batch_size), actions] = target\n",
        "        self.brain.train_on_batch(states, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, learner, num, mdl_dir, name, episodes_times = 1000, mode = 'test'):\n",
        "        self.env            = env\n",
        "        self.actor          = actor\n",
        "        self.learner        = learner\n",
        "        self.num            = str(num)\n",
        "        self.mdl_dir        = mdl_dir\n",
        "        self.scaler         = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode           = mode\n",
        "        self.name           = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.actor.epsilon = 0.01\n",
        "\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "\n",
        "    def play_game(self):\n",
        "        self.actor.layering()\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done  = False\n",
        "            start_time = datetime.now()\n",
        "        \n",
        "            while not done:\n",
        "                action = self.actor.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    self.actor.store_transition(state, action, reward, next_state, done)\n",
        "                    self.learner.learn()\n",
        "                \n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                learner.integration()\n",
        "                actor.layering()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "    \n",
        "            state = next_state\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name, self.num))\n",
        "\n",
        "\n",
        "    def _save(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name, self.num))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "2281259b-2258-481b-cef5-418667a191e9"
      },
      "source": [
        "initial_money  = 1000000\n",
        "episodes_times = 50\n",
        "batch_size     = 32\n",
        "max_size       = 500\n",
        "\n",
        "masterbrain    = ParameterServer()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env     = Environment(df, initial_money=initial_money, mode = mode)\n",
        "    brain   = Brain(masterbrain)\n",
        "    memory  = ReplayMemory(max_size, batch_size)\n",
        "    actor   = Actor(brain, memory, max_size, batch_size)\n",
        "    learner = Learner(brain, memory, batch_size)\n",
        "    main    = Main(env, actor, learner, i, mdl_dir, name, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_5 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_6 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_7 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_8 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_9 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Episode: 1/50 RapTime: 0:02:24.275092 FixedProfit: 1041503\n",
            "Episode: 1/50 RapTime: 0:02:25.859150 FixedProfit: 1243404\n",
            "Episode: 1/50 RapTime: 0:02:26.154102 FixedProfit: 1082748\n",
            "Episode: 1/50 RapTime: 0:02:26.375460 FixedProfit: 965801\n",
            "Episode: 2/50 RapTime: 0:02:32.985813 FixedProfit: 1227766\n",
            "Episode: 2/50 RapTime: 0:02:34.704435 FixedProfit: 1370085\n",
            "Episode: 2/50 RapTime: 0:02:33.814632 FixedProfit: 1036846\n",
            "Episode: 2/50 RapTime: 0:02:34.250289 FixedProfit: 900350\n",
            "Episode: 3/50 RapTime: 0:02:31.431973 FixedProfit: 1137452\n",
            "Episode: 3/50 RapTime: 0:02:33.516778 FixedProfit: 1396006\n",
            "Episode: 3/50 RapTime: 0:02:34.293857 FixedProfit: 1134513\n",
            "Episode: 3/50 RapTime: 0:02:33.827545 FixedProfit: 1154827\n",
            "Episode: 4/50 RapTime: 0:02:36.853911 FixedProfit: 1057392\n",
            "Episode: 4/50 RapTime: 0:02:35.370454 FixedProfit: 1129108\n",
            "Episode: 4/50 RapTime: 0:02:37.820374 FixedProfit: 1076093\n",
            "Episode: 4/50 RapTime: 0:02:38.491502 FixedProfit: 1014027\n",
            "Episode: 5/50 RapTime: 0:02:36.757869 FixedProfit: 1192029\n",
            "Episode: 5/50 RapTime: 0:02:37.175941 FixedProfit: 1276371\n",
            "Episode: 5/50 RapTime: 0:02:37.779666 FixedProfit: 1182104\n",
            "Episode: 5/50 RapTime: 0:02:38.340367 FixedProfit: 1134997\n",
            "Episode: 6/50 RapTime: 0:02:39.126629 FixedProfit: 1156145\n",
            "Episode: 6/50 RapTime: 0:02:39.584591 FixedProfit: 1216240\n",
            "Episode: 6/50 RapTime: 0:02:39.393513 FixedProfit: 1129608\n",
            "Episode: 6/50 RapTime: 0:02:39.373709 FixedProfit: 968656\n",
            "Episode: 7/50 RapTime: 0:02:39.244060 FixedProfit: 1078132\n",
            "Episode: 7/50 RapTime: 0:02:42.521067 FixedProfit: 1034704\n",
            "Episode: 7/50 RapTime: 0:02:43.489111 FixedProfit: 926499\n",
            "Episode: 7/50 RapTime: 0:02:41.493101 FixedProfit: 1173683\n",
            "Episode: 8/50 RapTime: 0:02:38.249078 FixedProfit: 1006977\n",
            "Episode: 8/50 RapTime: 0:02:41.073958 FixedProfit: 1115316\n",
            "Episode: 8/50 RapTime: 0:02:43.210964 FixedProfit: 1233974\n",
            "Episode: 8/50 RapTime: 0:02:38.961576 FixedProfit: 1180356\n",
            "Episode: 9/50 RapTime: 0:02:41.179772 FixedProfit: 1126865\n",
            "Episode: 9/50 RapTime: 0:02:41.242220 FixedProfit: 1194428\n",
            "Episode: 9/50 RapTime: 0:02:42.413296 FixedProfit: 1121501\n",
            "Episode: 9/50 RapTime: 0:02:43.627793 FixedProfit: 1097534\n",
            "Episode: 10/50 RapTime: 0:02:40.745893 FixedProfit: 1373881\n",
            "Episode: 10/50 RapTime: 0:02:44.798145 FixedProfit: 1225809\n",
            "Episode: 10/50 RapTime: 0:02:42.633314 FixedProfit: 1018589\n",
            "Episode: 10/50 RapTime: 0:02:46.260813 FixedProfit: 1320073\n",
            "Episode: 11/50 RapTime: 0:02:47.941870 FixedProfit: 1124815\n",
            "Episode: 11/50 RapTime: 0:02:45.135235 FixedProfit: 1006624\n",
            "Episode: 11/50 RapTime: 0:02:42.528445 FixedProfit: 1156207\n",
            "Episode: 11/50 RapTime: 0:02:46.702192 FixedProfit: 973560\n",
            "Episode: 12/50 RapTime: 0:02:51.314245 FixedProfit: 1116930\n",
            "Episode: 12/50 RapTime: 0:02:46.957154 FixedProfit: 1003047\n",
            "Episode: 12/50 RapTime: 0:02:47.204630 FixedProfit: 1198587\n",
            "Episode: 12/50 RapTime: 0:02:47.431133 FixedProfit: 1082050\n",
            "Episode: 13/50 RapTime: 0:02:47.237736 FixedProfit: 1082070\n",
            "Episode: 13/50 RapTime: 0:02:45.213869 FixedProfit: 1114869\n",
            "Episode: 13/50 RapTime: 0:02:47.914297 FixedProfit: 1024629\n",
            "Episode: 13/50 RapTime: 0:02:47.835459 FixedProfit: 1014755\n",
            "Episode: 14/50 RapTime: 0:02:48.929542 FixedProfit: 1200976\n",
            "Episode: 14/50 RapTime: 0:02:50.443798 FixedProfit: 1163441\n",
            "Episode: 14/50 RapTime: 0:02:52.379110 FixedProfit: 1121582\n",
            "Episode: 14/50 RapTime: 0:02:54.486396 FixedProfit: 1119646\n",
            "Episode: 15/50 RapTime: 0:02:54.416845 FixedProfit: 1253843\n",
            "Episode: 15/50 RapTime: 0:02:54.900248 FixedProfit: 923893\n",
            "Episode: 15/50 RapTime: 0:02:57.673595 FixedProfit: 995179\n",
            "Episode: 15/50 RapTime: 0:02:54.049446 FixedProfit: 1055352\n",
            "Episode: 16/50 RapTime: 0:02:59.641281 FixedProfit: 1111610\n",
            "Episode: 16/50 RapTime: 0:03:02.193581 FixedProfit: 1077741\n",
            "Episode: 16/50 RapTime: 0:03:03.435519 FixedProfit: 992886\n",
            "Episode: 16/50 RapTime: 0:03:03.834466 FixedProfit: 1052606\n",
            "Episode: 17/50 RapTime: 0:02:52.532216 FixedProfit: 1163312\n",
            "Episode: 17/50 RapTime: 0:02:55.864104 FixedProfit: 1073083\n",
            "Episode: 17/50 RapTime: 0:02:58.261348 FixedProfit: 1035416\n",
            "Episode: 17/50 RapTime: 0:02:55.128187 FixedProfit: 1175095\n",
            "Episode: 18/50 RapTime: 0:02:58.873435 FixedProfit: 1106744\n",
            "Episode: 18/50 RapTime: 0:02:59.865357 FixedProfit: 1094578\n",
            "Episode: 18/50 RapTime: 0:02:57.741463 FixedProfit: 944209\n",
            "Episode: 18/50 RapTime: 0:03:00.227948 FixedProfit: 1105620\n",
            "Episode: 19/50 RapTime: 0:02:55.380502 FixedProfit: 1172293\n",
            "Episode: 19/50 RapTime: 0:02:58.898324 FixedProfit: 1135518\n",
            "Episode: 19/50 RapTime: 0:02:58.284412 FixedProfit: 1145147\n",
            "Episode: 19/50 RapTime: 0:02:58.437525 FixedProfit: 1073440\n",
            "Episode: 20/50 RapTime: 0:02:59.211109 FixedProfit: 1122264\n",
            "Episode: 20/50 RapTime: 0:03:04.275291 FixedProfit: 1088625\n",
            "Episode: 20/50 RapTime: 0:03:00.901752 FixedProfit: 1128727\n",
            "Episode: 20/50 RapTime: 0:03:00.914581 FixedProfit: 1221145\n",
            "Episode: 21/50 RapTime: 0:03:00.156352 FixedProfit: 1213593\n",
            "Episode: 21/50 RapTime: 0:03:00.259862 FixedProfit: 924323\n",
            "Episode: 21/50 RapTime: 0:03:01.071376 FixedProfit: 1151758\n",
            "Episode: 21/50 RapTime: 0:03:02.829864 FixedProfit: 1022997\n",
            "Episode: 22/50 RapTime: 0:03:10.538692 FixedProfit: 977865\n",
            "Episode: 22/50 RapTime: 0:03:09.880414 FixedProfit: 1194179\n",
            "Episode: 22/50 RapTime: 0:03:09.072706 FixedProfit: 1145275\n",
            "Episode: 22/50 RapTime: 0:03:10.550201 FixedProfit: 925729\n",
            "Episode: 23/50 RapTime: 0:03:08.696798 FixedProfit: 1211959\n",
            "Episode: 23/50 RapTime: 0:03:07.493968 FixedProfit: 1001352\n",
            "Episode: 23/50 RapTime: 0:03:12.843353 FixedProfit: 980778\n",
            "Episode: 23/50 RapTime: 0:03:11.137221 FixedProfit: 966522\n",
            "Episode: 24/50 RapTime: 0:03:09.197402 FixedProfit: 1092263\n",
            "Episode: 24/50 RapTime: 0:03:12.727591 FixedProfit: 1100372\n",
            "Episode: 24/50 RapTime: 0:03:14.895104 FixedProfit: 1112627\n",
            "Episode: 24/50 RapTime: 0:03:09.984703 FixedProfit: 1054744\n",
            "Episode: 25/50 RapTime: 0:03:09.178770 FixedProfit: 1106443\n",
            "Episode: 25/50 RapTime: 0:03:11.456899 FixedProfit: 1027700\n",
            "Episode: 25/50 RapTime: 0:03:07.503536 FixedProfit: 1003786\n",
            "Episode: 25/50 RapTime: 0:03:12.985984 FixedProfit: 931154\n",
            "Episode: 26/50 RapTime: 0:03:16.266340 FixedProfit: 1112102\n",
            "Episode: 26/50 RapTime: 0:03:11.123816 FixedProfit: 1091193\n",
            "Episode: 26/50 RapTime: 0:03:19.146480 FixedProfit: 943904\n",
            "Episode: 26/50 RapTime: 0:03:16.234751 FixedProfit: 959893\n",
            "Episode: 27/50 RapTime: 0:03:11.516744 FixedProfit: 914363\n",
            "Episode: 27/50 RapTime: 0:03:14.956956 FixedProfit: 1114629\n",
            "Episode: 27/50 RapTime: 0:03:14.869311 FixedProfit: 1122477\n",
            "Episode: 27/50 RapTime: 0:03:15.932351 FixedProfit: 1089019\n",
            "Episode: 28/50 RapTime: 0:03:18.347693 FixedProfit: 1007220\n",
            "Episode: 28/50 RapTime: 0:03:20.611330 FixedProfit: 1056231\n",
            "Episode: 28/50 RapTime: 0:03:20.792089 FixedProfit: 1060341\n",
            "Episode: 28/50 RapTime: 0:03:19.790647 FixedProfit: 1085399\n",
            "Episode: 29/50 RapTime: 0:03:17.751547 FixedProfit: 1380287\n",
            "Episode: 29/50 RapTime: 0:03:15.776944 FixedProfit: 1039872\n",
            "Episode: 29/50 RapTime: 0:03:20.182069 FixedProfit: 1289324\n",
            "Episode: 29/50 RapTime: 0:03:20.207919 FixedProfit: 1093319\n",
            "Episode: 30/50 RapTime: 0:03:18.581394 FixedProfit: 970670\n",
            "Episode: 30/50 RapTime: 0:03:27.006541 FixedProfit: 932275\n",
            "Episode: 30/50 RapTime: 0:03:20.923986 FixedProfit: 1059814\n",
            "Episode: 30/50 RapTime: 0:03:22.342408 FixedProfit: 1275587\n",
            "Episode: 31/50 RapTime: 0:03:22.606557 FixedProfit: 1014886\n",
            "Episode: 31/50 RapTime: 0:03:21.453489 FixedProfit: 1093309\n",
            "Episode: 31/50 RapTime: 0:03:22.505513 FixedProfit: 1063655\n",
            "Episode: 31/50 RapTime: 0:03:23.878013 FixedProfit: 1219156\n",
            "Episode: 32/50 RapTime: 0:03:25.050510 FixedProfit: 1128043\n",
            "Episode: 32/50 RapTime: 0:03:29.954760 FixedProfit: 1207136\n",
            "Episode: 32/50 RapTime: 0:03:32.940605 FixedProfit: 1226559\n",
            "Episode: 32/50 RapTime: 0:03:35.744484 FixedProfit: 1067158\n",
            "Episode: 33/50 RapTime: 0:03:28.486396 FixedProfit: 1038336\n",
            "Episode: 33/50 RapTime: 0:03:33.284241 FixedProfit: 1105955\n",
            "Episode: 33/50 RapTime: 0:03:32.788371 FixedProfit: 975700\n",
            "Episode: 33/50 RapTime: 0:03:32.017650 FixedProfit: 915975\n",
            "Episode: 34/50 RapTime: 0:03:29.661815 FixedProfit: 1022822\n",
            "Episode: 34/50 RapTime: 0:03:33.507666 FixedProfit: 1263237\n",
            "Episode: 34/50 RapTime: 0:03:34.415379 FixedProfit: 1271931\n",
            "Episode: 34/50 RapTime: 0:03:33.871870 FixedProfit: 1120124\n",
            "Episode: 35/50 RapTime: 0:03:33.958170 FixedProfit: 1326153\n",
            "Episode: 35/50 RapTime: 0:03:34.466796 FixedProfit: 1120079\n",
            "Episode: 35/50 RapTime: 0:03:38.756718 FixedProfit: 1135986\n",
            "Episode: 35/50 RapTime: 0:03:33.658702 FixedProfit: 1232664\n",
            "Episode: 36/50 RapTime: 0:03:30.689408 FixedProfit: 1051575\n",
            "Episode: 36/50 RapTime: 0:03:30.035955 FixedProfit: 1029951\n",
            "Episode: 36/50 RapTime: 0:03:35.573866 FixedProfit: 999835\n",
            "Episode: 36/50 RapTime: 0:03:31.135439 FixedProfit: 1185594\n",
            "Episode: 37/50 RapTime: 0:03:41.474464 FixedProfit: 963437\n",
            "Episode: 37/50 RapTime: 0:03:32.748929 FixedProfit: 1079867\n",
            "Episode: 37/50 RapTime: 0:03:38.832664 FixedProfit: 1210090\n",
            "Episode: 37/50 RapTime: 0:03:36.300710 FixedProfit: 933431\n",
            "Episode: 38/50 RapTime: 0:03:41.215869 FixedProfit: 932775\n",
            "Episode: 38/50 RapTime: 0:03:33.311626 FixedProfit: 973312\n",
            "Episode: 38/50 RapTime: 0:03:36.557977 FixedProfit: 1047541\n",
            "Episode: 38/50 RapTime: 0:03:33.394758 FixedProfit: 1173186\n",
            "Episode: 39/50 RapTime: 0:03:39.833762 FixedProfit: 1063160\n",
            "Episode: 39/50 RapTime: 0:03:43.420255 FixedProfit: 1215094\n",
            "Episode: 39/50 RapTime: 0:03:42.362561 FixedProfit: 1077209\n",
            "Episode: 39/50 RapTime: 0:03:38.246678 FixedProfit: 969003\n",
            "Episode: 40/50 RapTime: 0:03:39.423055 FixedProfit: 1069795\n",
            "Episode: 40/50 RapTime: 0:03:38.385697 FixedProfit: 901048\n",
            "Episode: 40/50 RapTime: 0:03:37.704892 FixedProfit: 1022855\n",
            "Episode: 40/50 RapTime: 0:03:40.208827 FixedProfit: 1086621\n",
            "Episode: 41/50 RapTime: 0:03:50.985640 FixedProfit: 1100213\n",
            "Episode: 41/50 RapTime: 0:03:45.674111 FixedProfit: 1233530\n",
            "Episode: 41/50 RapTime: 0:03:50.224483 FixedProfit: 1119851\n",
            "Episode: 41/50 RapTime: 0:03:47.279298 FixedProfit: 1084833\n",
            "Episode: 42/50 RapTime: 0:03:46.862977 FixedProfit: 885800\n",
            "Episode: 42/50 RapTime: 0:03:50.379128 FixedProfit: 1374022\n",
            "Episode: 42/50 RapTime: 0:03:50.737210 FixedProfit: 1069289\n",
            "Episode: 42/50 RapTime: 0:03:47.167460 FixedProfit: 1210371\n",
            "Episode: 43/50 RapTime: 0:03:52.067656 FixedProfit: 1142475\n",
            "Episode: 43/50 RapTime: 0:03:47.002614 FixedProfit: 1144237\n",
            "Episode: 43/50 RapTime: 0:03:48.203121 FixedProfit: 1355129\n",
            "Episode: 43/50 RapTime: 0:03:49.647861 FixedProfit: 1081830\n",
            "Episode: 44/50 RapTime: 0:03:49.637658 FixedProfit: 1100349\n",
            "Episode: 44/50 RapTime: 0:03:51.003792 FixedProfit: 1151679\n",
            "Episode: 44/50 RapTime: 0:03:48.382400 FixedProfit: 1182104\n",
            "Episode: 44/50 RapTime: 0:03:49.046642 FixedProfit: 965526\n",
            "Episode: 45/50 RapTime: 0:03:48.066831 FixedProfit: 960554\n",
            "Episode: 45/50 RapTime: 0:03:49.182839 FixedProfit: 1262632\n",
            "Episode: 45/50 RapTime: 0:03:54.542054 FixedProfit: 947431\n",
            "Episode: 45/50 RapTime: 0:03:55.605059 FixedProfit: 1055824\n",
            "Episode: 46/50 RapTime: 0:03:50.664457 FixedProfit: 1072317\n",
            "Episode: 46/50 RapTime: 0:03:51.793473 FixedProfit: 947555\n",
            "Episode: 46/50 RapTime: 0:03:57.823408 FixedProfit: 1033952\n",
            "Episode: 46/50 RapTime: 0:03:51.746531 FixedProfit: 935114\n",
            "Episode: 47/50 RapTime: 0:03:57.767802 FixedProfit: 969036\n",
            "Episode: 47/50 RapTime: 0:03:58.784790 FixedProfit: 1116503\n",
            "Episode: 47/50 RapTime: 0:03:58.101426 FixedProfit: 1113668\n",
            "Episode: 47/50 RapTime: 0:04:04.950415 FixedProfit: 1137599\n",
            "Episode: 48/50 RapTime: 0:03:59.711659 FixedProfit: 995249\n",
            "Episode: 48/50 RapTime: 0:03:57.230516 FixedProfit: 965554\n",
            "Episode: 48/50 RapTime: 0:03:58.787893 FixedProfit: 1224469\n",
            "Episode: 48/50 RapTime: 0:03:59.360895 FixedProfit: 1245700\n",
            "Episode: 49/50 RapTime: 0:04:06.976589 FixedProfit: 1158186\n",
            "Episode: 49/50 RapTime: 0:04:00.445128 FixedProfit: 1149439\n",
            "Episode: 49/50 RapTime: 0:04:04.028046 FixedProfit: 1256542\n",
            "Episode: 49/50 RapTime: 0:03:58.546233 FixedProfit: 1162823\n",
            "Episode: 50/50 RapTime: 0:04:07.150627 FixedProfit: 1105314\n",
            "Episode: 50/50 RapTime: 0:03:57.484875 FixedProfit: 1155697\n",
            "Episode: 50/50 RapTime: 0:03:58.335390 FixedProfit: 1054025\n",
            "Episode: 50/50 RapTime: 0:03:49.758870 FixedProfit: 907831\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}