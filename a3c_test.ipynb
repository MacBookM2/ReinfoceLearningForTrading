{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3c_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/a3c_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tAp1naUv8Mo",
        "outputId": "ab72a21c-db46-4c05-8341-b5a729441f7b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "mode = 'test'\n",
        "name = 'a3c'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWpPcFntqTL"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test'):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "        else:\n",
        "            info = { 'cur_revenue' : cur_revenue }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            if self.mode == 'test':\n",
        "                self.trade_time += 1\n",
        "                if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                    self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    if self.mode == 'test':\n",
        "                        self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    if self.mode == 'test':\n",
        "                        self.trade_time += 1\n",
        "                        if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                            self.trade_win += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNTJB0pLlN08"
      },
      "source": [
        "class MasterBrain:\n",
        "    def __init__(self,n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        mastermodel = keras.Model(input_, [actor, critic])\n",
        "        mastermodel.compile(optimizer=Adam(lr=lr))\n",
        "        mastermodel.summary()\n",
        "        self.mastermodel = mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.mastermodel.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.mastermodel.save_weights(name)\n",
        "\n",
        "    def placement(self, model):\n",
        "        for m, mm in zip(model.trainable_weights, self.mastermodel.trainable_weights):\n",
        "            m.assign(mm)\n",
        "\n",
        "    def integration(self, model):\n",
        "        for mm, m in zip(self.mastermodel.trainable_weights, model.trainable_weights):\n",
        "            mm.assign(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQtk2tYMVgI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, masterbrain, n_action = 3):\n",
        "\n",
        "        n_shape = 3\n",
        "        self.n_action = n_action\n",
        "        lr = 0.01\n",
        "\n",
        "        common = input_ = keras.layers.Input(shape=n_shape)\n",
        "        common = keras.layers.Dense(128, activation=\"relu\")(common)\n",
        "\n",
        "        actor = keras.layers.Dense(self.n_action, activation=\"softmax\")(common)\n",
        "        critic = keras.layers.Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = keras.Model(input_, [actor, critic])\n",
        "        model.compile(optimizer=Adam(lr=lr))\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "\n",
        "        self.masterbrain = masterbrain\n",
        "        self.mastermodel = masterbrain.mastermodel\n",
        "\n",
        "    def load(self, name):\n",
        "        self.masterbrain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.masterbrain.save(name)\n",
        "\n",
        "    def layering(self):\n",
        "        self.masterbrain.placement(self.model)\n",
        "\n",
        "    def integration(self):\n",
        "        self.masterbrain.integration(self.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4mqXczMr-E"
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, brain, n_action = 3):\n",
        "        self.model = brain.model\n",
        "        self.n_action = n_action\n",
        "        self.brain = brain\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        act_p, _ = self.model(state.reshape((1,-1)))\n",
        "        return np.random.choice(self.n_action, p=act_p[0].numpy())\n",
        "\n",
        "    def load(self, name):\n",
        "        self.brain.load(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.brain.save(name)\n",
        "\n",
        "    def layering(self):\n",
        "        self.brain.layering()\n",
        "\n",
        "    def integration(self):\n",
        "        self.brain.integration()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31lzN_0uM3fU"
      },
      "source": [
        "class Critic:\n",
        "    def __init__(self,model,n_action=3):\n",
        "        self.model = model\n",
        "        self.n_action = n_action\n",
        "        self.gamma = 0.9\n",
        "        self.beta = 0.1\n",
        "\n",
        "    def valuenetwork(self, experiences):\n",
        "\n",
        "        discounted_return = self._discounted_return(experiences)\n",
        "\n",
        "        state_batch = np.asarray([e[\"state\"] for e in experiences])\n",
        "        action_batch = np.asarray([e[\"action\"] for e in experiences])\n",
        "\n",
        "        onehot_actions = tf.one_hot(action_batch, self.n_action)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = self.model(state_batch, training=True)\n",
        "            selct_pai = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            selected_action_probs = tf.clip_by_value(selct_pai, 1e-10, 1.0)\n",
        "            advantage = discounted_return - tf.stop_gradient(v)\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage,selected_action_probs,v,discounted_return)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.model.optimizer.apply_gradients(\n",
        "            (grad, var) \n",
        "            for (grad, var) in zip(gradients, model.trainable_variables) \n",
        "            if grad is not None\n",
        "        )\n",
        "\n",
        "    def _discounted_return(self,experiences):\n",
        "        if experiences[-1][\"done\"]:\n",
        "            G = 0\n",
        "        else:\n",
        "            next_state = np.atleast_2d(experiences[-1][\"next_state\"])\n",
        "            _, n_v = self.model(next_state)\n",
        "            G = n_v[0][0].numpy()\n",
        "\n",
        "        discounted_return = []\n",
        "        for exp in reversed(experiences):\n",
        "            if exp[\"done\"]:\n",
        "                G = 0\n",
        "            G = exp[\"reward\"] + self.gamma * G\n",
        "            discounted_return.append(G)\n",
        "        discounted_return.reverse()\n",
        "        discounted_return = np.asarray(discounted_return).reshape((-1, 1))\n",
        "        discounted_return -= np.mean(discounted_return)\n",
        "        return discounted_return\n",
        "\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,selected_action_probs,v,discounted_return):\n",
        "\n",
        "        a = tf.math.log(selected_action_probs) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "\n",
        "        a,_ = v.shape\n",
        "\n",
        "        ave = v.numpy()    \n",
        "        sigma2 = np.std(ave)\n",
        "        entropy = self.beta*0.5*(math.log(2 * math.pi * sigma2) + 1)\n",
        "\n",
        "        mylist = [[entropy] for i in range(a)]\n",
        "        rank_1_tensor = tf.constant(mylist)\n",
        "\n",
        "        return rank_1_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPGjyT83gyh"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, num, mdl_dir, name, batch_size = 32, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.num = str(num)\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.batch_size = batch_size\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit,TradeTimes,TradeWin'\n",
        "                print(row, file=f)\n",
        "        else:\n",
        "            with open(csv_path, 'w') as f:\n",
        "                row = 'FixedProfit'\n",
        "                print(row, file=f)\n",
        "        \n",
        "        self.actor.layering()\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            state = state.flatten()\n",
        "            done = False\n",
        "            start_time = datetime.now()\n",
        "            experiences = []\n",
        "    \n",
        "            while not done:\n",
        "                \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                next_state = next_state.flatten()\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    experiences.append({\"state\": state, \"action\": action, \"reward\": reward, \"next_state\": next_state, \"done\": done,})\n",
        "                    if len(experiences) == self.batch_size:\n",
        "                        self.critic.valuenetwork(experiences)\n",
        "                        experiences = []\n",
        "\n",
        "                state = next_state\n",
        "               \n",
        "            play_time = datetime.now() - start_time\n",
        "            if mode == 'test':\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f} TradeTimes: {} TradeWin: {}\".format(episode + 1, episodes_times, play_time, info['cur_revenue'], info['trade_time'], info['trade_win']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue']) + ',' + str(info['trade_time']) + ',' + str(info['trade_win'])\n",
        "                    print(row, file=f)\n",
        "            else:\n",
        "                actor.integration()\n",
        "                actor.layering()\n",
        "                print(\"Episode: {}/{} RapTime: {} FixedProfit: {:.0f}\".format(episode + 1, episodes_times, play_time, info['cur_revenue']))\n",
        "                with open(csv_path, 'a') as f:\n",
        "                    row = str(info['cur_revenue'])\n",
        "                    print(row, file=f)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.actor.load('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.actor.save('{}/{}.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}_{}.pkl'.format(self.mdl_dir, self.name, self.num), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgv85YlVOaum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a91b459-c975-40fc-b3f8-3ea450127324"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 25\n",
        "batch_size = 32\n",
        "masterbrain = MasterBrain()\n",
        "\n",
        "thread_num = 4\n",
        "envs = []\n",
        "for i in range(thread_num):\n",
        "    env = Environment(df, initial_money=initial_money,mode = mode)\n",
        "    brain = Brain(masterbrain)\n",
        "    model = brain.model\n",
        "    actor = Actor(brain)\n",
        "    critic = Critic(model)\n",
        "    main = Main(env, actor, critic, i, mdl_dir, name, batch_size, episodes_times, mode)\n",
        "    envs.append(main)\n",
        "\n",
        "datas = []\n",
        "with ThreadPoolExecutor(max_workers=thread_num) as executor:\n",
        "    for env in envs:\n",
        "        job = lambda: env.play_game()\n",
        "        datas.append(executor.submit(job))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          512         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            387         dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          512         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            129         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          512         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          512         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            129         dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 128)          512         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 3)            387         dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            129         dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 1,028\n",
            "Trainable params: 1,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode: 1/25 RapTime: 0:00:08.213163 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:00:08.215463 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:00:08.253484 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 1/25 RapTime: 0:00:08.287664 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:00:06.969092 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:00:06.989670 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:00:06.925262 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 2/25 RapTime: 0:00:07.004022 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:06.669648 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:06.700071 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:06.686387 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 3/25 RapTime: 0:00:06.796104 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:00:07.257089 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:00:07.333845 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:00:07.310246 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 4/25 RapTime: 0:00:07.290797 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:06.931798 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:06.949604 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:06.932485 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 5/25 RapTime: 0:00:06.991532 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:00:06.919390 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:00:06.891432 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:00:06.969018 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 6/25 RapTime: 0:00:06.967735 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:00:06.916783 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:00:06.841931 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:00:06.886998 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 7/25 RapTime: 0:00:06.916178 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:06.478267 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:06.539891 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:06.493612 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 8/25 RapTime: 0:00:06.543199 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:06.565576 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:06.532588 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:06.540498 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 9/25 RapTime: 0:00:06.558304 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:06.530011 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:06.506590 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:06.547830 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 10/25 RapTime: 0:00:06.534776 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:06.546783 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:06.566646 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:06.500431 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 11/25 RapTime: 0:00:06.497482 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:06.497793 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:06.526684 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:06.578855 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 12/25 RapTime: 0:00:06.538469 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:06.637825 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:06.591691 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:06.660958 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 13/25 RapTime: 0:00:06.610144 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:07.048872 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:06.962406 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:06.991848 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 14/25 RapTime: 0:00:07.021417 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:06.958919 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:06.947850 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:07.002042 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 15/25 RapTime: 0:00:06.933080 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:06.955642 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:07.006282 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:06.903934 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 16/25 RapTime: 0:00:06.994645 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:06.769544 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:06.804550 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:06.769828 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 17/25 RapTime: 0:00:06.771706 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:06.590461 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:06.517916 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:06.545402 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 18/25 RapTime: 0:00:06.576190 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:00:06.584638 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:00:06.549275 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:00:06.573509 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 19/25 RapTime: 0:00:06.591466 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:06.507142 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:06.545163 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:06.547774 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 20/25 RapTime: 0:00:06.473320 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:06.523076 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:06.527483 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:06.483652 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 21/25 RapTime: 0:00:06.513460 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:06.517715 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:06.587195 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:06.511626 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 22/25 RapTime: 0:00:06.628180 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:00:06.660322 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:00:06.743319 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:00:06.744552 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 23/25 RapTime: 0:00:06.779136 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:06.939546 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:06.944469 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:06.967896 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 24/25 RapTime: 0:00:07.000467 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:00:06.915516 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:00:06.942042 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:00:06.869899 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n",
            "Episode: 25/25 RapTime: 0:00:06.871973 FixedProfit: 1543971 TradeTimes: 1 TradeWin: 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}