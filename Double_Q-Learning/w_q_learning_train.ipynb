{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w_q_learning_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOGAu+DY1oaf5YCWjdPPj+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinfoceLearningForTrading/blob/main/Double_Q-Learning/w_q_learning_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NIXg6mTzk0K",
        "outputId": "d8109c38-2f90-4895-f4cc-3326cc802803"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import copy\n",
        "\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "mode = 'train'\n",
        "name = 'w_qlearning'\n",
        "level = 1\n",
        "if level == 2:\n",
        "    name += name + 'lv2'\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "nov_dir = 'Colab Notebooks/dataset/reinforcement_learning/'\n",
        "nov_path = '/content/drive/My Drive/' + nov_dir + f'sp500_{mode}.csv'\n",
        "\n",
        "exp_dir = 'Colab Notebooks/workspace/export/'\n",
        "mdl_dir = '/content/drive/My Drive/' + exp_dir + 'models'\n",
        "csv_path = '/content/drive/My Drive/' + exp_dir + f'csv_data/{name}_{mode}.csv'\n",
        "\n",
        "df = pd.read_csv(nov_path)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m-%d')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN1DKfV6zauY"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000, mode = 'test', commission = 0):\n",
        "\n",
        "        self.df = df.dropna().reset_index()\n",
        "\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.mode            = mode\n",
        "        self.commission      = commission\n",
        "        self.trade_time      = None\n",
        "        self.trade_win       = None\n",
        "        self.brfore_buy_cash = None\n",
        "        self.action_space    = np.array([0, 1, 2]) # buy,hold,sell\n",
        "        self.hold_a_position = None\n",
        "        self.now_price       = None\n",
        "        self.cash_in_hand    = None\n",
        "        self.sell_price      = None\n",
        "        self.buy_price       = None\n",
        "\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "\n",
        "        self.trade_time      = 0\n",
        "        self.trade_win       = 0\n",
        "        self.brfore_buy_cash = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "        self.now_step        = 0\n",
        "        self.hold_a_position = 0.0\n",
        "        self.now_price       = self.df.loc[self.now_step, 'SP500']\n",
        "        self.cash_in_hand    = self.initial_money\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_revenue = self._get_revenue()\n",
        "        self.now_step += 1\n",
        "        self.now_price = self.df.loc[self.now_step, 'SP500']\n",
        " \n",
        "        done = (self.end_step == self.now_step)\n",
        "\n",
        "        self._trade(action,done)\n",
        "        cur_revenue = self._get_revenue()\n",
        " \n",
        "        reward = cur_revenue - prev_revenue\n",
        "\n",
        "        info = { 'cur_revenue' : cur_revenue , 'trade_time' : self.trade_time, 'trade_win' : self.trade_win }\n",
        "\n",
        "        return self._get_now_state(), reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(3)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.now_price\n",
        "        state[2] = self.cash_in_hand\n",
        "        return state\n",
        "\n",
        "    def _get_revenue(self): \n",
        "        return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action,lastorder = False):\n",
        "        if lastorder:\n",
        "            self.cash_in_hand += self.now_price * self.hold_a_position\n",
        "            self.hold_a_position = 0\n",
        "            self.trade_time += 1\n",
        "            if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                self.trade_win += 1\n",
        "        else:\n",
        "            if self.action_space[0] == action: # buy\n",
        "                if self.hold_a_position == 0:\n",
        "                    buy_flag = True\n",
        "                    self.brfore_buy_cash = copy.copy(self.cash_in_hand)\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.now_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price + self.commission * self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "            if self.action_space[2] == action: # sell\n",
        "                if self.hold_a_position != 0:\n",
        "                    self.cash_in_hand += self.now_price * self.hold_a_position - self.commission * self.now_price * self.hold_a_position\n",
        "                    self.hold_a_position = 0\n",
        "                    self.trade_time += 1\n",
        "                    if self.cash_in_hand > self.brfore_buy_cash:\n",
        "                        self.trade_win += 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGeWOM-ZWNYK"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.9\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(3, input_shape=(3,)))\n",
        "        model.add(ReLU()) \n",
        "        model.add(Dense(3))\n",
        "        model.add(ReLU())\n",
        "        model.add(Dense(3))\n",
        "        model.compile(loss=\"mse\", optimizer=RMSprop())\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "    \n",
        "        model_2 = Sequential()\n",
        "        model_2.add(Dense(3, input_shape=(3,)))\n",
        "        model_2.add(ReLU()) \n",
        "        model_2.add(Dense(3))\n",
        "        model_2.add(ReLU()) \n",
        "        model_2.add(Dense(3))\n",
        "        model_2.compile(loss=\"mse\", optimizer=RMSprop())\n",
        "        model_2.summary()\n",
        "        self.model_2 = model_2\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done, s_flag):\n",
        "\n",
        "        if s_flag == 11:\n",
        "            q = self.model.predict(state)  \n",
        "            next_q = self.model_2.predict(next_state)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, action] = reward + (1 - done) * self.gamma*np.amax(next_q, axis=1)\n",
        "            self.model.train_on_batch(state, target)\n",
        "        else:\n",
        "            q = self.model_2.predict(state)  \n",
        "            next_q = self.model.predict(next_state)\n",
        "            target = np.copy(q)\n",
        "\n",
        "            target[:, action] = reward + (1 - done) * self.gamma*np.amax(next_q, axis=1)\n",
        "            self.model_2.train_on_batch(state, target)\n",
        "\n",
        "\n",
        "    def _predict(self, state, s_flag = 12):\n",
        "        values = None\n",
        "        q1 = self.model.predict(state)\n",
        "        q2 = self.model_2.predict(state)\n",
        "        if s_flag == 12:\n",
        "            values = np.array([q1[0,a] + q2[0,a] for a in range(3)])\n",
        "        elif s_flag == 11:\n",
        "            values = np.array([q1[0,a] + q1[0,a] for a in range(3)])\n",
        "        else:\n",
        "            values = np.array([q2[0,a] + q2[0,a] for a in range(3)])\n",
        "        return values\n",
        "\n",
        "    def load(self, name, name2):\n",
        "        self.model.load_weights(name)\n",
        "        self.model_2.load_weights(name2)\n",
        "\n",
        "    def save(self, name, name2):\n",
        "        self.model.save_weights(name)\n",
        "        self.model_2.save_weights(name2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxR4grMVRLCR"
      },
      "source": [
        "class Agent(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.r = 0.995\n",
        "\n",
        "    def act(self, state,s_flag=12):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(3)\n",
        "        act_values = self._predict(state,s_flag)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.r\n",
        "        return np.argmax(act_values)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On5S8YtLz3U4"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, agent, mdl_dir, name, episodes_times = 1000, mode = 'test'):\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.mdl_dir = mdl_dir\n",
        "        self.scaler = self._standard_scaler(self.env)\n",
        "        self.episodes_times = episodes_times\n",
        "        self.mode = mode\n",
        "        self.name = name\n",
        "        self.df_rec = pd.DataFrame(index=[], columns=['FixedProfit','TradeTimes','TradeWin'])\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            self._load()\n",
        "            self.agent.epsilon = 0.01\n",
        "\n",
        "    def play_game(self):\n",
        "\n",
        "        for episode in range(self.episodes_times):\n",
        "\n",
        "            if (episode % 10 == 0):\n",
        "                metrics_names = ['FixedProfit','TradeTimes','TradeWin']\n",
        "                pb_i = Progbar(10, stateful_metrics=metrics_names)\n",
        "                p_mean,trade_time,win_time = np.array([]),np.array([]),np.array([])\n",
        "\n",
        "            state = self.env.reset()\n",
        "            state = self.scaler.transform([state])\n",
        "            done = False\n",
        "        \n",
        "            while not done:\n",
        "                action = self.agent.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = self.scaler.transform([next_state])\n",
        "                reward = self._reward_clipping(reward)\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    s_flag = 11 if np.random.random() <= 0.5 else 22\n",
        "                    agent.train(state, action, reward, next_state, done, s_flag)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            record = pd.Series([info['cur_revenue'],info['trade_time'],info['trade_win']], index=self.df_rec.columns)\n",
        "            self.df_rec = self.df_rec.append(record, ignore_index=True)\n",
        "\n",
        "            p_mean,trade_time,win_time = np.append(p_mean,info['cur_revenue']),np.append(trade_time,info['trade_time']),np.append(win_time,info['trade_win'])\n",
        "            values=[('FixedProfit',int(np.mean(p_mean))), ('TradeTimes',int(np.mean(trade_time))), ('TradeWin',int(np.mean(win_time)))]\n",
        "            pb_i.add(1, values=values)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self._save()\n",
        "        self._save_csv()\n",
        "\n",
        "    def _standard_scaler(self, env):\n",
        "        states = []\n",
        "        for _ in range(env.df_total_steps):\n",
        "            action = np.random.choice(env.action_space)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            states.append(state)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(states)\n",
        "        return scaler\n",
        "\n",
        "    def _reward_clipping(self, val):\n",
        "        if val > 0:\n",
        "            return 1\n",
        "        elif val == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    def _load(self):\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        self.agent.load('{}/{}.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "\n",
        "    def _save(self):\n",
        "        self.agent.save('{}/{}.h5'.format(self.mdl_dir, self.name), '{}/{}_2.h5'.format(self.mdl_dir, self.name))\n",
        "        with open('{}/{}.pkl'.format(self.mdl_dir, self.name), 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "    def _save_csv(self):\n",
        "        self.df_rec.to_csv(csv_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYFNVDDQz9X9",
        "outputId": "42192740-1426-424c-f6eb-6e3c9934f389"
      },
      "source": [
        "initial_money=1000000\n",
        "episodes_times = 200\n",
        "commission = 0 if level == 1 else 0.002\n",
        "\n",
        "agent = Agent()\n",
        "env = Environment(df, initial_money = initial_money, mode = mode, commission = commission)\n",
        "main = Main(env, agent, mdl_dir, name, episodes_times, mode)\n",
        "main.play_game()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 36\n",
            "Trainable params: 36\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "10/10 [==============================] - 430s 43s/step - FixedProfit: 1134542.0000 - TradeTimes: 103.0000 - TradeWin: 57.0000\n",
            "10/10 [==============================] - 427s 43s/step - FixedProfit: 1073051.0000 - TradeTimes: 102.0000 - TradeWin: 57.0000\n",
            "10/10 [==============================] - 428s 43s/step - FixedProfit: 1120682.0000 - TradeTimes: 101.0000 - TradeWin: 58.0000\n",
            "10/10 [==============================] - 416s 42s/step - FixedProfit: 1070103.0000 - TradeTimes: 103.0000 - TradeWin: 55.0000\n",
            "10/10 [==============================] - 428s 43s/step - FixedProfit: 1122995.0000 - TradeTimes: 104.0000 - TradeWin: 57.0000\n",
            "10/10 [==============================] - 425s 42s/step - FixedProfit: 1102534.0000 - TradeTimes: 107.0000 - TradeWin: 60.0000\n",
            "10/10 [==============================] - 428s 43s/step - FixedProfit: 1112505.0000 - TradeTimes: 105.0000 - TradeWin: 55.0000\n",
            "10/10 [==============================] - 429s 43s/step - FixedProfit: 1080213.0000 - TradeTimes: 102.0000 - TradeWin: 56.0000\n",
            "10/10 [==============================] - 425s 42s/step - FixedProfit: 1081180.0000 - TradeTimes: 105.0000 - TradeWin: 57.0000\n",
            "10/10 [==============================] - 427s 43s/step - FixedProfit: 1011267.0000 - TradeTimes: 105.0000 - TradeWin: 57.0000\n",
            "10/10 [==============================] - 422s 42s/step - FixedProfit: 1016108.0000 - TradeTimes: 104.0000 - TradeWin: 55.0000\n",
            "10/10 [==============================] - 419s 42s/step - FixedProfit: 1108527.0000 - TradeTimes: 104.0000 - TradeWin: 58.0000\n",
            "10/10 [==============================] - 418s 42s/step - FixedProfit: 1111783.0000 - TradeTimes: 107.0000 - TradeWin: 58.0000\n",
            "10/10 [==============================] - 420s 42s/step - FixedProfit: 1085142.0000 - TradeTimes: 106.0000 - TradeWin: 58.0000\n",
            "10/10 [==============================] - 418s 42s/step - FixedProfit: 1092441.0000 - TradeTimes: 103.0000 - TradeWin: 56.0000\n",
            "10/10 [==============================] - 420s 42s/step - FixedProfit: 1059318.0000 - TradeTimes: 102.0000 - TradeWin: 57.0000\n",
            "10/10 [==============================] - 422s 42s/step - FixedProfit: 1015647.0000 - TradeTimes: 104.0000 - TradeWin: 56.0000\n",
            "10/10 [==============================] - 422s 42s/step - FixedProfit: 1107506.0000 - TradeTimes: 104.0000 - TradeWin: 57.0000\n",
            "10/10 [==============================] - 419s 42s/step - FixedProfit: 1127489.0000 - TradeTimes: 104.0000 - TradeWin: 57.0000\n",
            "10/10 [==============================] - 420s 42s/step - FixedProfit: 1107658.0000 - TradeTimes: 105.0000 - TradeWin: 58.0000\n"
          ]
        }
      ]
    }
  ]
}